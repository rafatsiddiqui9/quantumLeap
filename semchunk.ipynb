{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# Add these new data structures after imports\n",
    "class SectionType(Enum):\n",
    "    HEADER = \"header\"\n",
    "    CONTENT = \"content\"\n",
    "    QUOTE = \"quote\"\n",
    "    ATTRIBUTION = \"attribution\"\n",
    "    LIST = \"list\"\n",
    "    FRONT_MATTER = \"front_matter\"\n",
    "    TABLE_OF_CONTENTS = \"table_of_contents\"\n",
    "    \n",
    "@dataclass\n",
    "class Section:\n",
    "    text: str\n",
    "    type: SectionType\n",
    "    level: int = 0\n",
    "    metadata: Dict = None\n",
    "    \n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "        \"\"\"Initialize the semantic chunker with model configuration\"\"\"\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"http://localhost:8000/v1\",\n",
    "            api_key=\"dummy\"\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        self.max_tokens = 3000\n",
    "        \n",
    "        # Set up logging directory with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.log_dir = f\"/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/Psychology_Of_Unconscious_Mind/chunks_{timestamp}\"\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        \n",
    "        # Set up logging file for processing summary\n",
    "        self.log_file = os.path.join(self.log_dir, \"processing_log.txt\")\n",
    "        \n",
    "        # Initialize state variables\n",
    "        self.missed_text = \"\"  # Store text not included in LLM output\n",
    "        \n",
    "    def log_message(self, message: str):\n",
    "        \"\"\"Write log message with timestamp and print to console\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {message}\"\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(log_entry + \"\\n\")\n",
    "        print(log_entry)\n",
    "    \n",
    "    def print_separator(self, message: str = \"\"):\n",
    "        \"\"\"Print a separator line with optional message\"\"\"\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        if message:\n",
    "            print(f\"{message}\")\n",
    "            print('='*100)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def find_chapter_breaks(self, text: str) -> List[int]:\n",
    "        \"\"\"Find indices where chapters begin (centered headings)\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        chapter_breaks = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if self.is_chapter_heading(line):\n",
    "                chapter_breaks.append(i)\n",
    "        \n",
    "        return chapter_breaks\n",
    "    \n",
    "    def is_chapter_heading(self, text: str) -> Tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Enhanced chapter heading detection with level identification.\n",
    "        Returns (is_heading, level).\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            return False, 0\n",
    "            \n",
    "        # Chapter patterns\n",
    "        chapter_patterns = [\n",
    "            (r'^CHAPTER\\s+[IVXL]+', 1),  # Main chapter headers\n",
    "            (r'^[IVX]+\\.\\s*—\\s*', 2),    # Sub-chapter headers\n",
    "            (r'^\\d+\\.\\s*—\\s*', 2),       # Numbered sections\n",
    "        ]\n",
    "        \n",
    "        for pattern, level in chapter_patterns:\n",
    "            if re.match(pattern, text, re.I):\n",
    "                return True, level\n",
    "        \n",
    "        # Check for centered text formatting\n",
    "        line_length = len(text)\n",
    "        leading_spaces = len(text) - len(text.lstrip())\n",
    "        trailing_spaces = len(text) - len(text.rstrip())\n",
    "        \n",
    "        is_centered = abs(leading_spaces - trailing_spaces) <= 2 and leading_spaces > 5\n",
    "        is_caps = text.isupper()\n",
    "        reasonable_length = 10 < len(text.strip()) < 100\n",
    "        \n",
    "        if is_centered:\n",
    "            if is_caps and reasonable_length:\n",
    "                return True, 1  # Main header\n",
    "            elif reasonable_length:\n",
    "                return True, 2  # Sub header\n",
    "                \n",
    "        return False, 0\n",
    "    \n",
    "    def analyze_text_structure(self, text: str) -> List[Section]:\n",
    "        \"\"\"\n",
    "        Enhanced text structure analysis with better header and spacing detection.\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "        lines = text.split('\\n')\n",
    "        current_section = []\n",
    "        current_type = None\n",
    "        current_level = 0\n",
    "        \n",
    "        def flush_section():\n",
    "            nonlocal current_section, current_type\n",
    "            if current_section:\n",
    "                # Skip empty sections\n",
    "                content = '\\n'.join(current_section).strip()\n",
    "                if content:  # Only create section if there's actual content\n",
    "                    sections.append(Section(\n",
    "                        text='\\n'.join(current_section),\n",
    "                        type=current_type or SectionType.CONTENT,\n",
    "                        level=current_level\n",
    "                    ))\n",
    "                current_section = []\n",
    "                current_type = None\n",
    "        \n",
    "        in_toc = False\n",
    "        in_front_matter = False\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i]\n",
    "            next_line = lines[i + 1] if i + 1 < len(lines) else \"\"\n",
    "            \n",
    "            # Detect centered headers\n",
    "            if line.strip() and line.strip().isupper():\n",
    "                leading_spaces = len(line) - len(line.lstrip())\n",
    "                if leading_spaces > 10:  # Likely centered\n",
    "                    flush_section()\n",
    "                    current_type = SectionType.HEADER\n",
    "                    current_level = 1\n",
    "                    current_section = [line]\n",
    "                    if not next_line.strip():  # Include following blank line\n",
    "                        current_section.append(next_line)\n",
    "                        i += 1\n",
    "                    flush_section()\n",
    "                    i += 1\n",
    "                    continue\n",
    "            \n",
    "            # Detect Table of Contents\n",
    "            if re.match(r'^\\s*CONTENTS\\s*$', line, re.I):\n",
    "                flush_section()\n",
    "                in_toc = True\n",
    "                current_type = SectionType.TABLE_OF_CONTENTS\n",
    "                current_section = [line]\n",
    "                if not next_line.strip():  # Include following blank line\n",
    "                    current_section.append(next_line)\n",
    "                    i += 1\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Detect Author's Note\n",
    "            if re.match(r'^\\s*AUTHOR\\'S\\s+NOTE\\s*$', line, re.I):\n",
    "                flush_section()\n",
    "                in_front_matter = True\n",
    "                current_type = SectionType.FRONT_MATTER\n",
    "                current_section = [line]\n",
    "                if not next_line.strip():  # Include following blank line\n",
    "                    current_section.append(next_line)\n",
    "                    i += 1\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Detect chapter headings\n",
    "            is_heading, level = self.is_chapter_heading(line)\n",
    "            if is_heading:\n",
    "                flush_section()\n",
    "                current_type = SectionType.HEADER\n",
    "                current_level = level\n",
    "                current_section = [line]\n",
    "                if not next_line.strip():  # Include following blank line\n",
    "                    current_section.append(next_line)\n",
    "                    i += 1\n",
    "                flush_section()\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Handle section content\n",
    "            if in_toc:\n",
    "                if not line.strip() and not next_line.strip():\n",
    "                    in_toc = False\n",
    "                    flush_section()\n",
    "                else:\n",
    "                    current_section.append(line)\n",
    "            elif in_front_matter:\n",
    "                if not line.strip() and not next_line.strip():\n",
    "                    in_front_matter = False\n",
    "                    flush_section()\n",
    "                else:\n",
    "                    current_section.append(line)\n",
    "            else:\n",
    "                current_section.append(line)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        flush_section()  # Flush any remaining content\n",
    "        \n",
    "        # Filter out empty sections and preserve correct spacing\n",
    "        filtered_sections = []\n",
    "        for section in sections:\n",
    "            if section.text.strip():\n",
    "                filtered_sections.append(section)\n",
    "        \n",
    "        return filtered_sections\n",
    "    \n",
    "    def verify_output_completeness(self, input_text: str, output_sections: List[str]) -> str:\n",
    "        \"\"\"Verify all input text is present in output sections and return missing text\"\"\"\n",
    "        # Normalize texts for comparison\n",
    "        input_normalized = ' '.join(input_text.split())\n",
    "        output_normalized = ' '.join(' '.join(output_sections).split())\n",
    "        \n",
    "        # Find missing content\n",
    "        words = input_normalized.split()\n",
    "        window_size = 5  # Look for sequences of 5 words\n",
    "        \n",
    "        missing_sequences = []\n",
    "        i = 0\n",
    "        while i < len(words) - window_size:\n",
    "            sequence = ' '.join(words[i:i+window_size])\n",
    "            if sequence not in output_normalized:\n",
    "                # Find complete missing phrase\n",
    "                start = i\n",
    "                while start > 0 and ' '.join(words[start-1:i+window_size]) not in output_normalized:\n",
    "                    start -= 1\n",
    "                end = i + window_size\n",
    "                while end < len(words) and ' '.join(words[i:end+1]) not in output_normalized:\n",
    "                    end += 1\n",
    "                missing_sequences.append(' '.join(words[start:end]))\n",
    "                i = end\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return '\\n'.join(missing_sequences) if missing_sequences else \"\"\n",
    "    \n",
    "    def create_initial_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Create initial chunks with enhanced logging.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        remaining_text = text\n",
    "        chunk_number = 0\n",
    "        \n",
    "        while remaining_text.strip():\n",
    "            chunk_number += 1\n",
    "            self.log_message(f\"\\nProcessing chunk {chunk_number}\")\n",
    "            \n",
    "            # Add any missed text from previous chunk\n",
    "            if self.missed_text:\n",
    "                self.log_message(\"Adding missed text from previous chunk\")\n",
    "                remaining_text = self.missed_text + '\\n\\n' + remaining_text\n",
    "                self.missed_text = \"\"\n",
    "            \n",
    "            # Get complete paragraphs up to token limit\n",
    "            chunk_text, remaining_text = self.get_complete_paragraphs(remaining_text, self.max_tokens)\n",
    "            \n",
    "            if chunk_text.strip():\n",
    "                self.log_message(f\"Created chunk {chunk_number} with {self.count_tokens(chunk_text)} tokens\")\n",
    "                chunks.append(chunk_text)\n",
    "                \n",
    "                # Debug output\n",
    "                preview = chunk_text[:200] + \"...\" if len(chunk_text) > 200 else chunk_text\n",
    "                self.log_message(f\"Chunk {chunk_number} preview:\\n{preview}\")\n",
    "            else:\n",
    "                self.log_message(\"Warning: Empty chunk produced\")\n",
    "                if not remaining_text.strip():\n",
    "                    break\n",
    "            \n",
    "            if len(chunks) >= 100:  # Safety limit\n",
    "                self.log_message(\"Warning: Maximum chunk limit reached\")\n",
    "                break\n",
    "        \n",
    "        self.log_message(f\"Created {len(chunks)} initial chunks\")\n",
    "        \n",
    "        # Save the chunks\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            with open(os.path.join(self.log_dir, f\"chunk_{i+1:04d}.txt\"), 'w', encoding='utf-8') as f:\n",
    "                f.write(chunk)\n",
    "                \n",
    "        return chunks\n",
    "    \n",
    "    def get_semantic_sections(self, chunk: str) -> Tuple[List[str], Dict]:\n",
    "        \"\"\"Update the system prompt for better structural preservation.\"\"\"\n",
    "        try:\n",
    "            self.log_message(f\"Sending request to LLM (input tokens: {self.count_tokens(chunk)})\")\n",
    "            \n",
    "            # Add timeout to the request\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"You are a text analysis expert. Your task is to:\n",
    "                        1. Maintain the original document structure (headers, lists, quotes)\n",
    "                        2. Split the input text into coherent semantic sections\n",
    "                        3. Each section must respect structural boundaries\n",
    "                        4. Use <START_SECTION> and <END_SECTION> to mark sections\n",
    "                        5. Include ALL text from the input - do not skip any content\n",
    "                        6. Preserve ALL formatting, indentation, and special characters\n",
    "                        7. If there's a header, keep it with its content\n",
    "                        8. Keep lists and quotes intact within their sections\n",
    "                        9. If a section would be incomplete, mark it with <INCOMPLETE> tags\"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Split this text into coherent sections, preserving ALL content and structure:\\n\\n{chunk}\"\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=self.max_tokens,\n",
    "                temperature=0.2,\n",
    "                timeout=30  # Add 30 second timeout\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Extract sections\n",
    "            sections = []\n",
    "            section_pattern = r'<START_SECTION>(.*?)<END_SECTION>'\n",
    "            for match in re.finditer(section_pattern, result, re.DOTALL):\n",
    "                section_text = match.group(1).strip()\n",
    "                if section_text and len(section_text) > 50:  # Ignore empty or very short sections\n",
    "                    sections.append(section_text)\n",
    "            \n",
    "            # Check for incomplete section\n",
    "            incomplete_pattern = r'<INCOMPLETE>(.*?)</INCOMPLETE>'\n",
    "            incomplete_match = re.search(incomplete_pattern, result, re.DOTALL)\n",
    "            if incomplete_match:\n",
    "                incomplete_text = incomplete_match.group(1).strip()\n",
    "                if incomplete_text:\n",
    "                    self.missed_text = incomplete_text\n",
    "                    self.log_message(f\"Found incomplete section ({self.count_tokens(incomplete_text)} tokens)\")\n",
    "            \n",
    "            # Verify all content is included\n",
    "            if not incomplete_match:  # Only check if no explicit incomplete section\n",
    "                missed_text = self.verify_output_completeness(chunk, sections)\n",
    "                if missed_text:\n",
    "                    self.missed_text = missed_text\n",
    "                    self.log_message(f\"Found missed text ({self.count_tokens(missed_text)} tokens)\")\n",
    "            \n",
    "            metrics = {\n",
    "                \"completion_tokens\": response.usage.completion_tokens,\n",
    "                \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                \"total_tokens\": response.usage.total_tokens,\n",
    "                \"finish_reason\": response.choices[0].finish_reason,\n",
    "                \"sections_created\": len(sections),\n",
    "                \"has_missed_text\": bool(self.missed_text)\n",
    "            }\n",
    "            \n",
    "            return sections, metrics\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_message(f\"Error in LLM request: {str(e)}\")\n",
    "            return [], {}\n",
    "\n",
    "    def get_complete_paragraphs(self, text: str, max_tokens: int) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Enhanced version with corrected content processing logic.\n",
    "        \"\"\"\n",
    "        self.log_message(f\"Starting get_complete_paragraphs with {len(text)} chars of text\")\n",
    "        \n",
    "        sections = self.analyze_text_structure(text)\n",
    "        self.log_message(f\"Found {len(sections)} sections\")\n",
    "        \n",
    "        current_sections = []\n",
    "        current_tokens = 0\n",
    "        section_index = 0\n",
    "        \n",
    "        try:\n",
    "            while section_index < len(sections):\n",
    "                section = sections[section_index]\n",
    "                section_tokens = self.count_tokens(section.text)\n",
    "                \n",
    "                self.log_message(f\"Processing section {section_index + 1}: {section.type}, {section_tokens} tokens\")\n",
    "                \n",
    "                # If this section would exceed our token limit\n",
    "                if current_tokens + section_tokens > max_tokens:\n",
    "                    if current_sections:  # Only break if we have content\n",
    "                        break\n",
    "                \n",
    "                # Always include header with its following content\n",
    "                if section.type == SectionType.HEADER:\n",
    "                    # Add the header\n",
    "                    current_sections.append(section)\n",
    "                    current_tokens += section_tokens\n",
    "                    \n",
    "                    # Look ahead for content\n",
    "                    next_index = section_index + 1\n",
    "                    if next_index < len(sections) and sections[next_index].type == SectionType.CONTENT:\n",
    "                        next_section = sections[next_index]\n",
    "                        next_tokens = self.count_tokens(next_section.text)\n",
    "                        if current_tokens + next_tokens <= max_tokens:\n",
    "                            current_sections.append(next_section)\n",
    "                            current_tokens += next_tokens\n",
    "                            section_index += 1  # Skip the content section in next iteration\n",
    "                    \n",
    "                # Handle content sections not attached to headers\n",
    "                elif section.type == SectionType.CONTENT:\n",
    "                    current_sections.append(section)\n",
    "                    current_tokens += section_tokens\n",
    "                \n",
    "                # Handle other section types (TABLE_OF_CONTENTS, etc.)\n",
    "                else:\n",
    "                    current_sections.append(section)\n",
    "                    current_tokens += section_tokens\n",
    "                \n",
    "                section_index += 1\n",
    "                self.log_message(f\"After processing: current_tokens={current_tokens}, max_tokens={max_tokens}, sections_processed={len(current_sections)}\")\n",
    "            \n",
    "            # Combine sections with proper spacing\n",
    "            processed_sections = []\n",
    "            for i, section in enumerate(current_sections):\n",
    "                # Add extra newline before sections (except the first one)\n",
    "                if i > 0:\n",
    "                    processed_sections.append(\"\")\n",
    "                \n",
    "                # Add the section text\n",
    "                processed_sections.append(section.text.rstrip())\n",
    "                \n",
    "                # Add extra newline after headers\n",
    "                if section.type == SectionType.HEADER:\n",
    "                    processed_sections.append(\"\")\n",
    "            \n",
    "            processed_text = \"\\n\".join(processed_sections)\n",
    "            \n",
    "            # Prepare remaining sections\n",
    "            remaining_sections = []\n",
    "            if section_index < len(sections):\n",
    "                for section in sections[section_index:]:\n",
    "                    if remaining_sections:\n",
    "                        remaining_sections.append(\"\")\n",
    "                    remaining_sections.append(section.text.rstrip())\n",
    "            \n",
    "            remaining_text = \"\\n\".join(remaining_sections) if remaining_sections else \"\"\n",
    "            \n",
    "            self.log_message(f\"Completed processing: {len(current_sections)} sections included, {len(sections) - section_index} remaining\")\n",
    "            self.log_message(f\"Processed text preview: {processed_text[:200]}...\")\n",
    "            \n",
    "            return processed_text, remaining_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_message(f\"Error in get_complete_paragraphs: {str(e)}\")\n",
    "            if current_sections:\n",
    "                return \"\\n\".join([s.text for s in current_sections]), text\n",
    "            return \"\", text\n",
    "\n",
    "    def process_text(self, text: str, max_chunks: int = None) -> List[str]:\n",
    "        \"\"\"Process entire text into semantic sections with enhanced logging\"\"\"\n",
    "        self.log_message(\"Starting text processing\")\n",
    "        \n",
    "        # Create initial chunks\n",
    "        initial_chunks = self.create_initial_chunks(text)\n",
    "        \n",
    "        if max_chunks:\n",
    "            initial_chunks = initial_chunks[:max_chunks]\n",
    "            self.log_message(f\"Processing limited to first {max_chunks} chunks\")\n",
    "        \n",
    "        # Process each chunk\n",
    "        semantic_chunks = []\n",
    "        for i, chunk in enumerate(initial_chunks):\n",
    "            self.log_message(f\"Processing chunk {i+1}/{len(initial_chunks)}\")\n",
    "            \n",
    "            # Get semantic sections\n",
    "            sections, metrics = self.get_semantic_sections(chunk)\n",
    "            \n",
    "            # Print processing details\n",
    "            self.print_separator(\"INPUT CHUNK\")\n",
    "            print(f\"Chunk {i+1} (Tokens: {self.count_tokens(chunk)})\")\n",
    "            print(\"Content preview:\")\n",
    "            print(chunk[:1000] + \"...\" if len(chunk) > 1000 else chunk)\n",
    "            \n",
    "            self.print_separator(\"SEMANTIC SECTIONS\")\n",
    "            for j, section in enumerate(sections):\n",
    "                print(f\"\\nSection {j+1} (Tokens: {self.count_tokens(section)})\")\n",
    "                print(\"Content preview:\")\n",
    "                print(section[:500] + \"...\" if len(section) > 500 else section)\n",
    "            \n",
    "            self.print_separator(\"METRICS\")\n",
    "            pprint(metrics)\n",
    "            \n",
    "            if self.missed_text:\n",
    "                self.print_separator(\"MISSED TEXT\")\n",
    "                print(self.missed_text)\n",
    "            \n",
    "            semantic_chunks.extend(sections)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            self.save_chunk_log(i+1, chunk, sections, metrics)\n",
    "            \n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "        self.log_message(f\"Processing complete. Total semantic chunks created: {len(semantic_chunks)}\")\n",
    "        return semantic_chunks\n",
    "\n",
    "    def save_chunk_log(self, chunk_num: int, original_chunk: str, sections: List[str], metrics: Dict):\n",
    "        \"\"\"Save intermediate processing results\"\"\"\n",
    "        log_file = os.path.join(self.log_dir, f\"chunk_{chunk_num:04d}.json\")\n",
    "        log_data = {\n",
    "            \"chunk_number\": chunk_num,\n",
    "            \"original_text\": original_chunk,\n",
    "            \"semantic_sections\": sections,\n",
    "            \"missed_text\": self.missed_text,\n",
    "            \"metrics\": metrics,\n",
    "            \"token_counts\": {\n",
    "                \"input\": self.count_tokens(original_chunk),\n",
    "                \"sections\": [self.count_tokens(s) for s in sections],\n",
    "                \"missed\": self.count_tokens(self.missed_text) if self.missed_text else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(log_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(log_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "    def validate_chunk(self, chunk: str, original_sections: List[Section]) -> bool:\n",
    "        \"\"\"Validate that chunk contains all expected content\"\"\"\n",
    "        # Normalize texts for comparison\n",
    "        chunk_text = ' '.join(chunk.split())\n",
    "        original_text = ' '.join(' '.join(s.text for s in original_sections).split())\n",
    "        \n",
    "        # Check if all content is present\n",
    "        missing_content = []\n",
    "        words = original_text.split()\n",
    "        window_size = 5\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(words) - window_size:\n",
    "            sequence = ' '.join(words[i:i+window_size])\n",
    "            if sequence not in chunk_text:\n",
    "                missing_content.append(sequence)\n",
    "            i += 1\n",
    "        \n",
    "        if missing_content:\n",
    "            self.log_message(\"Missing content detected:\")\n",
    "            for mc in missing_content:\n",
    "                self.log_message(f\"  - {mc}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "def main():\n",
    "    # Initialize chunker\n",
    "    chunker = SemanticChunker()\n",
    "    \n",
    "    # Read input file\n",
    "    input_file = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Process text (limit to first 5 chunks for testing)\n",
    "    semantic_chunks = chunker.process_text(text, max_chunks=3)\n",
    "    \n",
    "    # Save final chunks\n",
    "    output_dir = os.path.join(chunker.log_dir, \"semantic_chunks\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, chunk in enumerate(semantic_chunks):\n",
    "        output_file = os.path.join(output_dir, f\"semantic_chunk_{i+1:04d}.txt\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(chunk)\n",
    "    \n",
    "    chunker.log_message(f\"Saved {len(semantic_chunks)} semantic chunks to {output_dir}\")\n",
    "\n",
    "def test_structure_analysis():\n",
    "    chunker = SemanticChunker()\n",
    "    \n",
    "    test_text = \"\"\"\n",
    "                             AUTHOR'S NOTE\n",
    "\n",
    "My task in this work has been to investigate an individual phantasy\n",
    "system, and in the doing of it problems of such magnitude have been,,,,,,,,,,,,,,\n",
    "uncovered, that my endeavor to grasp them in their entirety has\n",
    "necessarily meant only a superficial orientation toward those paths, the\n",
    "opening and exploration of which may possibly crown the work of future\n",
    "investigators with success.\n",
    "\n",
    "                                CONTENTS\n",
    "\n",
    "        INTRODUCTION                                                     3\n",
    "        \n",
    "        Relation of the Incest Phantasy to the Oedipus Legend—Moral\n",
    "        revulsion over such a discovery\n",
    "\n",
    " I.—    CONCERNING THE TWO KINDS OF THINKING                             8\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"Testing structural analysis...\")\n",
    "        sections = chunker.analyze_text_structure(test_text)\n",
    "        \n",
    "        print(\"\\nIdentified sections:\")\n",
    "        for i, section in enumerate(sections, 1):\n",
    "            print(f\"\\nSection {i}:\")\n",
    "            print(f\"Type: {section.type}\")\n",
    "            print(f\"Level: {section.level}\")\n",
    "            print(f\"Content preview: {section.text[:100]}...\")\n",
    "        \n",
    "        print(\"\\nTesting chunking with structure preservation...\")\n",
    "        chunks = chunker.create_initial_chunks(test_text)\n",
    "        \n",
    "        print(\"\\nResulting chunks:\")\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\nChunk {i}:\")\n",
    "            print(chunk[:200])\n",
    "            print(\"...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {str(e)}\")\n",
    "        \n",
    "      # Add validation\n",
    "    print(\"\\nValidating chunk content...\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\nValidating chunk {i}:\")\n",
    "        is_valid = chunker.validate_chunk(chunk, sections)\n",
    "        print(f\"Chunk {i} validation: {'PASSED' if is_valid else 'FAILED'}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # test_structure_analysis()\n",
    "    # Comment out main() for testing\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
