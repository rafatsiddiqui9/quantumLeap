{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- #\n",
    "# # Part 1.1: Install and Setup Libraries\n",
    "# # ----------------------------- #\n",
    "\n",
    "# # run below in terminal only. This code works only for Ola Krutrim Cloud Instance. Restart once you have installed the following\n",
    "# # pip install uv #install this in the virtual environment where you want to execute the notebook.\n",
    "# # uv venv virtualenvironment # if you are not in an externally managed environment, then you can run this\n",
    "# # source virtualenvironment/bin/activate # if you were able to run above code, then activate. DO NOT use --system flag in subsequent lines if you are able to do thi\n",
    "# !uv pip install unsloth --system\n",
    "# !uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --system\n",
    "# !uv pip install wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets --system\n",
    "# # restart once you have installed all of the above\n",
    "\n",
    "# !nvidia-smi\n",
    "\n",
    "# !nvcc --version\n",
    "\n",
    "# import torch\n",
    "# print(torch.__version__)          # Should reflect 2.5.0+cu124\n",
    "# print(torch.version.cuda)         # Should output 12.4\n",
    "# print(torch.cuda.is_available())  # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 1.2: Import Libraries\n",
    "# ----------------------------- #\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import xformers\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "import ipywidgets\n",
    "import unsloth\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is available\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print('punkt was already available.')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    print('punkt was not available. It has been downloaded')\n",
    "\n",
    "# Initialize spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print('en_core_web_sm was already available.')\n",
    "except OSError:\n",
    "    print(\"SpaCy English model not found. Downloading...\")\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 2: Load and Clean the Text Data\n",
    "# ----------------------------- #\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    # # Remove Project Gutenberg's license text and headers/footers\n",
    "    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
    "    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
    "\n",
    "    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n",
    "    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n",
    "    return text.strip()\n",
    "\n",
    "# Replace 'psychology_of_unconscious.txt' with your actual file path\n",
    "file_path = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/psychology_of_unconscious.txt'\n",
    "clean_text = load_and_clean_text(file_path)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 3: Parse Text into Discourse Units\n",
    "# ----------------------------- #\n",
    "\n",
    "# def parse_discourse_units(text):\n",
    "#     \"\"\"\n",
    "#     Parses text into discourse units using spaCy.\n",
    "#     Currently splits text into sentences.\n",
    "#     \"\"\"\n",
    "#     paragraphs = text.split('\\n\\n')\n",
    "#     paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
    "    \n",
    "#     discourse_units = []\n",
    "#     for para in paragraphs:\n",
    "#         doc = nlp(para)\n",
    "#         sentences = [sent.text for sent in doc.sents]\n",
    "#         discourse_units.extend(sentences)\n",
    "#     return discourse_units\n",
    "\n",
    "# discourse_units = parse_discourse_units(clean_text)\n",
    "\n",
    "# # Save discourse_units to a JSON file\n",
    "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "# Load discourse_units from the JSON file\n",
    "with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.json', 'r', encoding='utf-8') as f:\n",
    "    discourse_units = json.load(f)\n",
    "\n",
    "len(discourse_units)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 4: Create Chunks Using Hybrid Strategy\n",
    "# ----------------------------- #\n",
    "\n",
    "def create_chunks(discourse_units, tokenizer, max_length=512, overlap_size=0):\n",
    "    \"\"\"\n",
    "    Creates chunks from discourse units using a sliding window with overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for unit in discourse_units:\n",
    "        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n",
    "        unit_length = len(unit_tokens)\n",
    "\n",
    "        if current_length + unit_length <= max_length:\n",
    "            current_chunk.append(unit)\n",
    "            current_length += unit_length\n",
    "        else:\n",
    "            # Append the current chunk\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            # Create overlap\n",
    "            overlap_text = ' '.join(current_chunk)[-overlap_size:]\n",
    "            overlap_tokens = tokenizer.encode(overlap_text, add_special_tokens=False)\n",
    "            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n",
    "            # Start new chunk with overlap and current unit\n",
    "            current_chunk = [overlap_text, unit]\n",
    "            current_length = len(tokenizer.encode(overlap_text, add_special_tokens=False)) + unit_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "base_model_slug = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model_slug, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 5: : Load the Tokenizer and Model\n",
    "# ----------------------------- #\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 6: Create Chunks (After Tokenizer is Loaded)\n",
    "# ----------------------------- #\n",
    "\n",
    "chunks_max_length = max_seq_length\n",
    "overlap_size = 1\n",
    "chunks = create_chunks(discourse_units, tokenizer, max_length=chunks_max_length, overlap_size=overlap_size)\n",
    "\n",
    "# Save chunks to a JSON file (Optional)\n",
    "with open(f'/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final_{chunks_max_length}_{overlap_size}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# # If you need to reload from JSON (Optional)\n",
    "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.json', 'r', encoding='utf-8') as f:\n",
    "#     chunks = json.load(f)\n",
    "    \n",
    "print(len(chunks))\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 7: Create and Tokenize Dataset\n",
    "# ----------------------------- #\n",
    "\n",
    "# Create a Dataset object from chunks\n",
    "\n",
    "book_title = 'Psychology of the Unconscious by C. G. Jung'\n",
    "wikipedia_prompt = \"\"\"\n",
    "### Title: {}\n",
    "\n",
    "### Article: {}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    titles = book_title\n",
    "    texts  = examples[\"text\"]\n",
    "    outputs = []\n",
    "    for title, text in zip([book_title]*len(chunks), texts):\n",
    "        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n",
    "        outputs.append(text)\n",
    "    return { \"text\" : outputs, }\n",
    "pass\n",
    "\n",
    "# convert chunks variable to huggingface dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": chunks})\n",
    "\n",
    "# dataset = dataset.train_test_split(test_size = 0.1)[\"train\"]\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "len(dataset)\n",
    "\n",
    "# Find the maximum length of the text field in the entire dataset\n",
    "max_length = max(len(text) for text in dataset['text'])\n",
    "print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 8: Configure Training Arguments\n",
    "# ----------------------------- #\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import wandb\n",
    "\n",
    "# Define your parameters\n",
    "batchSize = 2\n",
    "ga = 8\n",
    "maxSteps = 10\n",
    "lRate = 5e-5\n",
    "embLRate = 1e-5\n",
    "optim = \"adamw_8bit\"\n",
    "lrSchedule = \"linear\"\n",
    "\n",
    "# Get the current date and time in Indian Standard Time (IST)\n",
    "ist = pytz.timezone('Asia/Kolkata')\n",
    "current_datetime = datetime.now(ist)\n",
    "\n",
    "# Format the datetime string\n",
    "# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n",
    "formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the run name with the current date and time\n",
    "run_name = f\"\"\"Unsloth-CPT-Base-{formatted_datetime}-{base_model_slug}-{max_seq_length}_max_seq_length-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "# It's recommended to set your W&B API key as an environment variable for security.\n",
    "# Example: export WANDB_API_KEY=\"your_api_key\"\n",
    "wandb.login(key=\"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\")  # Consider using environment variables for security\n",
    "wandb.init(project=\"Unsloth-CPT\", name=run_name)\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = batchSize,\n",
    "        gradient_accumulation_steps = ga,\n",
    "\n",
    "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
    "        max_steps = maxSteps,\n",
    "        warmup_steps = 10,\n",
    "        # warmup_ratio = 0.1,\n",
    "        # num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate =lRate,\n",
    "        embedding_learning_rate = embLRate,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = optim,\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = lrSchedule,\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        \n",
    "        \n",
    "        report_to=[\"tensorboard\", \"wandb\"],\n",
    "        logging_dir=f\"./trel-fft-logs/{run_name}\",\n",
    "\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 9: Define Compute Metrics Function\n",
    "# ----------------------------- #\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes perplexity based on model predictions and labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # Convert to torch tensors\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Ensure shapes match\n",
    "    if logits.shape[:2] != labels.shape:\n",
    "        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n",
    "    \n",
    "    # Shift logits and labels\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "    # Check label values\n",
    "    if shift_labels.max() >= model.config.vocab_size:\n",
    "        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n",
    "    \n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 10: Initialize logging\n",
    "# ----------------------------- #\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    level=logging.INFO,  # Set to DEBUG for more detailed logs\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "    \n",
    "# ----------------------------- #\n",
    "# Part 11: Start Training\n",
    "# ----------------------------- #\n",
    "\n",
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 12: Save the Bsae Fine-Tuned Model\n",
    "# ----------------------------- #\n",
    "\n",
    "model.save_pretrained(\"lora_model_pum\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model_pum\")\n",
    "\n",
    "!huggingface-cli login --token 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 --add-to-git-credential\n",
    "if False:\n",
    "    model.push_to_hub(\"olabs-ai/qLeap_base_v01\", token = \"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\") # Online saving\n",
    "    tokenizer.push_to_hub(\"olabs-ai/qLeap_base_v01\", token = \"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\") # Online saving\n",
    "    model.push_to_hub_gguf(\"olabs-ai/qLeap_base_v01\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n",
    "    \n",
    "    \n",
    "# ----------------------------- #\n",
    "# Part 13: Generate Inference from Base Fine-Tuned Model for testing purpose\n",
    "# ----------------------------- #\n",
    "    \n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model_pum\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    \n",
    "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: \n",
    "concept_name: {}\n",
    "detailed_explanation: {}\n",
    "Given the concept in concept_name variable and its detailed explanation in detailed_explanation variable, provide an example scenario that illustrates the concept.\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    instruction_prompt.format(\n",
    "        \"Hero Archetype\", # concept_name\n",
    "        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "model.config.torch_dtype = torch.bfloat16 \n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512) # using repetition_penalty of 0.1 leads to repetition of text and high values lead to wierd grammer issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 495 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff68b0ff2a84d0c91a4a7ee597ab3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample processed example:\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Explain the concept of Phenomenology and provide an example.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Phenomenology is a philosophical approach that focuses on the study of conscious experience or perception. It seeks to understand how individuals experience and interpret the world around them, without making assumptions or presuppositions. Developed by German philosopher Edmund Husserl in the early 20th century, phenomenology emphasizes the importance of subjective experience and the need to bracket, or set aside, preconceptions and biases in order to gain a more authentic understanding of phenomena. In qualitative market research, ethnography, and social anthropology, phenomenology is used to gain a deeper understanding of people's experiences, behaviors, and cultural practices. There are several types of phenomenology, including transcendental phenomenology, existential phenomenology, and hermeneutic phenomenology. Transcendental phenomenology focuses on the universal structures of conscious experience, while existential phenomenology emphasizes the individual's existence and experience in the world. Hermeneutic phenomenology, on the other hand, focuses on the interpretation and understanding of texts and other forms of human expression. Within these types, there are also subtypes, such as phenomenology of perception, phenomenology of embodiment, and phenomenology of intersubjectivity.\n",
      "\n",
      "Example:\n",
      "A market researcher uses phenomenology to study the experience of consumers when interacting with a new product. The researcher conducts in-depth interviews with participants, asking them to describe their experiences and perceptions of the product in detail. By bracketing preconceptions and biases, the researcher is able to gain a more authentic understanding of the participants' experiences and identify key themes and patterns that can inform product development and marketing strategies. For instance, the researcher may discover that participants experience a sense of excitement and novelty when using the product, but also encounter difficulties with navigation and usability. This information can be used to refine the product and improve the overall user experience.<|eot_id|>\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3 MIG 1g.10gb. Max memory: 9.75 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 9.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"attribute 'weight' already exists\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 173\u001b[0m\n\u001b[1;32m    163\u001b[0m special_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|end_header_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m ]\n\u001b[1;32m    172\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39madd_tokens(special_tokens)\n\u001b[0;32m--> 173\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize_token_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m special_tokens:\n\u001b[1;32m    176\u001b[0m     token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token)\n",
      "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/transformers/modeling_utils.py:2035\u001b[0m, in \u001b[0;36mPreTrainedModel.resize_token_embeddings\u001b[0;34m(self, new_num_tokens, pad_to_multiple_of)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m vocab_size\n\u001b[1;32m   2034\u001b[0m \u001b[38;5;66;03m# Tie weights again if needed\u001b[39;00m\n\u001b[0;32m-> 2035\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtie_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_embeds\n",
      "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/transformers/modeling_utils.py:1832\u001b[0m, in \u001b[0;36mPreTrainedModel.tie_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     output_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_output_embeddings()\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1832\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tie_or_clone_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_encoder_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_encoder_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prefix):\n",
      "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/transformers/modeling_utils.py:1944\u001b[0m, in \u001b[0;36mPreTrainedModel._tie_or_clone_weights\u001b[0;34m(self, output_embeddings, input_embeddings)\u001b[0m\n\u001b[1;32m   1942\u001b[0m     output_embeddings\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(input_embeddings\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1944\u001b[0m     \u001b[43moutput_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m \u001b[38;5;241m=\u001b[39m input_embeddings\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(output_embeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1947\u001b[0m     output_embeddings\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m   1948\u001b[0m         output_embeddings\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m   1949\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1955\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/torch/nn/modules/module.py:1956\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1947\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1948\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign parameters before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1949\u001b[0m         )\n\u001b[1;32m   1950\u001b[0m     remove_from(\n\u001b[1;32m   1951\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m,\n\u001b[1;32m   1952\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers,\n\u001b[1;32m   1953\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules,\n\u001b[1;32m   1954\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set,\n\u001b[1;32m   1955\u001b[0m     )\n\u001b[0;32m-> 1956\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/torch/nn/modules/module.py:608\u001b[0m, in \u001b[0;36mModule.register_parameter\u001b[0;34m(self, name, param)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameter name can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt be empty string \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"attribute 'weight' already exists\""
     ]
    }
   ],
   "source": [
    "# ----------------------------- #\n",
    "# Part 1: Import Libraries\n",
    "# ----------------------------- #\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "from datasets import Dataset\n",
    "from transformers import TextStreamer\n",
    "from unsloth import (\n",
    "    FastLanguageModel,\n",
    "    UnslothTrainer,\n",
    "    UnslothTrainingArguments,\n",
    "    is_bfloat16_supported\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='transformation_errors.log',\n",
    "    filemode='w',\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Define paths\n",
    "INPUT_CSV_PATH = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv'\n",
    "OUTPUT_JSON_PATH = '/root/qLeap-fft/data/input/Instruction_Data/transformed_data.json'\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 2: Load and Clean the Text Data\n",
    "# ----------------------------- #\n",
    "\n",
    "def read_csv_data(input_csv_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Read and validate the input CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(input_csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            return list(reader)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def transform_data(original_data: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Transform the original data by expanding example scenarios.\"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    for idx, entry in enumerate(original_data, start=1):\n",
    "        concept_name = entry.get('concept_name', '').strip()\n",
    "        detailed_explanation = entry.get('detailed_explanation', '').strip()\n",
    "        example_scenario_str = entry.get('example_scenario', '').strip()\n",
    "\n",
    "        if not all([concept_name, detailed_explanation, example_scenario_str]):\n",
    "            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            example_scenarios = json.loads(example_scenario_str)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                example_scenarios = ast.literal_eval(example_scenario_str)\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}') has invalid example_scenario: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not isinstance(example_scenarios, list):\n",
    "            logging.error(f\"Entry {idx} ('{concept_name}'): example_scenario is not a list\")\n",
    "            continue\n",
    "\n",
    "        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n",
    "            if not isinstance(scenario, str):\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}'): non-string scenario at position {scenario_idx}\")\n",
    "                continue\n",
    "\n",
    "            new_data.append({\n",
    "                'concept_name': concept_name,\n",
    "                'detailed_explanation': detailed_explanation,\n",
    "                'example_scenario': scenario.strip()\n",
    "            })\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# Process and save the data\n",
    "original_data = read_csv_data(INPUT_CSV_PATH)\n",
    "transformed_data = transform_data(original_data)\n",
    "\n",
    "# Save transformed data\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)\n",
    "with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Processed {len(transformed_data)} examples\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 3: Create Instruction Prompt Template and process data in that\n",
    "# ----------------------------- #\n",
    "\n",
    "# Define instruction template\n",
    "instruction_template = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Knowledge Date Cutoff: December 2023\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\"\"\"\n",
    "\n",
    "def create_instruction_dataset(transformed_data: List[Dict[str, str]]) -> Dataset:\n",
    "    \"\"\"Create an instruction dataset from transformed data.\"\"\"\n",
    "    def instruction_prompt_func(examples):\n",
    "        return {\n",
    "            \"text\": [\n",
    "                instruction_template.format(\n",
    "                    f\"Explain the concept of {cn} and provide an example.\",\n",
    "                    f\"{de}\\n\\nExample:\\n{es}\"\n",
    "                )\n",
    "                for cn, de, es in zip(\n",
    "                    examples[\"concept_name\"],\n",
    "                    examples[\"detailed_explanation\"],\n",
    "                    examples[\"example_scenario\"]\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    dataset = Dataset.from_list(transformed_data)\n",
    "    return dataset.map(instruction_prompt_func, batched=True)\n",
    "\n",
    "# Create the dataset\n",
    "instruction_dataset = create_instruction_dataset(transformed_data)\n",
    "\n",
    "# Print a sample to verify\n",
    "print(\"\\nSample processed example:\")\n",
    "print(instruction_dataset[0][\"text\"])\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 4: : Load the Tokenizer and Model\n",
    "# ----------------------------- #\n",
    "\n",
    "# Model initialization parameters\n",
    "base_model_slug = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
    "model_name = \"lora_model_pum\"\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_model_pum\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "\n",
    "# Check for special tokens\n",
    "special_tokens = [\n",
    "    \"<|start_header_id|>\",\n",
    "    \"<|end_header_id|>\",\n",
    "    \"<|eot_id|>\",\n",
    "    \"system\",\n",
    "    \"user\",\n",
    "    \"assistant\"\n",
    "]\n",
    "\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    assert token_id != tokenizer.unk_token_id, f\"Token {token} is still unknown.\"\n",
    "\n",
    "\n",
    "for token in special_tokens:\n",
    "    if token not in tokenizer.get_vocab():\n",
    "        print(f\"Warning: {token} not in vocabulary!\")\n",
    "\n",
    "# Configure model\n",
    "model.config.torch_dtype = torch.bfloat16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------- #\n",
    "# Part 5: : Configure Training Arguments\n",
    "# ----------------------------- #\n",
    "\n",
    "def setup_training(model, tokenizer, dataset, \n",
    "                  batch_size=2, gradient_accumulation=8, max_steps=10):\n",
    "    \"\"\"Setup the training configuration.\"\"\"\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    import wandb\n",
    "\n",
    "    # Define your parameters\n",
    "    batchSize = 2\n",
    "    ga = 8\n",
    "    maxSteps = 10\n",
    "    lRate = 5e-5\n",
    "    embLRate = 1e-5\n",
    "    optim = \"adamw_8bit\"\n",
    "    lrSchedule = \"linear\"\n",
    "\n",
    "    # Get the current date and time in Indian Standard Time (IST)\n",
    "    ist = pytz.timezone('Asia/Kolkata')\n",
    "    current_datetime = datetime.now(ist)\n",
    "\n",
    "    # Format the datetime string\n",
    "    # Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create the run name with the current date and time\n",
    "    run_name = f\"\"\"Unsloth-CPT-Instruct-{formatted_datetime}-{base_model_slug}-{max_seq_length}_max_seq_length-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n",
    "\n",
    "    # Initialize Weights & Biases\n",
    "    # It's recommended to set your W&B API key as an environment variable for security.\n",
    "    # Example: export WANDB_API_KEY=\"your_api_key\"\n",
    "    wandb.login(key=\"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\")  # Consider using environment variables for security\n",
    "    wandb.init(project=\"Unsloth-CPT\", name=run_name)\n",
    "    \n",
    "    training_args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        max_steps=max_steps,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        embedding_learning_rate=1e-5,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=[\"tensorboard\", \"wandb\"],\n",
    "        logging_dir=\"./trel-fft-logs\"\n",
    "    )\n",
    "\n",
    "    return UnslothTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        args=training_args\n",
    "    )\n",
    "\n",
    "# Setup trainer\n",
    "trainer = setup_training(model, tokenizer, instruction_dataset)\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 6: Save the Instruction Fine-Tuned Model\n",
    "# ----------------------------- #\n",
    "\n",
    "model.save_pretrained(\"lora_model_pum_instruct\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model_pum_instruct\")\n",
    "\n",
    "!huggingface-cli login --token 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 --add-to-git-credential\n",
    "if False:\n",
    "    model.push_to_hub(\"olabs-ai/qLeap_instruct_v01\", token = \"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\") # Online saving\n",
    "    tokenizer.push_to_hub(\"olabs-ai/qLeap_instruct_v01\", token = \"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\") # Online saving\n",
    "    model.push_to_hub_gguf(\"olabs-ai/qLeap_instruct_v01\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 7: Generate Inference from Instruction Fine-Tuned Model\n",
    "# ----------------------------- #\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model initialization parameters\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Enable faster inference\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model_pum_instruct\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Instruction prompt matching the fine-tuning template\n",
    "instruction_prompt = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Explain the concept of {} and provide an example.<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Set model dtype\n",
    "model.config.torch_dtype = torch.bfloat16\n",
    "\n",
    "# Example usage\n",
    "concept_name = \"Semiotics\"\n",
    "\n",
    "# Format input\n",
    "inputs = tokenizer(\n",
    "    [instruction_prompt.format(concept_name)],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate output with modified parameters\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.get_vocab().get(\"<|eot_id|>\", tokenizer.eos_token_id),  # Use <|eot_id|> if available\n",
    "    min_length=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Optional: Print the full response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
