{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import subprocess\n",
    "import signal\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Configure logging\n",
    "ist = pytz.timezone('Asia/Kolkata')\n",
    "ist_time = datetime.now(ist)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'llm_evaluation_{ist_time.strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Constants\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODELS = [\n",
    "    # \"hf.co/bartowski/Qwen2.5-7B-Instruct-GGUF:Q5_K_L\",\n",
    "    # \"hf.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF:Q5_K_L\",\n",
    "    # \"hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:F16\",\n",
    "    # \"hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:F16\",\n",
    "    # \"hf.co/bartowski/Qwen2.5-14B-Instruct-GGUF:Q2_K\",\n",
    "    # \"hf.co/bartowski/Qwen2.5-3B-Instruct-GGUF:F16\",\n",
    "    # \"hf.co/bartowski/Qwen2.5-1.5B-Instruct-GGUF:F16\",\n",
    "    \"hf.co/bartowski/Phi-3-medium-128k-instruct-GGUF:Q3_K_S\",\n",
    "    \"hf.co/RichardErkhov/princeton-nlp_-_gemma-2-9b-it-SimPO-gguf:Q4_K_M\",\n",
    "]\n",
    "\n",
    "def load_questions() -> pd.DataFrame:\n",
    "    \"\"\"Load questions from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('/home/ubuntu/quantumLeap/quantumEval_claude.csv')\n",
    "        logging.info(f\"Successfully loaded {len(df)} questions from CSV\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading questions: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def start_ollama_model(model_name: str) -> subprocess.Popen:\n",
    "    \"\"\"Start an Ollama model.\"\"\"\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            ['ollama', 'run', model_name],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        time.sleep(10)  # Wait for model to initialize\n",
    "        logging.info(f\"Started model: {model_name}\")\n",
    "        return process\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error starting model {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def stop_ollama_model(process: subprocess.Popen):\n",
    "    \"\"\"Stop the Ollama model process.\"\"\"\n",
    "    if process:\n",
    "        process.send_signal(signal.SIGTERM)\n",
    "        process.wait()\n",
    "        logging.info(\"Stopped Ollama model\")\n",
    "\n",
    "def query_model(question: str, model_name: str) -> str:\n",
    "    \"\"\"Query the model and get response.\"\"\"\n",
    "    prompt = f\"Please answer the following question about Clotaire Rapaille's Culture Code methodology:\\n\\n{question}\\n\\nProvide a clear and concise answer based on Rapaille's work.\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error querying model {model_name}: {e}\")\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "def evaluate_models(questions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate all models on all questions.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model in MODELS:\n",
    "        logging.info(f\"Starting evaluation for model: {model}\")\n",
    "        \n",
    "        # Start the model\n",
    "        process = start_ollama_model(model)\n",
    "        if not process:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            for _, row in questions_df.iterrows():\n",
    "                logging.info(f\"Processing Question {row['Question Number']}\")\n",
    "                \n",
    "                response = query_model(row['Question Text'], model)\n",
    "                \n",
    "                result = {\n",
    "                    'Question Number': row['Question Number'],\n",
    "                    'Attribute': row['Attribute'],\n",
    "                    'Expertise Level': row['Expertise Level'],\n",
    "                    'Question Text': row['Question Text'],\n",
    "                    'LLM Name': model,\n",
    "                    'Model Response': response,\n",
    "                    'Score': '',  # To be filled later\n",
    "                    'Explanation': ''  # To be filled later\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                # Save intermediate results with IST timestamp\n",
    "                ist_time = datetime.now(ist)\n",
    "                pd.DataFrame(results).to_csv(\n",
    "                    f'/home/ubuntu/quantumLeap/data/eval_for_base_model_selection/temp/results_{ist_time.strftime(\"%Y%m%d_%H%M%S\")}.csv',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                time.sleep(2)  # Brief pause between questions\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during evaluation of model {model}: {e}\")\n",
    "        finally:\n",
    "            stop_ollama_model(process)\n",
    "            time.sleep(5)  # Wait before starting next model\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        # Load questions\n",
    "        questions_df = load_questions()\n",
    "        \n",
    "        # Run evaluations\n",
    "        results_df = evaluate_models(questions_df)\n",
    "        \n",
    "        # Save final results with IST timestamp\n",
    "        ist_time = datetime.now(ist)\n",
    "        output_file = f'/home/ubuntu/quantumLeap/data/eval_for_base_model_selection/final/final_results_{ist_time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Results saved to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/ubuntu/quantumLeap/data/eval_for_base_model_selection/final/final_results_20241106_104834_v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:API request failed: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "Error generating text: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "class OllamaAPIClient:\n",
    "    \"\"\"\n",
    "    A client for interacting with the Ollama API with proper error handling and response parsing.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\"):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        })\n",
    "        \n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    def generate_text(self, \n",
    "                     model: str, \n",
    "                     prompt: str, \n",
    "                     stream: bool = True,  # Changed default to True for streaming\n",
    "                     **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate text using the specified model.\n",
    "        \n",
    "        Args:\n",
    "            model: Name of the model to use\n",
    "            prompt: Input prompt for generation\n",
    "            stream: Whether to stream the response\n",
    "            **kwargs: Additional parameters to pass to the API\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing the response from the API\n",
    "        \"\"\"\n",
    "        endpoint = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(endpoint, json=payload, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Always use streaming for better token handling\n",
    "            return self._handle_streaming_response(response)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"API request failed: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _handle_streaming_response(self, response: requests.Response) -> Dict[str, Any]:\n",
    "        \"\"\"Handle streaming response from the API with improved token formatting.\"\"\"\n",
    "        combined_response = \"\"\n",
    "        current_line = \"\"\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    json_response = json.loads(line)\n",
    "                    if \"response\" in json_response:\n",
    "                        token = json_response[\"response\"]\n",
    "                        current_line += token\n",
    "                        # Print the token without newline and flush immediately\n",
    "                        print(token, end='', flush=True)\n",
    "                        combined_response += token\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    self.logger.warning(f\"Failed to parse streaming JSON: {e}\")\n",
    "                    continue\n",
    "                \n",
    "        # Print final newline\n",
    "        print()\n",
    "        return {\"response\": combined_response}\n",
    "\n",
    "def main():\n",
    "    client = OllamaAPIClient()\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    Envision a groundbreaking theory that unites Dr. Clotaire Rapaille's 'The Culture Code' \n",
    "    with advanced artificial intelligence and machine learning technologies to predict and \n",
    "    influence consumer behavior in the era of globalization and cultural hybridization.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nGenerating response...\\n\")\n",
    "        result = client.generate_text(\n",
    "            # model=\"hf.co/RichardErkhov/Qwen_-_Qwen2.5-72B-Instruct-gguf:Q3_K\",\n",
    "            model=\"hf.co/bartowski/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF:Q3_K_XL \",\n",
    "            prompt=prompt\n",
    "        )\n",
    "        # No need to print the result again since it's already printed during streaming\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Generating response...\n",
    "\n",
    "### The Culture-AI Fusion Theory: Predicting and Influencing Consumer Behavior in a Globalized World\n",
    "\n",
    "#### Introduction\n",
    "In an increasingly globalized world, understanding consumer behavior is more complex than ever. Dr. Clotaire Rapaille's \"The Culture Code\" provides valuable insights into how cultural archetypes and collective unconscious influence consumer decisions. However, the rapid evolution of artificial intelligence (AI) and machine learning (ML) technologies offers a new dimension to these insights, enabling us to predict and influence consumer behavior with unprecedented accuracy and nuance.\n",
    "\n",
    "#### The Core Idea\n",
    "The Culture-AI Fusion Theory integrates Dr. Rapaille's cultural codes with advanced AI and ML techniques to create a comprehensive model for understanding and predicting consumer behavior. This theory leverages the following components:\n",
    "\n",
    "1. **Cultural Archetypes**: Drawing from Dr. Rapaille's work, we identify and codify cultural archetypes that are deeply embedded in various societies. These archetypes represent fundamental beliefs, values, and behaviors that shape consumer decisions.\n",
    "\n",
    "2. **Data Collection and Analysis**: Advanced AI and ML algorithms process vast amounts of data from diverse sources, including social media, online transactions, and cultural content (e.g., literature, films). This data is used to identify patterns and trends in consumer behavior across different cultures.\n",
    "\n",
    "3. **Predictive Modeling**: Machine learning models are trained to predict consumer behavior based on the identified cultural archetypes and real-time data. These models can forecast how consumers from different cultural backgrounds will respond to various marketing strategies, product designs, and brand messages.\n",
    "\n",
    "4. **Behavioral Influence**: By understanding the underlying cultural codes, AI systems can generate personalized marketing content that resonates with specific cultural values and preferences. This approach not only enhances consumer engagement but also builds trust and loyalty.\n",
    "\n",
    "#### Key Components of the Theory\n",
    "\n",
    "1. **Cultural Code Identification**\n",
    "   - **Archetype Extraction**: Using natural language processing (NLP) and sentiment analysis, AI algorithms extract key archetypes from cultural content.\n",
    "   - **Code Mapping**: Cultural codes are mapped to specific consumer behaviors and preferences using a combination of supervised and unsupervised learning techniques.\n",
    "\n",
    "2. **Data Integration and Processing**\n",
    "   - **Multisource Data Collection**: Data is collected from various sources, including social media platforms, e-commerce websites, and cultural databases.\n",
    "   - **Real-time Data Analysis**: AI models process real-time data to capture the dynamic nature of consumer behavior in a globalized context.\n",
    "\n",
    "3. **Predictive Modeling**\n",
    "   - **Behavioral Prediction**: Machine learning algorithms predict how consumers from different cultural backgrounds will respond to marketing strategies and product offerings.\n",
    "   - **Scenario Simulation**: Simulate various marketing scenarios to identify the most effective strategies for each cultural group.\n",
    "\n",
    "4. **Behavioral Influence**\n",
    "   - **Personalized Marketing**: AI systems generate personalized content that aligns with specific cultural codes, enhancing consumer engagement and satisfaction.\n",
    "   - **Cultural Sensitivity**: Ensure that marketing messages are culturally sensitive and respectful, building trust and loyalty among diverse consumer groups.\n",
    "\n",
    "#### Application Examples\n",
    "\n",
    "1. **Global Branding Strategies**\n",
    "   - **Cultural Code Alignment**: A global brand can use AI to align its messaging with the cultural codes of different regions, ensuring that its branding resonates with local consumers.\n",
    "   - **Adaptive Marketing Campaigns**: AI-driven campaigns adapt in real-time based on consumer feedback and cultural trends.\n",
    "\n",
    "2. **Product Design**\n",
    "   - **Culturally Relevant Designs**: AI can help design products that are culturally relevant and appealing to specific markets, reducing the risk of market failure.\n",
    "   - **User-Centric Innovations**: Machine learning algorithms can identify emerging cultural trends and incorporate them into product development.\n",
    "\n",
    "3. **Customer Experience**\n",
    "   - **Personalized Customer Service**: AI-powered chatbots and customer service systems can provide personalized assistance that aligns with cultural expectations.\n",
    "   - **Cultural Training for Employees**: AI can help train employees on cultural sensitivity, improving the overall customer experience.\n",
    "\n",
    "#### Conclusion\n",
    "The Culture-AI Fusion Theory represents a revolutionary approach to understanding and influencing consumer behavior in a globalized world. By combining Dr. Clotaire Rapaille's insights into cultural codes with advanced AI and ML technologies, businesses can gain a deeper understanding of their consumers and create more effective marketing strategies. This theory not only enhances consumer engagement and satisfaction but also fosters cultural sensitivity and respect, building stronger relationships between brands and diverse consumer groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
