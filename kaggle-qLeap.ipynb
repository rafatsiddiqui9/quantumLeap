{"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import subprocess\n\n# # Install required packages in terminal only. Otherwise jupyter kernel will die and remote server will crash\n# packages = [\n#     \"unsloth\",\n#     \"xformers\",\n#     \"torch\",\n#     \"nltk\",\n#     \"spacy\",\n#     \"wandb\",\n#     \"datasets\",\n#     \"huggingface_hub\"\n# ]\n\n# for package in packages:\n#     subprocess.run([\"pip\", \"install\", \"-q\", \"-U\", package, \"--no-cache-dir\"])","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip show unsloth","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport torch\nimport nltk\nimport spacy\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom datasets import Dataset\nimport logging\nimport argparse\nimport wandb  # Weights & Biases integration\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 1: Install and Setup Libraries\n# ----------------------------- #\n\n# Ensure NLTK's punkt tokenizer is available\nnltk.download('punkt')\n\n# Initialize spaCy English model\ntry:\n    nlp = spacy.load('en_core_web_sm')\nexcept OSError:\n    print(\"SpaCy English model not found. Downloading...\")\n    os.system('python -m spacy download en_core_web_sm')\n    nlp = spacy.load('en_core_web_sm')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Load and Clean the Text Data\n# ----------------------------- #\n\ndef load_and_clean_text(file_path):\n    \"\"\"\n    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        text = f.read()\n    # # Remove Project Gutenberg's license text and headers/footers\n    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n\n    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n    return text.strip()\n\n# Replace 'psychology_of_unconscious.txt' with your actual file path\nfile_path = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/psychology_of_unconscious.txt'\nclean_text = load_and_clean_text(file_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 3: Parse Text into Discourse Units\n# ----------------------------- #\n\ndef parse_discourse_units(text):\n    \"\"\"\n    Parses text into discourse units using spaCy.\n    Currently splits text into sentences.\n    \"\"\"\n    paragraphs = text.split('\\n\\n')\n    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n    \n    discourse_units = []\n    for para in paragraphs:\n        doc = nlp(para)\n        sentences = [sent.text for sent in doc.sents]\n        discourse_units.extend(sentences)\n    return discourse_units\n\ndiscourse_units = parse_discourse_units(clean_text)\n\n# Save discourse_units to a file (Optional)\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.txt', 'w') as f:\n    for unit in discourse_units:\n        f.write(unit + '\\n')\n\n# If you need to reload from file (Optional)\n# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.txt', 'r') as f:\n#     discourse_units = f.read().splitlines()\n\nlen(discourse_units)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 4: Create Chunks Using Hybrid Strategy\n# ----------------------------- #\n\ndef create_chunks(discourse_units, tokenizer, max_length=2048, overlap_size=100):\n    \"\"\"\n    Creates chunks from discourse units using a sliding window with overlapping chunks.\n    \"\"\"\n    chunks = []\n    current_chunk = []\n    current_length = 0\n\n    for unit in discourse_units:\n        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n        unit_length = len(unit_tokens)\n\n        if current_length + unit_length <= max_length:\n            current_chunk.append(unit)\n            current_length += unit_length\n        else:\n            # Append the current chunk\n            chunks.append(' '.join(current_chunk))\n            # Create overlap\n            overlap_text = ' '.join(current_chunk)[-overlap_size:]\n            overlap_tokens = tokenizer.encode(overlap_text, add_special_tokens=False)\n            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n            # Start new chunk with overlap and current unit\n            current_chunk = [overlap_text, unit]\n            current_length = len(tokenizer.encode(overlap_text, add_special_tokens=False)) + unit_length\n\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n\n    return chunks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Load the Tokenizer and Model\n# ----------------------------- #\n\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# if the model is already downloaded, then don't download it again; otherwise download it\nimport os\n\nmodel_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\nmodels_dir = os.path.join(os.path.dirname(os.getcwd()), \"models\")\nmodel_path = os.path.join(models_dir, model_name)\n\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n\nif os.path.exists(model_path):\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_path,\n        max_seq_length=max_seq_length,\n        dtype=dtype,\n        load_in_4bit=load_in_4bit,\n    )\nelse:\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name,\n        max_seq_length=max_seq_length,\n        dtype=dtype,\n        load_in_4bit=load_in_4bit,\n        token=\"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\",\n    )\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\n\n                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n    lora_alpha = 32,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = True,   # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# ----------------------------- #\n# Part 6: Create Chunks (After Tokenizer is Loaded)\n# ----------------------------- #\n\nchunks = create_chunks(discourse_units, tokenizer, max_length=2048, overlap_size=100)\n\n# Save chunks to a file (Optional)\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.txt', 'w') as f:\n    for unit in discourse_units:\n        f.write(unit + '\\n')\n\n# If you need to reload from file (Optional)\n# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.txt', 'r') as f:\n#     discourse_units = f.read().splitlines()\n\nlen(chunks)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 7: Create and Tokenize Dataset\n# ----------------------------- #\n\n# Create a Dataset object from chunks\n\nbook_title = 'Psychology of the Unconscious by C. G. Jung'\nwikipedia_prompt = \"\"\"\nPsychology Book\n\n### Title: {}\n\n### Article: {}\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    titles = book_title\n    texts  = examples[\"text\"]\n    outputs = []\n    for title, text in zip([book_title]*len(chunks), texts):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n        outputs.append(text)\n    return { \"text\" : outputs, }\npass\n\n# convert chunks variable to huggingface dataset\n\nfrom datasets import Dataset\n\ndataset = Dataset.from_dict({\"text\": chunks})\n\ndataset = dataset.train_test_split(train_size = 0.90)[\"train\"]\n\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n\ndataset\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 8: Configure Training Arguments\n# ----------------------------- #\n\n\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Use warmup_ratio and num_train_epochs for longer runs!\n        max_steps = 120,\n        warmup_steps = 10,\n        # warmup_ratio = 0.1,\n        # num_train_epochs = 1,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = 5e-5,\n        embedding_learning_rate = 1e-5,\n\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 9: Define Compute Metrics Function\n# ----------------------------- #\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Computes perplexity based on model predictions and labels.\n    \"\"\"\n    logits, labels = eval_pred\n    # Convert to torch tensors\n    logits = torch.tensor(logits)\n    labels = torch.tensor(labels)\n    \n    # Ensure shapes match\n    if logits.shape[:2] != labels.shape:\n        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n    \n    # Shift logits and labels\n    shift_logits = logits[:, :-1, :].contiguous()\n    shift_labels = labels[:, 1:].contiguous()\n\n    # Check label values\n    if shift_labels.max() >= model.config.vocab_size:\n        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n    \n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    perplexity = torch.exp(loss).item()\n    return {\"perplexity\": perplexity}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 10: Initialize the Trainer\n# ----------------------------- #\n\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n    level=logging.INFO,  # Set to DEBUG for more detailed logs\n)\nlogger = logging.getLogger(__name__)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n    compute_metrics=compute_metrics,\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 11: Start Training\n# ----------------------------- #\n\n#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add current timestamp to model name\nmodel.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\ntokenizer.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\")\nmodel.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_base_{int(time.time())}\", token = \"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\") # Online saving\ntokenizer.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_{int(time.time())}\", token = \"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\") # Online saving\n\n# # Merge to 16bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n\n# # Merge to 4bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n\n# # Just LoRA adapters\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n    \n    \n# # Save to 8bit Q8_0\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n\n# # Save to 16bit GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n\n# # Save to q4_k_m GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instruction  Tuning","metadata":{}},{"cell_type":"code","source":"\n# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n\nimport json\nimport ast\nimport logging\n\nimport csv\n\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    data = list(reader)\n    \ntype(data)\n\n\n# Configure logging\nlogging.basicConfig(\n    filename='transformation_errors.log',\n    filemode='w',\n    level=logging.ERROR,\n    format='%(levelname)s:%(message)s'\n)\n\n# Sample original data\noriginal_data = data\n\ndef transform_data(original_data):\n    \"\"\"\n    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n\n    Parameters:\n        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n\n    Returns:\n        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n    \"\"\"\n    new_data = []\n\n    for idx, entry in enumerate(original_data, start=1):\n        concept_name = entry.get('concept_name', '').strip()\n        detailed_explanation = entry.get('detailed_explanation', '').strip()\n        example_scenario_str = entry.get('example_scenario', '').strip()\n\n        if not concept_name or not detailed_explanation or not example_scenario_str:\n            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n            continue\n\n        # Attempt to parse with json.loads\n        try:\n            example_scenarios = json.loads(example_scenario_str)\n            if not isinstance(example_scenarios, list):\n                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n        except json.JSONDecodeError:\n            # Fallback to ast.literal_eval\n            try:\n                example_scenarios = ast.literal_eval(example_scenario_str)\n                if not isinstance(example_scenarios, list):\n                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n            except (ValueError, SyntaxError) as e:\n                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n                continue\n\n        # Iterate through each scenario and create a new entry\n        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n            if not isinstance(scenario, str):\n                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n                continue\n\n            new_entry = {\n                'concept_name': concept_name,\n                'detailed_explanation': detailed_explanation,\n                'example_scenario': scenario.strip()\n            }\n            new_data.append(new_entry)\n\n    return new_data\n\n# Transform the data\ntransformed_data = transform_data(original_data)\n\n# Optional: Save the transformed data to a JSON file\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n\nprint(f\"Transformation complete. {len(transformed_data)} entries created.\")\nprint(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n\nprint(len(transformed_data))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef instruction_prompt_func(examples):\n    concept_name = examples[\"concept_name\"]\n    detailed_explanation = examples[\"detailed_explanation\"]\n    example_scenario = examples[\"example_scenario\"]\n    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\npass\n\n\n# convert transformed_data to a huggingface dataset\ninstruction_dataset = Dataset.from_dict(transformed_data)\ninstruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = instruction_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 8,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Use num_train_epochs and warmup_ratio for longer runs!\n        max_steps = 120,\n        warmup_steps = 10,\n        # warmup_ratio = 0.1,\n        # num_train_epochs = 1,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = 5e-5,\n        embedding_learning_rate = 1e-5,\n\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.00,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add current timestamp to model name\nmodel.save_pretrained(f\"qLeap_model_v0_{int(time.time())}\") # Local saving\ntokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\nmodel.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\ntokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n\n# # Merge to 16bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n\n# # Merge to 4bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n\n# # Just LoRA adapters\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n    \n    \n# # Save to 8bit Q8_0\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n\n# # Save to 16bit GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n\n# # Save to q4_k_m GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{},"execution_count":null,"outputs":[]}]}