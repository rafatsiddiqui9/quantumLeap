{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Git clone qLeap-fft repo inside `/root/` directory\n","## Ensure to have the latest branch\n","## Switch to quantumLeap directory"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","\n","# Set these environment variables before importing torch-related modules\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","\n","from pathlib import Path\n","\n","def ensure_working_directory():\n","    \"\"\"\n","    Check if we're in the correct working directory, if not switch to it.\n","    Creates the directory if it doesn't exist.\n","    \"\"\"\n","    target_dir = '/home/ubuntu/quantumLeap'\n","    current_dir = os.getcwd()\n","    \n","    # Print current directory\n","    print(f\"Current directory: {current_dir}\")\n","    \n","    # Check if we need to switch directories\n","    if current_dir != target_dir:\n","        # Create directory if it doesn't exist\n","        Path(target_dir).mkdir(parents=True, exist_ok=True)\n","        \n","        try:\n","            # Change to target directory\n","            os.chdir(target_dir)\n","            print(f\"Successfully switched to: {target_dir}\")\n","        except Exception as e:\n","            print(f\"Error switching to directory: {str(e)}\")\n","            raise\n","    else:\n","        print(\"Already in correct directory\")\n","    \n","    # Verify current directory\n","    print(f\"Working directory: {os.getcwd()}\")\n","\n","# Call the function before your main code\n","ensure_working_directory()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ----------------------------- #\n","# Part 1.1: Install and Setup Libraries - for Ola Krutrim Cloud Instance\n","# ----------------------------- #\n","\n","# # if executing below in terminal with virtual env, do not need to add --system tag\n","# pip install uv #install this in the virtual environment where you want to execute the notebook.\n","# pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121 # as on 07Nov2024, xformers is compatible with torch=2.4.0 only; uv doesnt work for installing torch\n","# uv pip install packaging ninja\n","# uv pip install flash-attn --no-build-isolation\n","# uv pip install unsloth\n","# python -m xformers.info\n","# uv pip install wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets tqdm Iprogress ipywidgets python-dotenv tensorboard -q\n","\n","# # restart once you have installed all of the above\n","# !nvidia-smi\n","# !nvcc --version\n","# import torch\n","# print(torch.__version__)          # Should reflect 2.5.0+cu124\n","# print(torch.version.cuda)         # Should output 12.4\n","# print(torch.cuda.is_available())  # Should return True"]},{"cell_type":"markdown","metadata":{},"source":["# Restart again so that all the libraries are properly initialized"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:33.365845Z","iopub.status.busy":"2024-11-04T08:49:33.365559Z","iopub.status.idle":"2024-11-04T08:49:47.786343Z","shell.execute_reply":"2024-11-04T08:49:47.785092Z","shell.execute_reply.started":"2024-11-04T08:49:33.365811Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 1.2: Import Necessary Libraries\n","# ----------------------------- #\n","\n","# General Libraries\n","import os\n","import json\n","import sys\n","import subprocess\n","import argparse\n","import logging\n","import math\n","import random\n","from datetime import datetime\n","import re\n","import gc\n","import weakref\n","import multiprocessing\n","\n","# Torch related\n","import torch\n","from torch import nn\n","import torch.distributed as dist\n","\n","# Transformers related\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForLanguageModeling,\n","    Adafactor\n",")\n","\n","# Huggingface TRL for full finetune\n","from trl import SFTTrainer, SFTConfig\n","\n","# General huggingface libraries\n","import huggingface_hub\n","from datasets import load_dataset, Dataset\n","from accelerate import Accelerator\n","\n","\n","# Unsloth specificic libraries\n","import unsloth\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n","\n","# Other Libraries\n","from peft import LoraConfig\n","import wandb\n","import nltk\n","import spacy\n","# from galore_torch import GaLoreAdamW, GaLoreAdafactor, GaLoreAdamW8bit\n","\n","# Check and import NLTK and spacy modules\n","# Ensure NLTK's punkt tokenizer is available\n","import nltk\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","    print('punkt was already available.')\n","except LookupError:\n","    nltk.download('punkt')\n","    print('punkt was not available. It has been downloaded')\n","\n","# Initialize spaCy English model\n","try:\n","    nlp = spacy.load('en_core_web_sm')\n","    print('en_core_web_sm was already available.')\n","except OSError:\n","    print(\"SpaCy English model not found. Downloading...\")\n","    os.system('python -m spacy download en_core_web_sm')\n","    nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.535557Z","iopub.status.busy":"2024-11-04T08:49:54.535142Z","iopub.status.idle":"2024-11-04T08:49:54.549789Z","shell.execute_reply":"2024-11-04T08:49:54.549048Z","shell.execute_reply.started":"2024-11-04T08:49:54.535509Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 2: Load and Clean the Text Data\n","# ----------------------------- #\n","\n","def load_and_clean_text(file_path):\n","    \"\"\"\n","    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n","    \"\"\"\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    # # Remove Project Gutenberg's license text and headers/footers\n","    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","\n","    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n","    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n","    return text.strip()\n","\n","# Replace 'psychology_of_unconscious.txt' with your actual file path\n","file_path = '/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'\n","clean_text = load_and_clean_text(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install t"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tiktoken\n","from openai import OpenAI\n","import json\n","import time\n","from typing import List, Dict, Tuple, Optional\n","import numpy as np\n","import os\n","from datetime import datetime\n","from pprint import pprint\n","import re\n","from dataclasses import dataclass\n","from enum import Enum\n","\n","# Add these new data structures after imports\n","class SectionType(Enum):\n","    HEADER = \"header\"\n","    CONTENT = \"content\"\n","    QUOTE = \"quote\"\n","    ATTRIBUTION = \"attribution\"\n","    LIST = \"list\"\n","    FRONT_MATTER = \"front_matter\"\n","    TABLE_OF_CONTENTS = \"table_of_contents\"\n","    \n","@dataclass\n","class Section:\n","    text: str\n","    type: SectionType\n","    level: int = 0\n","    metadata: Dict = None\n","    \n","\n","class SemanticChunker:\n","    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n","        \"\"\"Initialize the semantic chunker with model configuration\"\"\"\n","        self.client = OpenAI(\n","            base_url=\"http://localhost:8000/v1\",\n","            api_key=\"dummy\"\n","        )\n","        self.model_name = model_name\n","        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n","        self.max_tokens = 3000\n","        \n","        # Set up logging directory with timestamp\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        self.log_dir = f\"/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/Psychology_Of_Unconscious_Mind/chunks/chunks_{timestamp}\"\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        \n","        # Set up logging file for processing summary\n","        self.log_file = os.path.join(self.log_dir, \"processing_log.txt\")\n","        \n","        # Initialize state variables\n","        self.missed_text = \"\"  # Store text not included in LLM output\n","        \n","    def log_message(self, message: str):\n","        \"\"\"Write log message with timestamp and print to console\"\"\"\n","        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        log_entry = f\"[{timestamp}] {message}\"\n","        with open(self.log_file, 'a', encoding='utf-8') as f:\n","            f.write(log_entry + \"\\n\")\n","        print(log_entry)\n","    \n","    def print_separator(self, message: str = \"\"):\n","        \"\"\"Print a separator line with optional message\"\"\"\n","        print(f\"\\n{'='*100}\")\n","        if message:\n","            print(f\"{message}\")\n","            print('='*100)\n","    \n","    def count_tokens(self, text: str) -> int:\n","        \"\"\"Count tokens in text using tiktoken\"\"\"\n","        return len(self.encoding.encode(text))\n","    \n","    def find_chapter_breaks(self, text: str) -> List[int]:\n","        \"\"\"Find indices where chapters begin (centered headings)\"\"\"\n","        lines = text.split('\\n')\n","        chapter_breaks = []\n","        \n","        for i, line in enumerate(lines):\n","            if self.is_chapter_heading(line):\n","                chapter_breaks.append(i)\n","        \n","        return chapter_breaks\n","    \n","    def is_chapter_heading(self, text: str) -> Tuple[bool, int]:\n","        \"\"\"\n","        Enhanced chapter heading detection with level identification.\n","        Returns (is_heading, level).\n","        \"\"\"\n","        text = text.strip()\n","        if not text:\n","            return False, 0\n","            \n","        # Chapter patterns\n","        chapter_patterns = [\n","            (r'^CHAPTER\\s+[IVXL]+', 1),  # Main chapter headers\n","            (r'^[IVX]+\\.\\s*—\\s*', 2),    # Sub-chapter headers\n","            (r'^\\d+\\.\\s*—\\s*', 2),       # Numbered sections\n","        ]\n","        \n","        for pattern, level in chapter_patterns:\n","            if re.match(pattern, text, re.I):\n","                return True, level\n","        \n","        # Check for centered text formatting\n","        line_length = len(text)\n","        leading_spaces = len(text) - len(text.lstrip())\n","        trailing_spaces = len(text) - len(text.rstrip())\n","        \n","        is_centered = abs(leading_spaces - trailing_spaces) <= 2 and leading_spaces > 5\n","        is_caps = text.isupper()\n","        reasonable_length = 10 < len(text.strip()) < 100\n","        \n","        if is_centered:\n","            if is_caps and reasonable_length:\n","                return True, 1  # Main header\n","            elif reasonable_length:\n","                return True, 2  # Sub header\n","                \n","        return False, 0\n","    \n","    def analyze_text_structure(self, text: str) -> List[Section]:\n","        \"\"\"\n","        Analyze text structure and break it into typed sections.\n","        \"\"\"\n","        sections = []\n","        lines = text.split('\\n')\n","        current_section = []\n","        current_type = None\n","        current_level = 0\n","        \n","        def flush_section():\n","            nonlocal current_section, current_type\n","            if current_section:\n","                sections.append(Section(\n","                    text='\\n'.join(current_section),\n","                    type=current_type or SectionType.CONTENT,\n","                    level=current_level\n","                ))\n","                current_section = []\n","                current_type = None\n","        \n","        in_toc = False\n","        in_front_matter = False\n","        \n","        for i, line in enumerate(lines):\n","            # Detect Table of Contents\n","            if re.match(r'^\\s*CONTENTS\\s*$', line, re.I):\n","                flush_section()\n","                in_toc = True\n","                current_type = SectionType.TABLE_OF_CONTENTS\n","                continue\n","                \n","            # Detect Front Matter\n","            if re.match(r'^\\s*AUTHOR\\'S\\s+NOTE\\s*$', line, re.I):\n","                flush_section()\n","                in_front_matter = True\n","                current_type = SectionType.FRONT_MATTER\n","                continue\n","                \n","            # Check for section transitions\n","            is_heading, level = self.is_chapter_heading(line)\n","            if is_heading:\n","                flush_section()\n","                current_section = [line]\n","                current_type = SectionType.HEADER\n","                current_level = level\n","                continue\n","                \n","            # Detect quotes\n","            if line.startswith('\"') and len(line) > 50:\n","                flush_section()\n","                current_type = SectionType.QUOTE\n","                \n","            # Detect attributions\n","            if re.match(r'^\\s*—\\s*[A-Z]', line):\n","                flush_section()\n","                current_type = SectionType.ATTRIBUTION\n","                \n","            # Detect lists\n","            if re.match(r'^\\s{4,}(?:[\\w\\-]+\\.|\\-|\\*)\\s', line):\n","                if current_type != SectionType.LIST:\n","                    flush_section()\n","                    current_type = SectionType.LIST\n","                    \n","            current_section.append(line)\n","            \n","            # Handle section transitions\n","            if in_toc and not line.strip() and i < len(lines)-1 and lines[i+1].strip():\n","                in_toc = False\n","                flush_section()\n","                \n","            if in_front_matter and not line.strip() and i < len(lines)-1 and lines[i+1].strip():\n","                in_front_matter = False\n","                flush_section()\n","        \n","        flush_section()  # Flush any remaining content\n","        return sections\n","    \n","    \n","    def get_complete_paragraphs(self, text: str, max_tokens: int) -> Tuple[str, str]:\n","        \"\"\"\n","        Enhanced version that respects document structure with optimization.\n","        \"\"\"\n","        # Add debug logging\n","        self.log_message(f\"Starting get_complete_paragraphs with {len(text)} chars of text\")\n","        \n","        # First, analyze the structure\n","        sections = self.analyze_text_structure(text)\n","        self.log_message(f\"Found {len(sections)} sections\")\n","        \n","        current_sections = []\n","        current_tokens = 0\n","    def analyze_text_structure(self, text: str) -> List[Section]:\n","        \"\"\"\n","        Analyze text structure and break it into typed sections.\n","        \"\"\"\n","        sections = []\n","        lines = text.split('\\n')\n","        current_section = []\n","        current_type = None\n","        current_level = 0\n","        \n","        def flush_section():\n","            nonlocal current_section, current_type\n","            if current_section:\n","                sections.append(Section(\n","                    text='\\n'.join(current_section),\n","                    type=current_type or SectionType.CONTENT,\n","                    level=current_level\n","                ))\n","                current_section = []\n","                current_type = None\n","        \n","        in_toc = False\n","        in_front_matter = False\n","        \n","        for i, line in enumerate(lines):\n","            # Detect Table of Contents\n","            if re.match(r'^\\s*CONTENTS\\s*$', line, re.I):\n","                flush_section()\n","                in_toc = True\n","                current_type = SectionType.TABLE_OF_CONTENTS\n","                continue\n","                \n","            # Detect Front Matter\n","            if re.match(r'^\\s*AUTHOR\\'S\\s+NOTE\\s*$', line, re.I):\n","                flush_section()\n","                in_front_matter = True\n","                current_type = SectionType.FRONT_MATTER\n","                continue\n","                \n","            # Check for section transitions\n","            is_heading, level = self.is_chapter_heading(line)\n","            if is_heading:\n","                flush_section()\n","                current_section = [line]\n","                current_type = SectionType.HEADER\n","                current_level = level\n","                continue\n","                \n","            # Detect quotes\n","            if line.startswith('\"') and len(line) > 50:\n","                flush_section()\n","                current_type = SectionType.QUOTE\n","                \n","            # Detect attributions\n","            if re.match(r'^\\s*—\\s*[A-Z]', line):\n","                flush_section()\n","                current_type = SectionType.ATTRIBUTION\n","                \n","            # Detect lists\n","            if re.match(r'^\\s{4,}(?:[\\w\\-]+\\.|\\-|\\*)\\s', line):\n","                if current_type != SectionType.LIST:\n","                    flush_section()\n","                    current_type = SectionType.LIST\n","                    \n","            current_section.append(line)\n","            \n","            # Handle section transitions\n","            if in_toc and not line.strip() and i < len(lines)-1 and lines[i+1].strip():\n","                in_toc = False\n","                flush_section()\n","                \n","            if in_front_matter and not line.strip() and i < len(lines)-1 and lines[i+1].strip():\n","                in_front_matter = False\n","                flush_section()\n","        \n","        flush_section()  # Flush any remaining content\n","        return sections\n","    \n","    \n","    def get_complete_paragraphs(self, text: str, max_tokens: int) -> Tuple[str, str]:\n","        \"\"\"\n","        Enhanced version that respects document structure with optimization.\n","        \"\"\"\n","        # Add debug logging\n","        self.log_message(f\"Starting get_complete_paragraphs with {len(text)} chars of text\")\n","        \n","        # First, analyze the structure\n","        sections = self.analyze_text_structure(text)\n","        self.log_message(f\"Found {len(sections)} sections\")\n","        \n","        current_sections = []\n","        current_tokens = 0\n","    def analyze_text_structure(self, text: str) -> List[Section]:\n","        \"\"\"\n","        Analyze text structure and break it into typed sections.\n","        \"\"\"\n","        sections = []\n","        lines = text.split('\\n')\n","        current_section = []\n","        current_type = None\n","        current_level = 0\n","        \n","        def flush_section():\n","            nonlocal current_section, current_type\n","            if current_section:\n","                sections.append(Section(\n","                    text='\\n'.join(current_section),\n","                    type=current_type or SectionType.CONTENT,\n","                    level=current_level\n","                ))\n","                current_section = []\n","                current_type = None\n","        \n","        in_toc = False\n","        in_front_matter = False\n","        \n","        for i, line in enumerate(lines):\n","            # Detect Table of Contents\n","            if re.match(r'^\\s*CONTENTS\\s*$', line, re.I):\n","                flush_section()\n","                in_toc = True\n","                current_type = SectionType.TABLE_OF_CONTENTS\n","                continue\n","                \n","            # Detect Front Matter\n","            if re.match(r'^\\s*AUTHOR\\'S\\s+NOTE\\s*$', line, re.I):\n","                flush_section()\n","                in_front_matter = True\n","                current_type = SectionType.FRONT_MATTER\n","                continue\n","                \n","            # Check for section transitions\n","            is_heading, level = self.is_chapter_heading(line)\n","            if is_heading:\n","                flush_section()\n","                current_section = [line]\n","                current_type = SectionType.HEADER\n","                current_level = level\n","                continue\n","                \n","            # Detect quotes\n","            if line.startswith('\"') and len(line) > 50:\n","                flush_section()\n","                current_type = SectionType.QUOTE\n","                \n","            # Detect attributions\n","            if re.match(r'^\\s*—\\s*[A-Z]', line):\n","                flush_section()\n","                current_type = SectionType.ATTRIBUTION\n","                \n","            # Detect lists\n","            if re.match(r'^\\s{4,}(?:[\\w\\-]+\\.|\\-|\\*)\\s', line):\n","                if current_type != SectionType.LIST:\n","                    flush_section()\n","                    current_type = SectionType.LIST\n","                    \n","            current_section.append(line)\n","            \n","            # Handle section transitions\n","            if in_toc and not line.strip() and i < len(lines)-1 and lines[i+1].strip():\n","                in_toc = False\n","                flush_section()\n","                \n","            if in_front_matter and not line.strip() and i < len(lines)-1 and lines[i+1].strip():\n","                in_front_matter = False\n","                flush_section()\n","        \n","        flush_section()  # Flush any remaining content\n","        return sections\n","    \n","    \n","    def get_complete_paragraphs(self, text: str, max_tokens: int) -> Tuple[str, str]:\n","        \"\"\"\n","        Enhanced version that respects document structure with optimization.\n","        \"\"\"\n","        # Add debug logging\n","        self.log_message(f\"Starting get_complete_paragraphs with {len(text)} chars of text\")\n","        \n","        # First, analyze the structure\n","        sections = self.analyze_text_structure(text)\n","        self.log_message(f\"Found {len(sections)} sections\")\n","        \n","        current_sections = []\n","        current_tokens = 0\n","    def analyze_text_structure(self, text: str) -> List[Section]:\n","        \"\"\"\n","        Analyze text structure and break it into typed sections.\n","        \"\"\"\n","        sections = []\n","        lines = text.split('\\n')\n","        current_section = []\n","        current_type = None\n","        current_level = 0\n","        \n","        def flush_section():\n","            nonlocal current_section, current_type\n","            if current_section:\n","                sections.append(Section(\n","                    text='\\n'.join(current_section),\n","                    type=current_type or SectionType.CONTENT,\n","                    level=current_level\n","                ))\n","                current_section = []\n","                current_type = None\n","        \n","        in_toc = False\n","        in_front_matter = False\n","        \n","        for i, line in enumerate(lines):\n","            # Detect Table of Contents\n","            if re.match(r'^\\s*CONTENTS\\s*$', line, re.I):\n","                flush_section()\n","                in_toc = True\n","                current_type = SectionType.TABLE_OF_CONTENTS\n","                continue\n","                \n","            # Detect Front Matter\n","            if re.match(r'^\\s*AUTHOR\\'S\\s+NOTE\\s*$', line, re.I):\n","                flush_section()\n","                in_front_matter = True\n","                current_type = SectionType.FRONT_MATTER\n","                continue\n","                \n","            # Check for section transitions\n","            is_heading, level = self.is_chapter_heading(line)\n","            if is_heading:\n","                flush_section()\n","                current_section = [line]\n","                current_type = SectionType.HEADER\n","                current_level = level\n","                continue\n","                \n","            # Detect quotes\n","            if line.startswith('\"') and len(line) > 50:\n","                flush_section()\n","                current_type = SectionType.QUOTE\n","                \n","            # Detect attributions\n","            if re.match(r'^\\s*—\\s*[A-Z]', line):\n","                flush_section()\n","                current_type = SectionType.ATTRIBUTION\n","                \n","            # Detect lists\n","            if re.match(r'^\\s{4,}(?:[\\w\\-]+\\.|\\-|\\*)\\s', line):\n","                if current_type != SectionType.LIST:\n","                    flush_section()\n","                    current_type = SectionType.LIST\n","                    \n","            current_section.append(line)\n","            \n","            # Handle section transitions\n","            if in_toc and not line.strip() and i < len(lines)-1 and lines[i+1].strip():\n","                in_toc = False\n","                flush_section()\n","                \n","            if in_front_matter and not line.strip() and i < len(lines)-1 and lines[i+1].strip():\n","                in_front_matter = False\n","                flush_section()\n","        \n","        flush_section()  # Flush any remaining content\n","        return sections\n","    \n","    def analyze_text_structure(self, text: str) -> List[Section]:\n","        \"\"\"\n","        Enhanced text structure analysis with better header and spacing detection.\n","        \"\"\"\n","        sections = []\n","        lines = text.split('\\n')\n","        current_section = []\n","        current_type = None\n","        current_level = 0\n","        \n","        def flush_section():\n","            nonlocal current_section, current_type\n","            if current_section:\n","                # Skip empty sections\n","                content = '\\n'.join(current_section).strip()\n","                if content:  # Only create section if there's actual content\n","                    sections.append(Section(\n","                        text='\\n'.join(current_section),\n","                        type=current_type or SectionType.CONTENT,\n","                        level=current_level\n","                    ))\n","                current_section = []\n","                current_type = None\n","        \n","        in_toc = False\n","        in_front_matter = False\n","        \n","        i = 0\n","        while i < len(lines):\n","            line = lines[i]\n","            next_line = lines[i + 1] if i + 1 < len(lines) else \"\"\n","            \n","            # Detect centered headers\n","            if line.strip() and line.strip().isupper():\n","                leading_spaces = len(line) - len(line.lstrip())\n","                if leading_spaces > 10:  # Likely centered\n","                    flush_section()\n","                    current_type = SectionType.HEADER\n","                    current_level = 1\n","                    current_section = [line]\n","                    if not next_line.strip():  # Include following blank line\n","                        current_section.append(next_line)\n","                        i += 1\n","                    flush_section()\n","                    i += 1\n","                    continue\n","            \n","            # Detect Table of Contents\n","            if re.match(r'^\\s*CONTENTS\\s*$', line, re.I):\n","                flush_section()\n","                in_toc = True\n","                current_type = SectionType.TABLE_OF_CONTENTS\n","                current_section = [line]\n","                if not next_line.strip():  # Include following blank line\n","                    current_section.append(next_line)\n","                    i += 1\n","                i += 1\n","                continue\n","            \n","            # Detect Author's Note\n","            if re.match(r'^\\s*AUTHOR\\'S\\s+NOTE\\s*$', line, re.I):\n","                flush_section()\n","                in_front_matter = True\n","                current_type = SectionType.FRONT_MATTER\n","                current_section = [line]\n","                if not next_line.strip():  # Include following blank line\n","                    current_section.append(next_line)\n","                    i += 1\n","                i += 1\n","                continue\n","            \n","            # Detect chapter headings\n","            is_heading, level = self.is_chapter_heading(line)\n","            if is_heading:\n","                flush_section()\n","                current_type = SectionType.HEADER\n","                current_level = level\n","                current_section = [line]\n","                if not next_line.strip():  # Include following blank line\n","                    current_section.append(next_line)\n","                    i += 1\n","                flush_section()\n","                i += 1\n","                continue\n","            \n","            # Handle section content\n","            if in_toc:\n","                if not line.strip() and not next_line.strip():\n","                    in_toc = False\n","                    flush_section()\n","                else:\n","                    current_section.append(line)\n","            elif in_front_matter:\n","                if not line.strip() and not next_line.strip():\n","                    in_front_matter = False\n","                    flush_section()\n","                else:\n","                    current_section.append(line)\n","            else:\n","                current_section.append(line)\n","            \n","            i += 1\n","        \n","        flush_section()  # Flush any remaining content\n","        \n","        # Filter out empty sections and preserve correct spacing\n","        filtered_sections = []\n","        for section in sections:\n","            if section.text.strip():\n","                filtered_sections.append(section)\n","        \n","        return filtered_sections\n","\n","    def get_complete_paragraphs(self, text: str, max_tokens: int) -> Tuple[str, str]:\n","        \"\"\"\n","        Enhanced version with corrected content processing logic.\n","        \"\"\"\n","        self.log_message(f\"Starting get_complete_paragraphs with {len(text)} chars of text\")\n","        \n","        sections = self.analyze_text_structure(text)\n","        self.log_message(f\"Found {len(sections)} sections\")\n","        \n","        current_sections = []\n","        current_tokens = 0\n","        section_index = 0\n","        \n","        try:\n","            while section_index < len(sections):\n","                section = sections[section_index]\n","                section_tokens = self.count_tokens(section.text)\n","                \n","                self.log_message(f\"Processing section {section_index + 1}: {section.type}, {section_tokens} tokens\")\n","                \n","                # If this section would exceed our token limit\n","                if current_tokens + section_tokens > max_tokens:\n","                    if current_sections:  # Only break if we have content\n","                        break\n","                \n","                # Always include header with its following content\n","                if section.type == SectionType.HEADER:\n","                    # Add the header\n","                    current_sections.append(section)\n","                    current_tokens += section_tokens\n","                    \n","                    # Look ahead for content\n","                    next_index = section_index + 1\n","                    if next_index < len(sections) and sections[next_index].type == SectionType.CONTENT:\n","                        next_section = sections[next_index]\n","                        next_tokens = self.count_tokens(next_section.text)\n","                        if current_tokens + next_tokens <= max_tokens:\n","                            current_sections.append(next_section)\n","                            current_tokens += next_tokens\n","                            section_index += 1  # Skip the content section in next iteration\n","                    \n","                # Handle content sections not attached to headers\n","                elif section.type == SectionType.CONTENT:\n","                    current_sections.append(section)\n","                    current_tokens += section_tokens\n","                \n","                # Handle other section types (TABLE_OF_CONTENTS, etc.)\n","                else:\n","                    current_sections.append(section)\n","                    current_tokens += section_tokens\n","                \n","                section_index += 1\n","                self.log_message(f\"After processing: current_tokens={current_tokens}, max_tokens={max_tokens}, sections_processed={len(current_sections)}\")\n","            \n","            # Combine sections with proper spacing\n","            processed_sections = []\n","            for i, section in enumerate(current_sections):\n","                # Add extra newline before sections (except the first one)\n","                if i > 0:\n","                    processed_sections.append(\"\")\n","                \n","                # Add the section text\n","                processed_sections.append(section.text.rstrip())\n","                \n","                # Add extra newline after headers\n","                if section.type == SectionType.HEADER:\n","                    processed_sections.append(\"\")\n","            \n","            processed_text = \"\\n\".join(processed_sections)\n","            \n","            # Prepare remaining sections\n","            remaining_sections = []\n","            if section_index < len(sections):\n","                for section in sections[section_index:]:\n","                    if remaining_sections:\n","                        remaining_sections.append(\"\")\n","                    remaining_sections.append(section.text.rstrip())\n","            \n","            remaining_text = \"\\n\".join(remaining_sections) if remaining_sections else \"\"\n","            \n","            self.log_message(f\"Completed processing: {len(current_sections)} sections included, {len(sections) - section_index} remaining\")\n","            self.log_message(f\"Processed text preview: {processed_text[:200]}...\")\n","            \n","            return processed_text, remaining_text\n","            \n","        except Exception as e:\n","            self.log_message(f\"Error in get_complete_paragraphs: {str(e)}\")\n","            if current_sections:\n","                return \"\\n\".join([s.text for s in current_sections]), text\n","            return \"\", text\n","\n","    def process_text(self, text: str, max_chunks: int = None) -> List[str]:\n","        \"\"\"Process entire text into semantic sections with enhanced logging\"\"\"\n","        self.log_message(\"Starting text processing\")\n","        \n","        # Create initial chunks\n","        initial_chunks = self.create_initial_chunks(text)\n","        \n","        if max_chunks:\n","            initial_chunks = initial_chunks[:max_chunks]\n","            self.log_message(f\"Processing limited to first {max_chunks} chunks\")\n","        \n","        # Process each chunk\n","        semantic_chunks = []\n","        for i, chunk in enumerate(initial_chunks):\n","            self.log_message(f\"Processing chunk {i+1}/{len(initial_chunks)}\")\n","            \n","            # Get semantic sections\n","            sections, metrics = self.get_semantic_sections(chunk)\n","            \n","            # Print processing details\n","            self.print_separator(\"INPUT CHUNK\")\n","            print(f\"Chunk {i+1} (Tokens: {self.count_tokens(chunk)})\")\n","            print(\"Content preview:\")\n","            print(chunk[:1000] + \"...\" if len(chunk) > 1000 else chunk)\n","            \n","            self.print_separator(\"SEMANTIC SECTIONS\")\n","            for j, section in enumerate(sections):\n","                print(f\"\\nSection {j+1} (Tokens: {self.count_tokens(section)})\")\n","                print(\"Content preview:\")\n","                print(section[:500] + \"...\" if len(section) > 500 else section)\n","            \n","            self.print_separator(\"METRICS\")\n","            pprint(metrics)\n","            \n","            if self.missed_text:\n","                self.print_separator(\"MISSED TEXT\")\n","                print(self.missed_text)\n","            \n","            semantic_chunks.extend(sections)\n","            \n","            # Save intermediate results\n","            self.save_chunk_log(i+1, chunk, sections, metrics)\n","            \n","            time.sleep(1)  # Rate limiting\n","            \n","        self.log_message(f\"Processing complete. Total semantic chunks created: {len(semantic_chunks)}\")\n","        return semantic_chunks\n","    \n","    def verify_output_completeness(self, input_text: str, output_sections: List[str]) -> str:\n","        \"\"\"Verify all input text is present in output sections and return missing text\"\"\"\n","        # Normalize texts for comparison\n","        input_normalized = ' '.join(input_text.split())\n","        output_normalized = ' '.join(' '.join(output_sections).split())\n","        \n","        # Find missing content\n","        words = input_normalized.split()\n","        window_size = 5  # Look for sequences of 5 words\n","        \n","        missing_sequences = []\n","        i = 0\n","        while i < len(words) - window_size:\n","            sequence = ' '.join(words[i:i+window_size])\n","            if sequence not in output_normalized:\n","                # Find complete missing phrase\n","                start = i\n","                while start > 0 and ' '.join(words[start-1:i+window_size]) not in output_normalized:\n","                    start -= 1\n","                end = i + window_size\n","                while end < len(words) and ' '.join(words[i:end+1]) not in output_normalized:\n","                    end += 1\n","                missing_sequences.append(' '.join(words[start:end]))\n","                i = end\n","            else:\n","                i += 1\n","        \n","        return '\\n'.join(missing_sequences) if missing_sequences else \"\"\n","    \n","    def create_initial_chunks(self, text: str) -> List[str]:\n","        \"\"\"\n","        Create initial chunks with enhanced logging.\n","        \"\"\"\n","        chunks = []\n","        remaining_text = text\n","        chunk_number = 0\n","        \n","        while remaining_text.strip():\n","            chunk_number += 1\n","            self.log_message(f\"\\nProcessing chunk {chunk_number}\")\n","            \n","            # Add any missed text from previous chunk\n","            if self.missed_text:\n","                self.log_message(\"Adding missed text from previous chunk\")\n","                remaining_text = self.missed_text + '\\n\\n' + remaining_text\n","                self.missed_text = \"\"\n","            \n","            # Get complete paragraphs up to token limit\n","            chunk_text, remaining_text = self.get_complete_paragraphs(remaining_text, self.max_tokens)\n","            \n","            if chunk_text.strip():\n","                self.log_message(f\"Created chunk {chunk_number} with {self.count_tokens(chunk_text)} tokens\")\n","                chunks.append(chunk_text)\n","                \n","                # Debug output\n","                preview = chunk_text[:200] + \"...\" if len(chunk_text) > 200 else chunk_text\n","                self.log_message(f\"Chunk {chunk_number} preview:\\n{preview}\")\n","            else:\n","                self.log_message(\"Warning: Empty chunk produced\")\n","                if not remaining_text.strip():\n","                    break\n","            \n","            if len(chunks) >= 100:  # Safety limit\n","                self.log_message(\"Warning: Maximum chunk limit reached\")\n","                break\n","        \n","        self.log_message(f\"Created {len(chunks)} initial chunks\")\n","        \n","        # Save the chunks\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        for i, chunk in enumerate(chunks):\n","            with open(os.path.join(self.log_dir, f\"chunk_{i+1:04d}.txt\"), 'w', encoding='utf-8') as f:\n","                f.write(chunk)\n","                \n","        return chunks\n","    \n","    def get_semantic_sections(self, chunk: str) -> Tuple[List[str], Dict]:\n","        \"\"\"Update the system prompt for better structural preservation.\"\"\"\n","        try:\n","            self.log_message(f\"Sending request to LLM (input tokens: {self.count_tokens(chunk)})\")\n","            \n","            response = self.client.chat.completions.create(\n","                model=self.model_name,\n","                messages=[\n","                    {\n","                        \"role\": \"system\",\n","                        \"content\": \"\"\"You are a text analysis expert. Your task is to:\n","                        1. Maintain the original document structure (headers, lists, quotes)\n","                        2. Split the input text into coherent semantic sections\n","                        3. Each section must respect structural boundaries\n","                        4. Use <START_SECTION> and <END_SECTION> to mark sections\n","                        5. Include ALL text from the input - do not skip any content\n","                        6. Preserve ALL formatting, indentation, and special characters\n","                        7. If there's a header, keep it with its content\n","                        8. Keep lists and quotes intact within their sections\n","                        9. If a section would be incomplete, mark it with <INCOMPLETE> tags\n","                        \n","                        Rules:\n","                        - Preserve ALL text exactly as provided\n","                        - Maintain original formatting and spacing\n","                        - Don't add any commentary\n","                        - Don't modify the text\n","                        - Keep structural elements together\n","                        - Respect document hierarchy\"\"\"\n","                    },\n","                    {\n","                        \"role\": \"user\",\n","                        \"content\": f\"Split this text into coherent sections, preserving ALL content and structure:\\n\\n{chunk}\"\n","                    }\n","                ],\n","                max_tokens=self.max_tokens,\n","                temperature=0.2\n","            )\n","            \n","            result = response.choices[0].message.content\n","            \n","            # Extract sections\n","            sections = []\n","            section_pattern = r'<START_SECTION>(.*?)<END_SECTION>'\n","            for match in re.finditer(section_pattern, result, re.DOTALL):\n","                section_text = match.group(1).strip()\n","                if section_text and len(section_text) > 50:  # Ignore empty or very short sections\n","                    sections.append(section_text)\n","            \n","            # Check for incomplete section\n","            incomplete_pattern = r'<INCOMPLETE>(.*?)</INCOMPLETE>'\n","            incomplete_match = re.search(incomplete_pattern, result, re.DOTALL)\n","            if incomplete_match:\n","                incomplete_text = incomplete_match.group(1).strip()\n","                if incomplete_text:\n","                    self.missed_text = incomplete_text\n","                    self.log_message(f\"Found incomplete section ({self.count_tokens(incomplete_text)} tokens)\")\n","            \n","            # Verify all content is included\n","            if not incomplete_match:  # Only check if no explicit incomplete section\n","                missed_text = self.verify_output_completeness(chunk, sections)\n","                if missed_text:\n","                    self.missed_text = missed_text\n","                    self.log_message(f\"Found missed text ({self.count_tokens(missed_text)} tokens)\")\n","            \n","            metrics = {\n","                \"completion_tokens\": response.usage.completion_tokens,\n","                \"prompt_tokens\": response.usage.prompt_tokens,\n","                \"total_tokens\": response.usage.total_tokens,\n","                \"finish_reason\": response.choices[0].finish_reason,\n","                \"sections_created\": len(sections),\n","                \"has_missed_text\": bool(self.missed_text)\n","            }\n","            \n","            return sections, metrics\n","                \n","        except Exception as e:\n","            self.log_message(f\"Error in LLM request: {str(e)}\")\n","            return [], {}\n","    \n","    def process_text(self, text: str, max_chunks: int = None) -> List[str]:\n","        \"\"\"Process entire text into semantic sections\"\"\"\n","        self.log_message(\"Starting text processing\")\n","        \n","        # Create initial chunks\n","        initial_chunks = self.create_initial_chunks(text)\n","        \n","        if max_chunks:\n","            initial_chunks = initial_chunks[:max_chunks]\n","            self.log_message(f\"Processing limited to first {max_chunks} chunks\")\n","        \n","        # Process each chunk\n","        semantic_chunks = []\n","        for i, chunk in enumerate(initial_chunks):\n","            self.log_message(f\"Processing chunk {i+1}/{len(initial_chunks)}\")\n","            \n","            # Get semantic sections\n","            sections, metrics = self.get_semantic_sections(chunk)\n","            \n","            # Print processing details\n","            self.print_separator(\"INPUT CHUNK\")\n","            print(f\"Chunk {i+1} (Tokens: {self.count_tokens(chunk)})\")\n","            print(chunk[:1000] + \"...\" if len(chunk) > 1000 else chunk)\n","            \n","            self.print_separator(\"SEMANTIC SECTIONS\")\n","            for j, section in enumerate(sections):\n","                print(f\"\\nSection {j+1} (Tokens: {self.count_tokens(section)})\")\n","                print(section[:500] + \"...\" if len(section) > 500 else section)\n","            \n","            self.print_separator(\"METRICS\")\n","            pprint(metrics)\n","            \n","            if self.missed_text:\n","                self.print_separator(\"MISSED TEXT\")\n","                print(self.missed_text)\n","            \n","            semantic_chunks.extend(sections)\n","            \n","            # Save intermediate results\n","            self.save_chunk_log(i+1, chunk, sections, metrics)\n","            \n","            time.sleep(1)  # Rate limiting\n","            \n","        self.log_message(f\"Processing complete. Total semantic chunks created: {len(semantic_chunks)}\")\n","        return semantic_chunks\n","\n","    def save_chunk_log(self, chunk_num: int, original_chunk: str, sections: List[str], metrics: Dict):\n","        \"\"\"Save intermediate processing results\"\"\"\n","        log_file = os.path.join(self.log_dir, f\"chunk_{chunk_num:04d}.json\")\n","        log_data = {\n","            \"chunk_number\": chunk_num,\n","            \"original_text\": original_chunk,\n","            \"semantic_sections\": sections,\n","            \"missed_text\": self.missed_text,\n","            \"metrics\": metrics,\n","            \"token_counts\": {\n","                \"input\": self.count_tokens(original_chunk),\n","                \"sections\": [self.count_tokens(s) for s in sections],\n","                \"missed\": self.count_tokens(self.missed_text) if self.missed_text else 0\n","            }\n","        }\n","        \n","        with open(log_file, 'w', encoding='utf-8') as f:\n","            json.dump(log_data, f, indent=2, ensure_ascii=False)\n","\n","    def validate_chunk(self, chunk: str, original_sections: List[Section]) -> bool:\n","        \"\"\"Validate that chunk contains all expected content\"\"\"\n","        # Normalize texts for comparison\n","        chunk_text = ' '.join(chunk.split())\n","        original_text = ' '.join(' '.join(s.text for s in original_sections).split())\n","        \n","        # Check if all content is present\n","        missing_content = []\n","        words = original_text.split()\n","        window_size = 5\n","        \n","        i = 0\n","        while i < len(words) - window_size:\n","            sequence = ' '.join(words[i:i+window_size])\n","            if sequence not in chunk_text:\n","                missing_content.append(sequence)\n","            i += 1\n","        \n","        if missing_content:\n","            self.log_message(\"Missing content detected:\")\n","            for mc in missing_content:\n","                self.log_message(f\"  - {mc}\")\n","            return False\n","        \n","        return True\n","    \n","    \n","def main():\n","    # Initialize chunker\n","    chunker = SemanticChunker()\n","    \n","    # Read input file\n","    input_file = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    \n","    # Process text (limit to first 5 chunks for testing)\n","    semantic_chunks = chunker.process_text(text)\n","    \n","    # Save final chunks\n","    output_dir = os.path.join(chunker.log_dir, \"semantic_chunks\")\n","    os.makedirs(output_dir, exist_ok=True)\n","    \n","    for i, chunk in enumerate(semantic_chunks):\n","        output_file = os.path.join(output_dir, f\"semantic_chunk_{i+1:04d}.txt\")\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","            f.write(chunk)\n","    \n","    chunker.log_message(f\"Saved {len(semantic_chunks)} semantic chunks to {output_dir}\")\n","\n","def test_structure_analysis():\n","    chunker = SemanticChunker()\n","    \n","    test_text = \"\"\"\n","                             AUTHOR'S NOTE\n","\n","My task in this work has been to investigate an individual phantasy\n","system, and in the doing of it problems of such magnitude have been\n","uncovered, that my endeavor to grasp them in their entirety has\n","necessarily meant only a superficial orientation toward those paths, the\n","opening and exploration of which may possibly crown the work of future\n","investigators with success.\n","\n","                                CONTENTS\n","\n","        INTRODUCTION                                                     3\n","        \n","        Relation of the Incest Phantasy to the Oedipus Legend—Moral\n","        revulsion over such a discovery\n","\n"," I.—    CONCERNING THE TWO KINDS OF THINKING                             8\n","\"\"\"\n","    \n","    try:\n","        print(\"Testing structural analysis...\")\n","        sections = chunker.analyze_text_structure(test_text)\n","        \n","        print(\"\\nIdentified sections:\")\n","        for i, section in enumerate(sections, 1):\n","            print(f\"\\nSection {i}:\")\n","            print(f\"Type: {section.type}\")\n","            print(f\"Level: {section.level}\")\n","            print(f\"Content preview: {section.text[:100]}...\")\n","        \n","        print(\"\\nTesting chunking with structure preservation...\")\n","        chunks = chunker.create_initial_chunks(test_text)\n","        \n","        print(\"\\nResulting chunks:\")\n","        for i, chunk in enumerate(chunks, 1):\n","            print(f\"\\nChunk {i}:\")\n","            print(chunk[:200])\n","            print(\"...\")\n","            \n","    except Exception as e:\n","        print(f\"Error during testing: {str(e)}\")\n","    \n","    # Add validation\n","    print(\"\\nValidating chunk content...\")\n","    for i, chunk in enumerate(chunks, 1):\n","        print(f\"\\nValidating chunk {i}:\")\n","        is_valid = chunker.validate_chunk(chunk, sections)\n","        print(f\"Chunk {i} validation: {'PASSED' if is_valid else 'FAILED'}\")\n","        \n","if __name__ == \"__main__\":\n","    # test_structure_analysis()\n","    main()  # Comment out for testing"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.825136Z","iopub.status.busy":"2024-11-04T08:49:54.824806Z","iopub.status.idle":"2024-11-04T08:49:54.833600Z","shell.execute_reply":"2024-11-04T08:49:54.832541Z","shell.execute_reply.started":"2024-11-04T08:49:54.825103Z"},"trusted":true},"outputs":[],"source":["# # ----------------------------- #\n","# # Part 3: Parse Text into Discourse Units\n","# # ----------------------------- #\n","\n","def parse_discourse_units(text, overwrite=False):\n","    \"\"\"\n","    Parses text into discourse units using spaCy.\n","    Currently splits text into sentences.\n","    \"\"\"\n","    paragraphs = text.split('\\n\\n')\n","    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n","\n","    discourse_units = []\n","    for para in paragraphs:\n","        doc = nlp(para)\n","        sentences = [sent.text for sent in doc.sents]\n","        discourse_units.extend(sentences)\n","\n","    output_path = '/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_discourse_units.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Discourse Units: {len(discourse_units)}\")\n","    return discourse_units"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.987822Z","iopub.status.busy":"2024-11-04T08:49:54.987546Z","iopub.status.idle":"2024-11-04T08:49:54.997464Z","shell.execute_reply":"2024-11-04T08:49:54.996587Z","shell.execute_reply.started":"2024-11-04T08:49:54.987793Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 4: Create Chunks Using Hybrid Strategy\n","# ----------------------------- #\n","\n","def create_chunks(discourse_units, tokenizer, max_length=4096, overlap_size=1, overwrite=False):\n","    \"\"\"\n","    Creates chunks from discourse units using a sliding window with overlapping chunks.\n","    Optimized to work directly with token IDs and utilize efficient list operations.\n","    \"\"\"\n","    chunks = []\n","    current_chunk_tokens = []\n","    current_length = 0\n","\n","    for unit in discourse_units:\n","        unit_tokens = tokenizer.encode(unit, add_special_tokens=True)\n","        unit_length = len(unit_tokens)\n","\n","        if current_length + unit_length <= max_length:\n","            current_chunk_tokens.extend(unit_tokens)\n","            current_length += unit_length\n","        else:\n","            # Decode and append the current chunk\n","            chunk_text = tokenizer.decode(\n","                current_chunk_tokens, skip_special_tokens=True)\n","            chunks.append(chunk_text)\n","\n","            # Prepare overlap tokens\n","            overlap_tokens = current_chunk_tokens[-overlap_size:]\n","            current_chunk_tokens = overlap_tokens + unit_tokens\n","            current_length = len(current_chunk_tokens)\n","\n","    # Append any remaining tokens as the last chunk\n","    if current_chunk_tokens:\n","        chunk_text = tokenizer.decode(\n","            current_chunk_tokens, skip_special_tokens=True)\n","        chunks.append(chunk_text)\n","\n","    # Write or read chunks as before\n","    output_path = '/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_chunks.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Chunks Created: {len(chunks)}\")\n","    return chunks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:56.853343Z","iopub.status.busy":"2024-11-04T08:49:56.852589Z","iopub.status.idle":"2024-11-04T08:49:56.862517Z","shell.execute_reply":"2024-11-04T08:49:56.861550Z","shell.execute_reply.started":"2024-11-04T08:49:56.853301Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 5: Create and Tokenize Dataset\n","# ----------------------------- #\n","\n","# To Do - make book titles and prompt generic so\n","def create_tokenized_dataset(chunks, tokenizer, max_length=1024, num_proc=2):\n","\n","    # Create a Dataset object from chunks\n","\n","    book_title = 'Psychology of the Unconscious by C. G. Jung'\n","    wikipedia_prompt = \"\"\"\n","    Psychology Book\n","\n","    ### Title: {}\n","\n","    ### Article: {}\n","    \"\"\"\n","\n","    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n","\n","    def formatting_prompts_func(examples):\n","        titles = book_title\n","        texts = examples[\"text\"]\n","        outputs = []\n","        for title, text in zip([book_title]*len(chunks), texts):\n","            text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n","            outputs.append(text)\n","        return {\"text\": outputs, }\n","    pass\n","\n","    # convert chunks variable to huggingface dataset\n","\n","    from datasets import Dataset\n","\n","    dataset = Dataset.from_dict({\"text\": chunks})\n","\n","    dataset = dataset.map(formatting_prompts_func,\n","                          batched=True, num_proc=num_proc)\n","    # Split the dataset into training and validation sets\n","    split = dataset.train_test_split(test_size=0.1, seed=42)\n","    train_dataset = split['train']\n","    eval_dataset = split['test']\n","\n","    print(len(dataset))\n","    # Find the maximum length of the text field in the entire dataset\n","    max_length = max(len(text) for text in dataset['text'])\n","    print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","#     print(f\"First 5 rows of training dataset: {train_dataset[:5]}\")\n","    print(f\"Validation Dataset Size: {len(eval_dataset)}\")\n","#     print(f\"First 5 rows of validation dataset: {eval_dataset[:5]}\")\n","    return train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:50:01.810305Z","iopub.status.busy":"2024-11-04T08:50:01.809400Z","iopub.status.idle":"2024-11-04T08:50:01.844994Z","shell.execute_reply":"2024-11-04T08:50:01.844131Z","shell.execute_reply.started":"2024-11-04T08:50:01.810262Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 6: Set up environment and other important utilities\n","# ----------------------------- #\n","\n","def setup_environment():\n","    \"\"\"\n","    Initializes the Accelerator for distributed training.\n","    \"\"\"\n","    return Accelerator()\n","\n","\n","def get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n","    \"\"\"\n","    Defines a custom learning rate scheduler with warmup, constant, and linear annealing phases.\n","    \"\"\"\n","    def lr_lambda(current_step):\n","        if current_step < num_warmup_steps:\n","            return current_step / num_warmup_steps  # Linear warmup\n","        elif current_step < initial_phase_steps:\n","            return 1.0  # Constant learning rate for initial phase\n","        else:\n","            # Linear annealing for the remaining steps\n","            return 1.0 - ((current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps))\n","\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","\n","def setup_training_steps(initial_rows, annealing_rows, batch_size, gradient_accumulation_steps, num_epochs):\n","    \"\"\"\n","    Calculates total and initial training steps based on dataset size and training parameters.\n","    \"\"\"\n","    total_rows = initial_rows + annealing_rows\n","    total_steps = (total_rows * num_epochs) // (batch_size *\n","                                                gradient_accumulation_steps)\n","    initial_steps = (initial_rows * num_epochs) // (batch_size *\n","                                                    gradient_accumulation_steps)\n","    return max(1, total_steps), max(1, initial_steps)\n","\n","\n","def print_memory_usage(step_desc):\n","    \"\"\"\n","    Prints the CUDA memory summary if CUDA is available.\n","    \"\"\"\n","    if torch.cuda.is_available():\n","        print(f\"Memory Usage at {step_desc}:\")\n","        print(torch.cuda.memory_summary())\n","        print(\"\\n\")\n","    else:\n","        print(f\"No CUDA available at {step_desc}.\\n\")\n","\n","\n","def inference(model, tokenizer):\n","    \"\"\"\n","    Runs inference using the trained model.\n","    \"\"\"\n","    # Define sample prompts\n","    prompts = [\n","        \"Explain what is medical anthropology and its importance in elevating the quality of life?\",\n","        \"Explain what are the types of Jungian archetypes and how they manifest in the human psyche?\"\n","    ]\n","\n","    for prompt in prompts:\n","        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, max_length=256)\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n","        \n","def compute_metrics(eval_pred):\n","    \"\"\"\n","    Computes perplexity based on model predictions and labels.\n","    \"\"\"\n","    logits, labels = eval_pred\n","    # Convert to torch tensors\n","    logits = torch.tensor(logits)\n","    labels = torch.tensor(labels)\n","    \n","    # Ensure shapes match\n","    if logits.shape[:2] != labels.shape:\n","        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n","    \n","    # Shift logits and labels\n","    shift_logits = logits[:, :-1, :].contiguous()\n","    shift_labels = labels[:, 1:].contiguous()\n","\n","    # Check label values\n","    if shift_labels.max() >= model.config.vocab_size:\n","        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n","    \n","    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n","    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","    perplexity = torch.exp(loss).item()\n","    return {\"perplexity\": perplexity}\n","\n","#  Login to Huggingface\n","from huggingface_hub import login\n","import os\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","def setup_huggingface_access():\n","    \"\"\"Setup Hugging Face access with token from environment or manual input\"\"\"\n","    # First try to get token from environment variable\n","    token = os.getenv('HUGGINGFACE_TOKEN')\n","    \n","    if not token:\n","        # If not in environment, prompt for token\n","        token = input(\"Enter your Hugging Face token: \")\n","        \n","    if token:\n","        try:\n","            login(token, add_to_git_credential=True)\n","            print(\"Successfully logged in to Hugging Face!\")\n","        except Exception as e:\n","            print(f\"Failed to log in to Hugging Face: {str(e)}\")\n","            return False\n","    else:\n","        print(\"No Hugging Face token provided\")\n","        return False\n","    \n","    return True"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:51:23.895754Z","iopub.status.busy":"2024-11-04T08:51:23.894928Z","iopub.status.idle":"2024-11-04T08:51:23.904037Z","shell.execute_reply":"2024-11-04T08:51:23.903118Z","shell.execute_reply.started":"2024-11-04T08:51:23.895714Z"},"trusted":true},"outputs":[],"source":["def load_model_and_tokenizer(base_model_slug, max_seq_length=4096, dtype=None, load_in_4bit=True):\n","    \"\"\"\n","    Load and configure the model and tokenizer with specified parameters on a single GPU.\n","    \"\"\"\n","    import torch\n","    import os\n","\n","    # Force CUDA if available\n","    if torch.cuda.is_available():\n","        print(\"CUDA is available.\")\n","        print(f\"Using GPU: {torch.cuda.get_device_properties(0).name}\")\n","        device = torch.device(\"cuda:0\")\n","        device_map = {\"\": 0}  # Force everything to GPU 0\n","    else:\n","        print(\"WARNING: CUDA is not available. Using CPU.\")\n","        device = torch.device(\"cpu\")\n","        device_map = \"cpu\"\n","\n","    # Print initial GPU memory\n","    if torch.cuda.is_available():\n","        print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n","\n","    try:\n","        # Load base model and tokenizer\n","        model, tokenizer = FastLanguageModel.from_pretrained(\n","            model_name=base_model_slug,\n","            max_seq_length=max_seq_length,\n","            dtype=dtype,\n","            load_in_4bit=load_in_4bit,\n","            device_map=device_map,\n","            token=os.getenv('HUGGINGFACE_TOKEN'),\n","        )\n","        \n","        print(f\"Model device after loading: {next(model.parameters()).device}\")\n","        \n","        # Configure PEFT model\n","        model = FastLanguageModel.get_peft_model(\n","            model,\n","            r=128,\n","            target_modules=[\n","                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                \"gate_proj\", \"up_proj\", \"down_proj\",\n","                \"embed_tokens\", \"lm_head\",\n","            ],\n","            lora_alpha=32,\n","            lora_dropout=0,\n","            bias=\"none\",\n","            use_gradient_checkpointing=\"unsloth\",\n","            random_state=3407,\n","            use_rslora=True,\n","            loftq_config=None,\n","        )\n","        \n","        # Ensure model is on GPU after PEFT configuration\n","        if torch.cuda.is_available():\n","            model = model.to(device)\n","            \n","        # Verify final device placement\n","        print(f\"Final model device: {next(model.parameters()).device}\")\n","        \n","        # Print GPU memory usage\n","        if torch.cuda.is_available():\n","            print(f\"\\nGPU Memory After Complete Setup: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n","            \n","    except Exception as e:\n","        print(f\"Error in model loading/configuration: {str(e)}\")\n","        raise\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:51:27.038467Z","iopub.status.busy":"2024-11-04T08:51:27.038068Z","iopub.status.idle":"2024-11-04T08:51:27.044121Z","shell.execute_reply":"2024-11-04T08:51:27.043250Z","shell.execute_reply.started":"2024-11-04T08:51:27.038425Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","# Set the environment variable\n","os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n","os.environ[\"WANDB_API_KEY\"] = \"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\"\n","# Verify it's set correctly\n","print(os.getenv(\"HUGGINGFACE_TOKEN\"))\n","print(os.getenv(\"WANDB_API_KEY\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:53:40.380684Z","iopub.status.busy":"2024-11-04T08:53:40.380279Z","iopub.status.idle":"2024-11-04T08:54:30.243475Z","shell.execute_reply":"2024-11-04T08:54:30.242381Z","shell.execute_reply.started":"2024-11-04T08:53:40.380645Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 2: Configure Environment Variables & Create Main Variables\n","# ----------------------------- #\n","\n","# Unsloth modell initialization variables\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","max_length = max_seq_length\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","# device_map = \"auto\"\n","base_model_slug = \"Qwen/Qwen2.5-7B-Instruct\"\n","base_model_name = \"lora_model_pum\"\n","chunks_max_length = max_seq_length\n","overlap_size = 1\n","# Define your parameters\n","batchSize = 2\n","ga = 8\n","maxSteps = 10\n","warmupSteps = 10\n","numTrainEpochs = 1\n","lRate = 5e-5\n","embLRate = 1e-5\n","optim = \"adamw_8bit\"\n","lrSchedule = \"linear\"\n","dataset_slug = \"psychology_of_unconscious\"\n","\n","from datetime import datetime\n","import pytz\n","import wandb\n","# Get the current date and time in Indian Standard Time (IST)\n","ist = pytz.timezone('Asia/Kolkata')\n","current_datetime = datetime.now(ist)\n","\n","# Format the datetime string\n","# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n","formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Define Run Name\n","run_name = f\"\"\"Kaggle-quantumLeap-{formatted_datetime}-{base_model_slug}-{dataset_slug}-{max_seq_length}_maxSeqLength-{max_length}_maxLength-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{numTrainEpochs}_numTrainEpochs-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n","\n","# Initialize Weights & Biases\n","# It's recommended to set your W&B API key as an environment variable for security.\n","wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n","wandb.init(project=\"OLA-quantumLeap\", name=run_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:55:17.806518Z","iopub.status.busy":"2024-11-04T08:55:17.806143Z","iopub.status.idle":"2024-11-04T08:55:20.860165Z","shell.execute_reply":"2024-11-04T08:55:20.859337Z","shell.execute_reply.started":"2024-11-04T08:55:17.806483Z"},"trusted":true},"outputs":[],"source":["\n","# ----------------------------- #\n","# Part 9: Data Processing\n","# ----------------------------- #\n","\n","# # Perform Inference Before Training\n","# inference(model, tokenizer)\n","\n","# Set number of processes to use for data loading\n","num_cpus = multiprocessing.cpu_count()\n","num_proc = max(num_cpus-2, 2)  # Adjust based on prior recommendations\n","print(f\"Number of CPU cores: {num_cpus}\")\n","print(f\"Number of processes: {num_proc}\")\n","\n","# Login to Hugging Face\n","if not setup_huggingface_access():\n","    raise Exception(\"Failed to setup Hugging Face access. Please check your token.\")\n","\n","# Load Model and Tokenizer\n","model, tokenizer = load_model_and_tokenizer(base_model_slug)\n","print(f\"Model Device: {model.device}\")\n","\n","# Load and Clean Text Data\n","file_path = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","clean_text = load_and_clean_text(file_path)\n","\n","# Parse Discourse Units\n","discourse_units = parse_discourse_units(clean_text, overwrite=True)\n","\n","# Create Chunks\n","chunks = create_chunks(\n","    discourse_units,\n","    tokenizer,\n","    max_length=max_length,\n","    overlap_size=overlap_size,\n","    overwrite=True,\n",")\n","\n","# Create Tokenized Dataset\n","train_dataset, eval_dataset = create_tokenized_dataset(\n","    chunks, tokenizer, max_length)\n","\n","# Save datasets as Hugging Face `datasets`\n","train_dataset.save_to_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","eval_dataset.save_to_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')\n","\n","### To Do - Make the below as dynamic and as a functio\n","# # Uncomment following if you want to just load the data from temp directory\n","# from datasets import load_from_disk\n","\n","# train_dataset = load_from_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","# eval_dataset = load_from_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import IntervalStrategy\n","from transformers.integrations import TensorBoardCallback\n","\n","import wandb\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_dataset,\n","    eval_dataset = eval_dataset,  # Use 10% of data for evaluation\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = batchSize,\n","        gradient_accumulation_steps = ga,\n","\n","        # Set both max_steps and num_train_epochs\n","        max_steps = maxSteps,\n","        num_train_epochs = numTrainEpochs,\n","\n","        # Use a single learning rate for all parameters\n","        learning_rate = lRate,\n","\n","        # Warmup strategy from successful runs\n","        warmup_steps = warmupSteps,\n","        # warmup_ratio = 0,\n","\n","        # Explicitly set precision based on hardware support\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        \n","        logging_steps = 1,\n","        \n","        optim = optim,\n","        weight_decay = 0.01,\n","        lr_scheduler_type = lrSchedule,\n","        \n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        \n","        report_to=[\"tensorboard\", \"wandb\"],\n","        logging_dir=f\"./trel-fft-logs/{run_name}\",\n","        \n","        # Set both save and evaluation strategies to 'steps'\n","        # save_strategy = IntervalStrategy.STEPS,\n","        # eval_strategy = IntervalStrategy.STEPS,\n","        # save_steps = 1,  # Save checkpoint every 20 steps\n","        # eval_steps = 1,  # Evaluate every 20 steps (matching save_steps)\n","        \n","        # load_best_model_at_end = True,\n","        # metric_for_best_model = \"eval_loss\",\n","    ),\n","    # compute_metrics = compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %%time\n","\n","# instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","# ### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","# concept_name: {}\n","# detailed_explanation: {}\n","\n","# ### Response:\n","# {}\"\"\"\n","\n","# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","# outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n","# tokenizer.batch_decode(outputs)\n","\n","\n","# %%time\n","# # Text Streaming goes into a loop and doesnt adher to EOS\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)\n","\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:55:41.075256Z","iopub.status.busy":"2024-11-04T08:55:41.074858Z","iopub.status.idle":"2024-11-04T10:16:26.590311Z","shell.execute_reply":"2024-11-04T10:16:26.589250Z","shell.execute_reply.started":"2024-11-04T08:55:41.075219Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 11: Start Training\n","# ----------------------------- #\n","\n","#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","trainer_stats = trainer.train()\n","\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import wandb\n","from pprint import pprint\n","\n","def get_run_config(project_name, run_id):\n","    try:\n","        # Initialize the wandb API\n","        api = wandb.Api()\n","\n","        # Access the specific run\n","        run = api.run(f\"{project_name}/{run_id}\")\n","\n","        # Get the full configuration\n","        config = run.config\n","\n","        # Filter for trainer-specific configuration\n","        trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n","\n","        return trainer_config\n","\n","    except wandb.errors.CommError:\n","        print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n","        return None\n","    except Exception as e:\n","        print(f\"An error occurred: {str(e)}\")\n","        return None\n","\n","# Usage\n","project_name = \"olabs-asia-olabs-pro/OLA-quantumLeap\"\n","run_id = \"we4axhd1\"\n","\n","trainer_config = get_run_config(project_name, run_id)\n","\n","if trainer_config:\n","    print(f\"Trainer configuration for run {run_id}:\")\n","    pprint(trainer_config)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %%time\n","\n","# instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","# ### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","# concept_name: {}\n","# detailed_explanation: {}\n","\n","# ### Response:\n","# {}\"\"\"\n","\n","# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","# outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n","# tokenizer.batch_decode(outputs)\n","\n","\n","# %%time\n","# # Text Streaming goes into a loop and doesnt adher to EOS\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)\n","\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","import os\n","\n","# Create timestamp\n","timestamp = int(time.time())\n","\n","# Create directory if it doesn't exist\n","save_dir = f\"/root/quantumLeap/models/qLeap_model_v0_{timestamp}\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# Save functions with explicit paths\n","def save_model_versions(model, tokenizer, timestamp, token):\n","    \"\"\"\n","    Save model in different formats with proper error handling\n","    \"\"\"\n","    try:\n","        # Save base model locally\n","        print(\"Saving base model locally...\")\n","        # model.save_pretrained(f\"{save_dir}/base\")\n","        # tokenizer.save_pretrained(f\"{save_dir}/base\")\n","        \n","        # Save 8-bit Q8_0 version\n","        print(\"Saving 8-bit Q8_0 version...\")\n","        try:\n","            model.push_to_hub_gguf(\n","                f\"olabs-ai/qLeap_model_v0_8bit_Q8_{timestamp}\",\n","                tokenizer,\n","                token=token,\n","                quantization_method=\"q8_0\"\n","            )\n","            print(\"Successfully saved 8-bit model\")\n","        except Exception as e:\n","            print(f\"Error saving 8-bit model: {str(e)}\")\n","            \n","        # Optional: Save 16-bit version\n","        print(\"Saving 16-bit version...\")\n","        try:\n","            model.push_to_hub_gguf(\n","                f\"olabs-ai/qLeap_model_v0_16bit_GGUF_{timestamp}\",\n","                tokenizer,\n","                quantization_method=\"f16\",\n","                token=token\n","            )\n","            print(\"Successfully saved 16-bit model\")\n","        except Exception as e:\n","            print(f\"Error saving 16-bit model: {str(e)}\")\n","            \n","    except Exception as e:\n","        print(f\"Error in save process: {str(e)}\")\n","        raise\n","\n","# Call the save function\n","huggingface_token = \"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\"\n","save_model_versions(model, tokenizer, timestamp, huggingface_token)"]},{"cell_type":"markdown","metadata":{},"source":["### if the loss from earlier training is too high try training arguments from unsloth colab notebook \"Llama-3.1 8b + Unsloth 2x faster finetuning.ipynb\". URL below\n","### https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=95_Nn-89DhsL"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset creation based on the book itself using AugmenToolkit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Instruction  Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n","\n","import json\n","import ast\n","import logging\n","\n","import csv\n","\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n","    reader = csv.DictReader(f)\n","    data = list(reader)\n","    \n","type(data)\n","\n","\n","# Configure logging\n","logging.basicConfig(\n","    filename='transformation_errors.log',\n","    filemode='w',\n","    level=logging.ERROR,\n","    format='%(levelname)s:%(message)s'\n",")\n","\n","# Sample original data\n","original_data = data\n","\n","def transform_data(original_data):\n","    \"\"\"\n","    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n","\n","    Parameters:\n","        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n","\n","    Returns:\n","        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n","    \"\"\"\n","    new_data = []\n","\n","    for idx, entry in enumerate(original_data, start=1):\n","        concept_name = entry.get('concept_name', '').strip()\n","        detailed_explanation = entry.get('detailed_explanation', '').strip()\n","        example_scenario_str = entry.get('example_scenario', '').strip()\n","\n","        if not concept_name or not detailed_explanation or not example_scenario_str:\n","            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n","            continue\n","\n","        # Attempt to parse with json.loads\n","        try:\n","            example_scenarios = json.loads(example_scenario_str)\n","            if not isinstance(example_scenarios, list):\n","                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","        except json.JSONDecodeError:\n","            # Fallback to ast.literal_eval\n","            try:\n","                example_scenarios = ast.literal_eval(example_scenario_str)\n","                if not isinstance(example_scenarios, list):\n","                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","            except (ValueError, SyntaxError) as e:\n","                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n","                continue\n","\n","        # Iterate through each scenario and create a new entry\n","        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n","            if not isinstance(scenario, str):\n","                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n","                continue\n","\n","            new_entry = {\n","                'concept_name': concept_name,\n","                'detailed_explanation': detailed_explanation,\n","                'example_scenario': scenario.strip()\n","            }\n","            new_data.append(new_entry)\n","\n","    return new_data\n","\n","# Transform the data\n","transformed_data = transform_data(original_data)\n","\n","# Optional: Save the transformed data to a JSON file\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n","    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n","\n","print(f\"Transformation complete. {len(transformed_data)} entries created.\")\n","print(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n","\n","print(len(transformed_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","\n","EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","\n","def instruction_prompt_func(examples):\n","    concept_name = examples[\"concept_name\"]\n","    detailed_explanation = examples[\"detailed_explanation\"]\n","    example_scenario = examples[\"example_scenario\"]\n","    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\n","pass\n","\n","\n","# convert transformed_data to a huggingface dataset\n","instruction_dataset = Dataset.from_dict(transformed_data)\n","instruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n","\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = instruction_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 8,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 8,\n","\n","        # Use num_train_epochs and warmup_ratio for longer runs!\n","        max_steps = 120,\n","        warmup_steps = 10,\n","        # warmup_ratio = 0.1,\n","        # num_train_epochs = 1,\n","\n","        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n","        learning_rate = 5e-5,\n","        embedding_learning_rate = 1e-5,\n","\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.00,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")\n","trainer_stats = trainer.train()\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# add current timestamp to model name\n","model.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\n","tokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\n","model.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","tokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","        \n","# Save to 8bit GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_GGUF_{int(time.time())}\", tokenizer,quantization_method = \"q8_0\", token = \"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\")\n","\n","# Save to 16bit GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n","\n","# Save to q4_k_m GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")\n","\n","\n","\n","# # Merge to 16bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n","\n","# # Merge to 4bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n","\n","# # Just LoRA adapters\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
