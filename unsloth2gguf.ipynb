{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 1.2: Import Libraries\n",
    "# ----------------------------- #\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import xformers\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "import ipywidgets\n",
    "import unsloth\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is available\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print('punkt was already available.')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    print('punkt was not available. It has been downloaded')\n",
    "\n",
    "# Initialize spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print('en_core_web_sm was already available.')\n",
    "except OSError:\n",
    "    print(\"SpaCy English model not found. Downloading...\")\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3 MIG 1g.10gb. Max memory: 9.75 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 9.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/olabs/lib/python3.12/site-packages/unsloth/models/_utils.py:902: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n",
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "base_model_slug = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model_slug, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"unsloth_instruct_gguf\") # Local saving\n",
    "tokenizer.save_pretrained(\"unsloth_instruct_gguf\")\n",
    "\n",
    "!huggingface-cli login --token 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 --add-to-git-credential\n",
    "if True:\n",
    "    model.push_to_hub(\"olabs-ai/unsloth-Llama-3.2-1B-Instruct-bnb-4bit-GGUF\", token = \"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\") # Online saving\n",
    "    tokenizer.push_to_hub(\"olabs-ai/unsloth-Llama-3.2-1B-Instruct-bnb-4bit-GGUF\", token = \"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\") # Online saving\n",
    "    model.push_to_hub_gguf(\"olabs-ai/unsloth-Llama-3.2-1B-Instruct-bnb-4bit-GGUF\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
