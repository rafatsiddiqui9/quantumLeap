{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:14:14.999769Z","iopub.execute_input":"2024-11-23T04:14:15.000826Z","iopub.status.idle":"2024-11-23T04:14:15.006837Z","shell.execute_reply.started":"2024-11-23T04:14:15.000761Z","shell.execute_reply":"2024-11-23T04:14:15.006110Z"}},"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# !nvidia-smi\n# !nvcc --version\n# import torch\n# import torchvision\n# import torchaudio\n# print(f\"PyTorch version: {torch.__version__}\")\n# print(f\"PyTorch version: {torchvision.__version__}\")\n# print(f\"PyTorch version: {torchaudio.__version__}\")\n# print(f\"CUDA available: {torch.cuda.is_available()}\")\n# print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:04.106351Z","iopub.execute_input":"2024-11-23T03:31:04.107124Z","iopub.status.idle":"2024-11-23T03:31:04.120483Z","shell.execute_reply.started":"2024-11-23T03:31:04.107086Z","shell.execute_reply":"2024-11-23T03:31:04.119733Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# # # # ----------------------------- #\n# # # # Part 1.1: Install and Setup Libraries\n# # # # ----------------------------- #\n# !pip uninstall -y torch torchvision torchaudio\n# !pip install uv\n# !uv pip install torch=='2.4.1+cu121' torchvision=='0.19.1+cu121' torchaudio=='2.4.1+cu121'  --index-url https://download.pytorch.org/whl/cu121 --system -q\n# !uv pip install unsloth -q --system\n# !uv pip install flash_attn --no-build-isolation -q --system\n# !pip show flash_attn\n# !uv pip install -q wandb bitsandbytes tensorboard ipywidgets nltk spacy huggingface_hub datasets --system\n# !python -m xformers.info # # !python -m xformers.info\n# !uv pip install  -q  --upgrade triton cut-cross-entropy --no-deps --system\n# !uv pip install -U ipywidgets nbformat ipywidgets IProgress tqdm -q --no-cache-dir --system","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:04.121326Z","iopub.execute_input":"2024-11-23T03:31:04.121532Z","iopub.status.idle":"2024-11-23T03:31:04.131222Z","shell.execute_reply.started":"2024-11-23T03:31:04.121510Z","shell.execute_reply":"2024-11-23T03:31:04.130560Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# !nvidia-smi\n# !nvcc --version\n# import torch\n# import torchvision\n# import torchaudio\n# print(f\"PyTorch version: {torch.__version__}\")\n# print(f\"PyTorch version: {torchvision.__version__}\")\n# print(f\"PyTorch version: {torchaudio.__version__}\")\n# print(f\"CUDA available: {torch.cuda.is_available()}\")\n# print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:04.132745Z","iopub.execute_input":"2024-11-23T03:31:04.133049Z","iopub.status.idle":"2024-11-23T03:31:04.145408Z","shell.execute_reply.started":"2024-11-23T03:31:04.133018Z","shell.execute_reply":"2024-11-23T03:31:04.144605Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# !git clone -b 10Nov2024v3  https://githomein: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@github.com/githomein/quantumLeap.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:04.146451Z","iopub.execute_input":"2024-11-23T03:31:04.147283Z","iopub.status.idle":"2024-11-23T03:31:04.157588Z","shell.execute_reply.started":"2024-11-23T03:31:04.147244Z","shell.execute_reply":"2024-11-23T03:31:04.156823Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\n\n# # Set these environment variables before importing torch-related modules\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\nfrom pathlib import Path\n\ndef ensure_working_directory():\n    \"\"\"\n    Check if we're in the correct working directory, if not switch to it.\n    Creates the directory if it doesn't exist.\n    \"\"\"\n    target_dir = '/kaggle/working/quantumLeap'\n    current_dir = os.getcwd()\n    \n    # Print current directory\n    print(f\"Current directory: {current_dir}\")\n    \n    # Check if we need to switch directories\n    if current_dir != target_dir:\n        # Create directory if it doesn't exist\n        Path(target_dir).mkdir(parents=True, exist_ok=True)\n        \n        try:\n            # Change to target directory\n            os.chdir(target_dir)\n            print(f\"Successfully switched to: {target_dir}\")\n        except Exception as e:\n            print(f\"Error switching to directory: {str(e)}\")\n            raise\n    else:\n        print(\"Already in correct directory\")\n    \n    # Verify current directory\n    print(f\"Working directory: {os.getcwd()}\")\n\n# Call the function before your main code\nensure_working_directory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:04.158471Z","iopub.execute_input":"2024-11-23T03:31:04.158673Z","iopub.status.idle":"2024-11-23T03:31:04.171373Z","shell.execute_reply.started":"2024-11-23T03:31:04.158652Z","shell.execute_reply":"2024-11-23T03:31:04.170482Z"}},"outputs":[{"name":"stdout","text":"Current directory: /kaggle/working\nSuccessfully switched to: /kaggle/working/quantumLeap\nWorking directory: /kaggle/working/quantumLeap\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ----------------------------- #\n# Part 1.2: Import Necessary Libraries\n# ----------------------------- #\n\n# General Libraries\nimport os\nimport json\nimport sys\nimport subprocess\nimport argparse\nimport logging\nimport math\nimport random\nfrom datetime import datetime\nimport re\nimport gc\nimport weakref\nimport multiprocessing\n\n# Torch related\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\n\n# Transformers related\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    Adafactor\n)\n\n# Huggingface TRL for full finetune\nfrom trl import SFTTrainer, SFTConfig\n\n# General huggingface libraries\nimport huggingface_hub\nfrom datasets import load_dataset, Dataset\nfrom accelerate import Accelerator\n\n\n# Unsloth specificic libraries\nimport unsloth\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n\n# Other Libraries\nfrom peft import LoraConfig\nimport wandb\nimport nltk\nimport spacy\n# from galore_torch import GaLoreAdamW, GaLoreAdafactor, GaLoreAdamW8bit\n\n# Check and import NLTK and spacy modules\n# Ensure NLTK's punkt tokenizer is available\nimport nltk\ntry:\n    nltk.data.find('tokenizers/punkt')\n    print('punkt was already available.')\nexcept LookupError:\n    nltk.download('punkt')\n    print('punkt was not available. It has been downloaded')\n\n# Initialize spaCy English model\ntry:\n    nlp = spacy.load('en_core_web_sm')\n    print('en_core_web_sm was already available.')\nexcept OSError:\n    print(\"SpaCy English model not found. Downloading...\")\n    os.system('python -m spacy download en_core_web_sm')\n    nlp = spacy.load('en_core_web_sm')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:04.172420Z","iopub.execute_input":"2024-11-23T03:31:04.172734Z","iopub.status.idle":"2024-11-23T03:31:28.821339Z","shell.execute_reply.started":"2024-11-23T03:31:04.172699Z","shell.execute_reply":"2024-11-23T03:31:28.820396Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\npunkt was already available.\nen_core_web_sm was already available.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Load and Clean the Text Data\n# ----------------------------- #\n\ndef load_and_clean_text(file_path):\n    \"\"\"\n    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        text = f.read()\n    # # Remove Project Gutenberg's license text and headers/footers\n    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n\n    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n    return text.strip()\n\n# Replace 'psychology_of_unconscious.txt' with your actual file pathrm -rfcd \nfile_path = '/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'\nclean_text = load_and_clean_text(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:28.822407Z","iopub.execute_input":"2024-11-23T03:31:28.822686Z","iopub.status.idle":"2024-11-23T03:31:28.830226Z","shell.execute_reply.started":"2024-11-23T03:31:28.822658Z","shell.execute_reply":"2024-11-23T03:31:28.829574Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# # ----------------------------- #\n# # Part 3: Parse Text into Discourse Units\n# # ----------------------------- #\n\ndef parse_discourse_units(text, overwrite=False):\n    \"\"\"\n    Parses text into discourse units using spaCy.\n    Currently splits text into sentences.\n    \"\"\"\n    paragraphs = text.split('\\n\\n')\n    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n\n    discourse_units = []\n    for para in paragraphs:\n        doc = nlp(para)\n        sentences = [sent.text for sent in doc.sents]\n        discourse_units.extend(sentences)\n\n    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_discourse_units.json'\n    if not os.path.exists(output_path) or overwrite:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        with open(output_path, 'w') as f:\n            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n    else:\n        with open(output_path, 'r') as f:\n            discourse_units = json.load(f)\n\n    print(f\"Total Discourse Units: {len(discourse_units)}\")\n    return discourse_units","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:28.831271Z","iopub.execute_input":"2024-11-23T03:31:28.831531Z","iopub.status.idle":"2024-11-23T03:31:28.850245Z","shell.execute_reply.started":"2024-11-23T03:31:28.831506Z","shell.execute_reply":"2024-11-23T03:31:28.849488Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ----------------------------- #\n# Part 4: Create Chunks Using Hybrid Strategy\n# ----------------------------- #\n\ndef create_chunks(discourse_units, tokenizer, max_length=4096, overlap_size=1, overwrite=False):\n    \"\"\"\n    Creates chunks from discourse units using a sliding window with overlapping chunks.\n    Optimized to work directly with token IDs and utilize efficient list operations.\n    \"\"\"\n    chunks = []\n    current_chunk_tokens = []\n    current_length = 0\n\n    for unit in discourse_units:\n        unit_tokens = tokenizer.encode(unit, add_special_tokens=True)\n        unit_length = len(unit_tokens)\n\n        if current_length + unit_length <= max_length:\n            current_chunk_tokens.extend(unit_tokens)\n            current_length += unit_length\n        else:\n            # Decode and append the current chunk\n            chunk_text = tokenizer.decode(\n                current_chunk_tokens, skip_special_tokens=True)\n            chunks.append(chunk_text)\n\n            # Prepare overlap tokens\n            overlap_tokens = current_chunk_tokens[-overlap_size:]\n            current_chunk_tokens = overlap_tokens + unit_tokens\n            current_length = len(current_chunk_tokens)\n\n    # Append any remaining tokens as the last chunk\n    if current_chunk_tokens:\n        chunk_text = tokenizer.decode(\n            current_chunk_tokens, skip_special_tokens=True)\n        chunks.append(chunk_text)\n\n    # Write or read chunks as before\n    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_chunks.json'\n    if not os.path.exists(output_path) or overwrite:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        with open(output_path, 'w') as f:\n            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n    else:\n        with open(output_path, 'r') as f:\n            discourse_units = json.load(f)\n\n    print(f\"Total Chunks Created: {len(chunks)}\")\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:28.853553Z","iopub.execute_input":"2024-11-23T03:31:28.853849Z","iopub.status.idle":"2024-11-23T03:31:28.864839Z","shell.execute_reply.started":"2024-11-23T03:31:28.853805Z","shell.execute_reply":"2024-11-23T03:31:28.863965Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Create and Tokenize Dataset\n# ----------------------------- #\n\n# To Do - make book titles and prompt generic so\ndef create_tokenized_dataset(chunks, tokenizer, max_length=1024, num_proc=2):\n\n    # Create a Dataset object from chunks\n\n    book_title = 'Psychology of the Unconscious by C. G. Jung'\n    wikipedia_prompt = \"\"\"\n    Psychology Book\n\n    ### Title: {}\n\n    ### Article: {}\n    \"\"\"\n\n    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\n    def formatting_prompts_func(examples):\n        titles = book_title\n        texts = examples[\"text\"]\n        outputs = []\n        for title, text in zip([book_title]*len(chunks), texts):\n            text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n            outputs.append(text)\n        return {\"text\": outputs, }\n    pass\n\n    # convert chunks variable to huggingface dataset\n\n    from datasets import Dataset\n\n    dataset = Dataset.from_dict({\"text\": chunks})\n\n    dataset = dataset.map(formatting_prompts_func,\n                          batched=True, num_proc=num_proc)\n    # Split the dataset into training and validation sets\n    split = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = split['train']\n    eval_dataset = split['test']\n\n    print(len(dataset))\n    # Find the maximum length of the text field in the entire dataset\n    max_length = max(len(text) for text in dataset['text'])\n    print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n    print(f\"Training Dataset Size: {len(train_dataset)}\")\n#     print(f\"First 5 rows of training dataset: {train_dataset[:5]}\")\n    print(f\"Validation Dataset Size: {len(eval_dataset)}\")\n#     print(f\"First 5 rows of validation dataset: {eval_dataset[:5]}\")\n    return train_dataset, eval_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:28.865837Z","iopub.execute_input":"2024-11-23T03:31:28.866079Z","iopub.status.idle":"2024-11-23T03:31:28.878711Z","shell.execute_reply.started":"2024-11-23T03:31:28.866055Z","shell.execute_reply":"2024-11-23T03:31:28.877969Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ----------------------------- #\n# Part 6: Set up environment and other important utilities\n# ----------------------------- #\n\ndef setup_environment():\n    \"\"\"\n    Initializes the Accelerator for distributed training.\n    \"\"\"\n    return Accelerator()\n\n\ndef get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n    \"\"\"\n    Defines a custom learning rate scheduler with warmup, constant, and linear annealing phases.\n    \"\"\"\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return current_step / num_warmup_steps  # Linear warmup\n        elif current_step < initial_phase_steps:\n            return 1.0  # Constant learning rate for initial phase\n        else:\n            # Linear annealing for the remaining steps\n            return 1.0 - ((current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps))\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\ndef setup_training_steps(initial_rows, annealing_rows, batch_size, gradient_accumulation_steps, num_epochs):\n    \"\"\"\n    Calculates total and initial training steps based on dataset size and training parameters.\n    \"\"\"\n    total_rows = initial_rows + annealing_rows\n    total_steps = (total_rows * num_epochs) // (batch_size *\n                                                gradient_accumulation_steps)\n    initial_steps = (initial_rows * num_epochs) // (batch_size *\n                                                    gradient_accumulation_steps)\n    return max(1, total_steps), max(1, initial_steps)\n\n\ndef print_memory_usage(step_desc):\n    \"\"\"\n    Prints the CUDA memory summary if CUDA is available.\n    \"\"\"\n    if torch.cuda.is_available():\n        print(f\"Memory Usage at {step_desc}:\")\n        print(torch.cuda.memory_summary())\n        print(\"\\n\")\n    else:\n        print(f\"No CUDA available at {step_desc}.\\n\")\n\n\ndef inference(model, tokenizer):\n    \"\"\"\n    Runs inference using the trained model.\n    \"\"\"\n    # Define sample prompts\n    prompts = [\n        \"Explain what is medical anthropology and its importance in elevating the quality of life?\",\n        \"Explain what are the types of Jungian archetypes and how they manifest in the human psyche?\"\n    ]\n\n    for prompt in prompts:\n        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_length=256)\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n        \n#  Login to Huggingface\nfrom huggingface_hub import login\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef setup_huggingface_access():\n    \"\"\"Setup Hugging Face access with token from environment or manual input\"\"\"\n    # First try to get token from environment variable\n    token = os.getenv('HUGGINGFACE_TOKEN')\n    \n    if not token:\n        # If not in environment, prompt for token\n        token = input(\"Enter your Hugging Face token: \")\n        \n    if token:\n        try:\n            login(token, add_to_git_credential=True)\n            print(\"Successfully logged in to Hugging Face!\")\n        except Exception as e:\n            print(f\"Failed to log in to Hugging Face: {str(e)}\")\n            return False\n    else:\n        print(\"No Hugging Face token provided\")\n        return False\n    \n    return True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:28.880020Z","iopub.execute_input":"2024-11-23T03:31:28.880357Z","iopub.status.idle":"2024-11-23T03:31:28.911025Z","shell.execute_reply.started":"2024-11-23T03:31:28.880320Z","shell.execute_reply":"2024-11-23T03:31:28.910070Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Load the Tokenizer and Model\n# ----------------------------- #\n\ndef load_model_and_tokenizer(base_model_slug, max_seq_length=4096, dtype=None, load_in_4bit=True, device_map = \"auto\"):\n    \"\"\"\n    Load and configure the model and tokenizer with specified parameters.\n    \n    Args:\n        base_model_slug (str): The model identifier from HuggingFace\n        max_seq_length (int): Maximum sequence length for the model\n        dtype: Data type for model parameters\n        load_in_4bit (bool): Whether to use 4-bit quantization\n        \n    Returns:\n        tuple: (model, tokenizer)\n    \"\"\"\n    # Check CUDA is available\n    import torch\n    if not torch.cuda.is_available():\n        print(\"WARNING: CUDA is not available. This might affect performance.\")\n    else:\n        print(\"CUDA available\")\n        \n    # Check available GPUs\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of available GPUs: {num_gpus}\")\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_properties(i).name}\")\n\n    # Determine optimal device map based on available GPUs\n    device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using single device: {device_map}\")\n\n    # Model paths\n    model_name = base_model_slug\n    models_dir = os.path.join(os.path.dirname(\"~/\"), \"models\")\n    model_path = os.path.join(models_dir, model_name)\n\n    # Create models directory if it doesn't exist\n    if not os.path.exists(models_dir):\n        os.makedirs(models_dir)\n\n    # Load or download model\n    try:\n        if os.path.exists(model_path):\n            print(f\"Loading model from local path: {model_path}\")\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name=base_model_slug,\n                max_seq_length=max_seq_length,\n                dtype=dtype,\n                load_in_4bit=load_in_4bit,\n                token=os.getenv('HUGGINGFACE_TOKEN'),\n            )\n        else:\n            print(f\"Downloading model from HuggingFace: {model_name}\")\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name=base_model_slug,\n                max_seq_length=max_seq_length,\n                dtype=dtype,\n                load_in_4bit=load_in_4bit,\n                token=os.getenv('HUGGINGFACE_TOKEN'),\n            )\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        raise\n\n    # Configure PEFT model\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=128,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n        target_modules=[\n            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n            \"gate_proj\", \"up_proj\", \"down_proj\",\n            \"embed_tokens\", \"lm_head\",  # Add for continual pretraining\n        ],\n        lora_alpha=32,\n        lora_dropout=0,  # Supports any, but = 0 is optimized\n        bias=\"none\",    # Supports any, but = \"none\" is optimized\n        use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n        random_state=3407,\n        use_rslora=True,   # We support rank stabilized LoRA\n        loftq_config=None,  # And LoftQ\n    )\n\n    print(\"Model and tokenizer loaded successfully!\")\n    return model, tokenizer\n\n    # Print model device distribution\n    if hasattr(model, 'hf_device_map'):\n        print(\"\\nModel Device Distribution:\")\n        for name, device in model.hf_device_map.items():\n            print(f\"{name}: {device}\")\n\n    # Print memory usage per GPU\n    print(\"\\nGPU Memory Usage after model loading:\")\n    for i in range(num_gpus):\n        memory_used = torch.cuda.memory_allocated(i) / (1024**3)\n        memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n        print(f\"GPU {i}: {memory_used:.2f}GB / {memory_total:.2f}GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:28.912382Z","iopub.execute_input":"2024-11-23T03:31:28.912735Z","iopub.status.idle":"2024-11-23T03:31:28.922841Z","shell.execute_reply.started":"2024-11-23T03:31:28.912698Z","shell.execute_reply":"2024-11-23T03:31:28.921924Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\n\n# Set the environment variable\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\nos.environ[\"WANDB_API_KEY\"] = \"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\"\n# Verify it's set correctly\nprint(os.getenv(\"HUGGINGFACE_TOKEN\"))\nprint(os.getenv(\"WANDB_API_KEY\"))\n!huggingface-cli login --token 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 --add-to-git-credential","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:28.923713Z","iopub.execute_input":"2024-11-23T03:31:28.923984Z","iopub.status.idle":"2024-11-23T03:31:30.627574Z","shell.execute_reply.started":"2024-11-23T03:31:28.923959Z","shell.execute_reply":"2024-11-23T03:31:30.626541Z"}},"outputs":[{"name":"stdout","text":"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\n1ca3c5e9222c2504acbc07cf7f88267006ae68c4\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Token is valid (permission: fineGrained).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Configure Environment Variables & Create Main Variables\n# ----------------------------- #\n\n# Unsloth modell initialization variables\nmax_seq_length = 256 # Choose any! We auto support RoPE Scaling internally!\nmax_length = max_seq_length\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nbase_model_slug = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\nbase_model_name = \"lora_model_pum\"\nchunks_max_length = max_seq_length\noverlap_size = 1\n# Define your parameters\nbatchSize = 2\nga = 16\nmaxSteps = 10\n# warmupSteps = 10\nnumTrainEpochs = 1\nlRate = 5e-5\nembLRate = 1e-5\noptim = \"adamw_8bit\"\nlrSchedule = \"linear\"\ndataset_slug = \"psychology_of_unconscious\"\n\nfrom datetime import datetime\nimport pytz\nimport wandb\n# Get the current date and time in Indian Standard Time (IST)\nist = pytz.timezone('Asia/Kolkata')\ncurrent_datetime = datetime.now(ist)\n\n# Format the datetime string\n# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\nformatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n\n# Define Run Name\nrun_name = f\"\"\"Kaggle-quantumLeap-{formatted_datetime}-{base_model_slug}-{dataset_slug}-{max_seq_length}_maxSeqLength-{max_length}_maxLength-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{numTrainEpochs}_numTrainEpochs-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n\n# Initialize Weights & Biases\n# It's recommended to set your W&B API key as an environment variable for security.\nwandb.login(key=os.getenv(\"WANDB_API_KEY\"))\nwandb.init(project=\"KAGGLE-quantumLeap\", name=run_name)\n\n# Set environment variables before importing torch-related modules\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:30.629460Z","iopub.execute_input":"2024-11-23T03:31:30.629768Z","iopub.status.idle":"2024-11-23T03:31:36.486458Z","shell.execute_reply.started":"2024-11-23T03:31:30.629737Z","shell.execute_reply":"2024-11-23T03:31:36.485711Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molabs-asia\u001b[0m (\u001b[33molabs-asia-olabs-pro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112836933333863, max=1.0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5042b7dab0dc4e16bee631f79c45aebf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/quantumLeap/wandb/run-20241123_033133-x3lgcc5l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap/runs/x3lgcc5l' target=\"_blank\">Kaggle-quantumLeap-20241123_090130-unsloth/Llama-3.2-1B-Instruct-bnb-4bit-psychology_of_unconscious-256_maxSeqLength-256_maxLength-2_batchSize-16_ga-10_maxSteps-1_numTrainEpochs-5e-05_lRate-1e-05_embLRate-adamw_8bit_optim-linear_lrSchedule</a></strong> to <a href='https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap/runs/x3lgcc5l' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap/runs/x3lgcc5l</a>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"!jupyter nbextension enable --py widgetsnbextension","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:36.487412Z","iopub.execute_input":"2024-11-23T03:31:36.487666Z","iopub.status.idle":"2024-11-23T03:31:37.820059Z","shell.execute_reply.started":"2024-11-23T03:31:36.487639Z","shell.execute_reply":"2024-11-23T03:31:37.819293Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Enabling notebook extension jupyter-js-widgets/extension...\n      - Validating: \u001b[32mOK\u001b[0m\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\n# ----------------------------- #\n# Part 9: Data Processing\n# ----------------------------- #\n\n# # Perform Inference Before Training\n# inference(model, tokenizer)\n\n# Set number of processes to use for data loading\nnum_cpus = multiprocessing.cpu_count()\nnum_proc = max(num_cpus-2, 2)  # Adjust based on prior recommendations\nprint(f\"Number of CPU cores: {num_cpus}\")\nprint(f\"Number of processes: {num_proc}\")\n\n# Login to Hugging Face\nif not setup_huggingface_access():\n    raise Exception(\"Failed to setup Hugging Face access. Please check your token.\")\n\n# Load Model and Tokenizer\nmodel, tokenizer = load_model_and_tokenizer(base_model_slug)\nprint(f\"Model Device: {model.device}\")\n\n# Load and Clean Text Data\nfile_path = \"/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\nclean_text = load_and_clean_text(file_path)\n\n# Parse Discourse Units\ndiscourse_units = parse_discourse_units(clean_text, overwrite=True)\n\n# Create Chunks\nchunks = create_chunks(\n    discourse_units,\n    tokenizer,\n    max_length=max_length,\n    overlap_size=overlap_size,\n    overwrite=True,\n)\n\n# Create Tokenized Dataset\ntrain_dataset, eval_dataset = create_tokenized_dataset(\n    chunks, tokenizer, max_length)\n\n# Save datasets as Hugging Face `datasets`\ntrain_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\neval_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Model is now on device: {next(model.parameters()).device}\")\n\n### To Do - Make the below as dynamic and as a functio\n# # Uncomment following if you want to just load the data from temp directory\n# from datasets import load_from_disk\n\n# train_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n# eval_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:31:37.821876Z","iopub.execute_input":"2024-11-23T03:31:37.822289Z","iopub.status.idle":"2024-11-23T03:32:56.162771Z","shell.execute_reply.started":"2024-11-23T03:31:37.822246Z","shell.execute_reply":"2024-11-23T03:32:56.161853Z"}},"outputs":[{"name":"stdout","text":"Number of CPU cores: 4\nNumber of processes: 2\nToken is valid (permission: fineGrained).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nSuccessfully logged in to Hugging Face!\nCUDA available\nNumber of available GPUs: 1\nGPU 0: Tesla T4\nUsing single device: cuda:0\nDownloading model from HuggingFace: unsloth/Llama-3.2-1B-Instruct-bnb-4bit\n==((====))==  Unsloth 2024.11.9: Fast Llama patching. Transformers = 4.46.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150909e2c7ec48338dd04f9f291fd283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5fb1b189c846cb9a7c1e12a6f9d4b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44583122b47d46caadccfd785473cfd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae937b8709eb498ca1f69945a4cd7dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a087779f004426da1cefb4dceed84c7"}},"metadata":{}},{"name":"stdout","text":"Unsloth: Offloading input_embeddings to disk to save VRAM\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/unsloth/models/_utils.py:746: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Offloading output_embeddings to disk to save VRAM\n","output_type":"stream"},{"name":"stderr","text":"Unsloth 2024.11.9 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Training embed_tokens in mixed precision to save VRAM\nUnsloth: Training lm_head in mixed precision to save VRAM\nModel and tokenizer loaded successfully!\nModel Device: cpu\nTotal Discourse Units: 6175\nTotal Chunks Created: 871\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/871 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9506a2a3d9c420092f52cf9810436d8"}},"metadata":{}},{"name":"stdout","text":"871\nThe maximum length of the text field in the dataset is: 2948 characters\nTraining Dataset Size: 783\nValidation Dataset Size: 88\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/783 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b85f210a00d241288de1077aabc8d210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/88 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"675eee6a2d14478abfbeb9baeb7022bc"}},"metadata":{}},{"name":"stdout","text":"Model is now on device: cuda:0\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ----------------------------- #\n# Part 8: Configure Training Arguments\n# ----------------------------- #\n\nimport torch\n\n# Ensure device is set to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transfer model to GPU\nmodel = model.to(device)\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = batchSize,\n        gradient_accumulation_steps = ga,\n\n        # Use warmup_ratio and num_train_epochs for longer runs!\n        max_steps = maxSteps,\n#         warmup_steps = warmupSteps,\n        # warmup_ratio = 0.1,\n        num_train_epochs = numTrainEpochs,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = lRate,\n        embedding_learning_rate = embLRate,\n\n        # Ensure fp16 precision is used\n        fp16 = True,\n        bf16 = False,\n\n        logging_steps = 1,\n        optim = optim,\n        weight_decay = 0.01,\n        lr_scheduler_type = lrSchedule,\n        seed = 3407,\n        output_dir = \"outputs\",\n\n        report_to=[\"tensorboard\", \"wandb\"],\n        logging_dir=f\"./trel-fft-logs/{run_name}\",\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:32:56.164158Z","iopub.execute_input":"2024-11-23T03:32:56.164470Z","iopub.status.idle":"2024-11-23T03:33:00.646533Z","shell.execute_reply.started":"2024-11-23T03:32:56.164438Z","shell.execute_reply":"2024-11-23T03:33:00.645456Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/783 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92fffe8c4fe9434b85d314f2a4a88125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/88 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1c3e6f04b09439eadb1cdedcf36fd4f"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ----------------------------- #\n# Part 11: Start Training\n# ----------------------------- #\n\n#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\ntrainer_stats = trainer.train()\n\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:33:00.648119Z","iopub.execute_input":"2024-11-23T03:33:00.648450Z","iopub.status.idle":"2024-11-23T03:33:08.999678Z","shell.execute_reply.started":"2024-11-23T03:33:00.648405Z","shell.execute_reply":"2024-11-23T03:33:08.991631Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n4.4 GB of memory reserved.\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 783 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 16\n\\        /    Total batch size = 32 | Total steps = 10\n \"-____-\"     Number of trainable parameters = 615,514,112\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\nUnsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/language/core.py:35\u001b[0m, in \u001b[0;36mbuiltin.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you forget to add @triton.jit ? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(`_builder` argument must be provided outside of JIT functions.)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/language/core.py:1534\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(input, other, acc, input_precision, allow_tf32, max_num_imprecise_acc, out_dtype, _builder)\u001b[0m\n\u001b[1;32m   1533\u001b[0m max_num_imprecise_acc \u001b[38;5;241m=\u001b[39m _constexpr_to_value(max_num_imprecise_acc)\n\u001b[0;32m-> 1534\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msemantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_imprecise_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_builder\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/language/semantic.py:1355\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(lhs, rhs, acc, input_precision, max_num_imprecise_acc, out_dtype, builder)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mis_block() \u001b[38;5;129;01mand\u001b[39;00m rhs\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mis_block()\n\u001b[0;32m-> 1355\u001b[0m \u001b[43massert_dtypes_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_fp8e4b15() \u001b[38;5;129;01mor\u001b[39;00m rhs\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_fp8e4b15():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/language/semantic.py:1328\u001b[0m, in \u001b[0;36mdot.<locals>.assert_dtypes_valid\u001b[0;34m(lhs_dtype, rhs_dtype, options)\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1328\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m lhs_dtype \u001b[38;5;241m==\u001b[39m rhs_dtype, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst input (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlhs_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and second input (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrhs_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must have the same dtype!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mAssertionError\u001b[0m: First input (fp16) and second input (fp32) must have the same dtype!","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpu_stats\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Max memory = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_memory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_gpu_memory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB of memory reserved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#@title Show final memory and time stats\u001b[39;00m\n\u001b[1;32m     15\u001b[0m used_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n","File \u001b[0;32m<string>:157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n","File \u001b[0;32m<string>:380\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/_utils.py:1027\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_compile.py:31\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:1082\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1081\u001b[0m ):\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:985\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m HAS_CUT_CROSS_ENTROPY \u001b[38;5;129;01mand\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    984\u001b[0m     n_items \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_items\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 985\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mfused_linear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlm_weight\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    993\u001b[0m         output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth_zoo/loss_utils.py:147\u001b[0m, in \u001b[0;36mfused_linear_cross_entropy\u001b[0;34m(hidden_states, lm_weight, labels, num_items_in_batch, ignore_index, reduction, logit_softcapping, accuracy_threshold)\u001b[0m\n\u001b[1;32m    145\u001b[0m reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logit_softcapping \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: logit_softcapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_eps\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccuracy_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m num_items_in_batch\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cut_cross_entropy/linear_cross_entropy.py:58\u001b[0m, in \u001b[0;36mlinear_cross_entropy\u001b[0;34m(e, c, targets, ignore_index, softcap, reduction, shift, filter_eps, impl)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCCE does not support MacOS. Please use torch_compile when running on MacOS instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m cce_linear_cross_entropy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcce_linear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_eps\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_compile_linear_cross_entropy(\n\u001b[1;32m     63\u001b[0m         e, c, targets, ignore_index, softcap, reduction, shift\n\u001b[1;32m     64\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cut_cross_entropy/cce.py:168\u001b[0m, in \u001b[0;36mcce_linear_cross_entropy\u001b[0;34m(e, c, targets, ignore_index, softcap, reduction, shift, filter_eps)\u001b[0m\n\u001b[1;32m    165\u001b[0m e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    166\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinear_cross_entropy_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCCEParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_handle_eps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cut_cross_entropy/cce.py:125\u001b[0m, in \u001b[0;36mlinear_cross_entropy_apply\u001b[0;34m(e, c, params)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinear_cross_entropy_apply\u001b[39m(\n\u001b[1;32m    121\u001b[0m     e: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    122\u001b[0m     c: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    123\u001b[0m     params: CCEParams,\n\u001b[1;32m    124\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 125\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mLinearCrossEntropyFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params\u001b[38;5;241m.\u001b[39mshift \u001b[38;5;129;01mand\u001b[39;00m params\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cut_cross_entropy/cce.py:46\u001b[0m, in \u001b[0;36mLinearCrossEntropyFunction.forward\u001b[0;34m(ctx, e, c, params)\u001b[0m\n\u001b[1;32m     43\u001b[0m needs_grad \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;129;01mor\u001b[39;00m c\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[1;32m     44\u001b[0m return_logit_avg \u001b[38;5;241m=\u001b[39m needs_grad \u001b[38;5;129;01mand\u001b[39;00m params\u001b[38;5;241m.\u001b[39mfilter_eps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mcce_lse_forward_kernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logit_avg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_logit_avg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_logit_avg:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, \u001b[38;5;28mtuple\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cut_cross_entropy/cce_lse_forward.py:182\u001b[0m, in \u001b[0;36mcce_lse_forward_kernel\u001b[0;34m(e, c, valids, softcap, return_logit_avg)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrid\u001b[39m(META) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (triton\u001b[38;5;241m.\u001b[39mcdiv(B, META[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLOCK_B\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m*\u001b[39m triton\u001b[38;5;241m.\u001b[39mcdiv(V, META[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLOCK_V\u001b[39m\u001b[38;5;124m\"\u001b[39m]),)\n\u001b[0;32m--> 182\u001b[0m \u001b[43m_cce_lse_forward_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_avg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_locks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mB_BIN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_bin_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_logit_avg:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logit_avg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/runtime/autotuner.py:338\u001b[0m, in \u001b[0;36mHeuristics.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, heur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    337\u001b[0m     kwargs[v] \u001b[38;5;241m=\u001b[39m heur({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marg_names, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs})\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/runtime/autotuner.py:338\u001b[0m, in \u001b[0;36mHeuristics.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, heur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    337\u001b[0m     kwargs[v] \u001b[38;5;241m=\u001b[39m heur({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marg_names, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs})\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/runtime/jit.py:662\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 662\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Check that used global values have not changed.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/compiler/compiler.py:276\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    274\u001b[0m codegen_fns \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_codegen_implementation()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    278\u001b[0m     filter_traceback(e)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/compiler/compiler.py:113\u001b[0m, in \u001b[0;36mASTSource.make_ir\u001b[0;34m(self, options, codegen_fns, context)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, context):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mCompilationError\u001b[0m: at 60:16:\n\n    accum = tl.zeros((BLOCK_B, BLOCK_V), dtype=tl.float32)\n    for d in range(0, tl.cdiv(D, BLOCK_D)):\n        # Load the next block of A and B, generate a mask by checking the K dimension.\n        # If it is out of bounds, set it to 0.\n        if EVEN_D:\n            e = tl.load(e_ptrs)\n            c = tl.load(c_ptrs)\n        else:\n            e = tl.load(e_ptrs, mask=offs_d[None, :] < D - d * BLOCK_D, other=0.0)\n            c = tl.load(c_ptrs, mask=offs_d[:, None] < D - d * BLOCK_D, other=0.0)\n        accum = tl.dot(e, c, accum, input_precision=DOT_PRECISION)\n                ^"],"ename":"CompilationError","evalue":"at 60:16:\n\n    accum = tl.zeros((BLOCK_B, BLOCK_V), dtype=tl.float32)\n    for d in range(0, tl.cdiv(D, BLOCK_D)):\n        # Load the next block of A and B, generate a mask by checking the K dimension.\n        # If it is out of bounds, set it to 0.\n        if EVEN_D:\n            e = tl.load(e_ptrs)\n            c = tl.load(c_ptrs)\n        else:\n            e = tl.load(e_ptrs, mask=offs_d[None, :] < D - d * BLOCK_D, other=0.0)\n            c = tl.load(c_ptrs, mask=offs_d[:, None] < D - d * BLOCK_D, other=0.0)\n        accum = tl.dot(e, c, accum, input_precision=DOT_PRECISION)\n                ^","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"model.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:33:09.000494Z","iopub.status.idle":"2024-11-23T03:33:09.000832Z","shell.execute_reply.started":"2024-11-23T03:33:09.000663Z","shell.execute_reply":"2024-11-23T03:33:09.000680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:33:09.001725Z","iopub.status.idle":"2024-11-23T03:33:09.002076Z","shell.execute_reply.started":"2024-11-23T03:33:09.001919Z","shell.execute_reply":"2024-11-23T03:33:09.001936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}