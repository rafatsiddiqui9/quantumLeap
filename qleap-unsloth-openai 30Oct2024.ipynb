{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- #\n",
    "# # Part 1.1: Install and Setup Libraries\n",
    "# # ----------------------------- #\n",
    "\n",
    "# # run below in terminal only. This code works only for Ola Krutrim Cloud Instance. Restart once you have installed the following\n",
    "# # pip install uv #install this in the virtual environment where you want to execute the notebook.\n",
    "# # uv venv virtualenvironment # if you are not in an externally managed environment, then you can run this\n",
    "# # source virtualenvironment/bin/activate # if you were able to run above code, then activate. DO NOT use --system flag in subsequent lines if you are able to do thi\n",
    "# !uv pip install unsloth --system\n",
    "# !uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --system\n",
    "# !uv pip install wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets --system\n",
    "# # restart once you have installed all of the above\n",
    "# !nvidia-smi\n",
    "\n",
    "# !nvcc --version\n",
    "\n",
    "# import torch\n",
    "# print(torch.__version__)          # Should reflect 2.5.0+cu124\n",
    "# print(torch.version.cuda)         # Should output 12.4\n",
    "# print(torch.cuda.is_available())  # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 1.2: Import Libraries\n",
    "# ----------------------------- #\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import xformers\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "import ipywidgets\n",
    "import unsloth\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import argparse\n",
    "import ast\n",
    "import logging\n",
    "import csv\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported, UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is available\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print('punkt was already available.')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    print('punkt was not available. It has been downloaded')\n",
    "\n",
    "# Initialize spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print('en_core_web_sm was already available.')\n",
    "except OSError:\n",
    "    print(\"SpaCy English model not found. Downloading...\")\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 1.2: Create Main Variables\n",
    "# ----------------------------- #\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "base_model_slug = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "base_model_name = \"lora_model_pum\"\n",
    "chunks_max_length = max_seq_length\n",
    "overlap_size = 1\n",
    "# Define your parameters\n",
    "batchSize = 2\n",
    "ga = 8\n",
    "maxSteps = 120\n",
    "lRate = 5e-5\n",
    "embLRate = 1e-5\n",
    "optim = \"adamw_8bit\"\n",
    "lrSchedule = \"linear\"\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 2: Load and Clean the Text Data\n",
    "# ----------------------------- #\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    # # Remove Project Gutenberg's license text and headers/footers\n",
    "    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
    "    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
    "\n",
    "    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n",
    "    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n",
    "    return text.strip()\n",
    "\n",
    "# Replace 'psychology_of_unconscious.txt' with your actual file path\n",
    "file_path = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/psychology_of_unconscious.txt'\n",
    "clean_text = load_and_clean_text(file_path)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 3: Parse Text into Discourse Units\n",
    "# ----------------------------- #\n",
    "\n",
    "# def parse_discourse_units(text):\n",
    "#     \"\"\"\n",
    "#     Parses text into discourse units using spaCy.\n",
    "#     Currently splits text into sentences.\n",
    "#     \"\"\"\n",
    "#     paragraphs = text.split('\\n\\n')\n",
    "#     paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
    "    \n",
    "#     discourse_units = []\n",
    "#     for para in paragraphs:\n",
    "#         doc = nlp(para)\n",
    "#         sentences = [sent.text for sent in doc.sents]\n",
    "#         discourse_units.extend(sentences)\n",
    "#     return discourse_units\n",
    "\n",
    "# discourse_units = parse_discourse_units(clean_text)\n",
    "\n",
    "# # Save discourse_units to a JSON file\n",
    "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "# Load discourse_units from the JSON file\n",
    "with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.json', 'r', encoding='utf-8') as f:\n",
    "    discourse_units = json.load(f)\n",
    "\n",
    "len(discourse_units)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 4: Create Chunks Using Hybrid Strategy\n",
    "# ----------------------------- #\n",
    "\n",
    "def create_chunks(discourse_units, tokenizer, max_length=1024, overlap_size=1):\n",
    "    \"\"\"\n",
    "    Creates chunks from discourse units using a sliding window with overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for unit in discourse_units:\n",
    "        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n",
    "        unit_length = len(unit_tokens)\n",
    "\n",
    "        if current_length + unit_length <= max_length:\n",
    "            current_chunk.append(unit)\n",
    "            current_length += unit_length\n",
    "        else:\n",
    "            # Append the current chunk\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            # Create overlap\n",
    "            overlap_text = ' '.join(current_chunk)[-overlap_size:]\n",
    "            overlap_tokens = tokenizer.encode(overlap_text, add_special_tokens=False)\n",
    "            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n",
    "            # Start new chunk with overlap and current unit\n",
    "            current_chunk = [overlap_text, unit]\n",
    "            current_length = len(tokenizer.encode(overlap_text, add_special_tokens=False)) + unit_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model_slug, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 5: : Load the Tokenizer and Model\n",
    "# ----------------------------- #\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 6: Create Chunks (After Tokenizer is Loaded)\n",
    "# ----------------------------- #\n",
    "\n",
    "chunks = create_chunks(discourse_units, tokenizer, max_length=chunks_max_length, overlap_size=overlap_size)\n",
    "\n",
    "# Save chunks to a JSON file (Optional)\n",
    "with open(f'/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final_{chunks_max_length}_{overlap_size}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# # If you need to reload from JSON (Optional)\n",
    "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.json', 'r', encoding='utf-8') as f:\n",
    "#     chunks = json.load(f)\n",
    "    \n",
    "print(len(chunks))\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 7: Create and Tokenize Dataset\n",
    "# ----------------------------- #\n",
    "\n",
    "# Create a Dataset object from chunks\n",
    "\n",
    "book_title = 'Psychology of the Unconscious by C. G. Jung'\n",
    "wikipedia_prompt = \"\"\"\n",
    "### Title: {}\n",
    "\n",
    "### Article: {}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    titles = book_title\n",
    "    texts  = examples[\"text\"]\n",
    "    outputs = []\n",
    "    for title, text in zip([book_title]*len(chunks), texts):\n",
    "        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n",
    "        outputs.append(text)\n",
    "    return { \"text\" : outputs, }\n",
    "pass\n",
    "\n",
    "# convert chunks variable to huggingface dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": chunks})\n",
    "\n",
    "# dataset = dataset.train_test_split(test_size = 0.1)[\"train\"]\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "len(dataset)\n",
    "\n",
    "# Find the maximum length of the text field in the entire dataset\n",
    "max_length = max(len(text) for text in dataset['text'])\n",
    "print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 8: Configure Training Arguments\n",
    "# ----------------------------- #\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import wandb\n",
    "\n",
    "# Get the current date and time in Indian Standard Time (IST)\n",
    "ist = pytz.timezone('Asia/Kolkata')\n",
    "current_datetime = datetime.now(ist)\n",
    "\n",
    "# Format the datetime string\n",
    "# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n",
    "formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the run name with the current date and time\n",
    "run_name = f\"\"\"Unsloth-CPT-Base-{formatted_datetime}-{base_model_slug}-{max_seq_length}_max_seq_length-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "# It's recommended to set your W&B API key as an environment variable for security.\n",
    "# Example: export WANDB_API_KEY=\"your_api_key\"\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))  # Consider using environment variables for security\n",
    "wandb.init(project=\"Unsloth-CPT\", name=run_name)\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = batchSize,\n",
    "        gradient_accumulation_steps = ga,\n",
    "\n",
    "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
    "        max_steps = maxSteps,\n",
    "        warmup_steps = 10,\n",
    "        # warmup_ratio = 0.1,\n",
    "        # num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate =lRate,\n",
    "        embedding_learning_rate = embLRate,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = optim,\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = lrSchedule,\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        \n",
    "        \n",
    "        report_to=[\"tensorboard\", \"wandb\"],\n",
    "        logging_dir=f\"./trel-fft-logs/{run_name}\",\n",
    "\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 9: Define Compute Metrics Function\n",
    "# ----------------------------- #\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes perplexity based on model predictions and labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # Convert to torch tensors\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Ensure shapes match\n",
    "    if logits.shape[:2] != labels.shape:\n",
    "        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n",
    "    \n",
    "    # Shift logits and labels\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "    # Check label values\n",
    "    if shift_labels.max() >= model.config.vocab_size:\n",
    "        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n",
    "    \n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 10: Initialize logging\n",
    "# ----------------------------- #\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    level=logging.INFO,  # Set to DEBUG for more detailed logs\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "    \n",
    "# ----------------------------- #\n",
    "# Part 11: Start Training\n",
    "# ----------------------------- #\n",
    "\n",
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 12: Save the Bsae Fine-Tuned Model\n",
    "# ----------------------------- #\n",
    "\n",
    "model.save_pretrained(\"lora_model_pum\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model_pum\")\n",
    "\n",
    "!huggingface-cli login --token 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 --add-to-git-credential\n",
    "if False:\n",
    "    model.push_to_hub(\"olabs-ai/qLeap_base_v01\", token = os.getenv(\"HUGGINGFACE_TOKEN\")) # Online saving\n",
    "    tokenizer.push_to_hub(\"olabs-ai/qLeap_base_v01\", token = os.getenv(\"HUGGINGFACE_TOKEN\")) # Online saving\n",
    "    model.push_to_hub_gguf(\"olabs-ai/qLeap_base_v01\", tokenizer, quantization_method = \"q4_k_m\", token = os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "    \n",
    "# ----------------------------- #\n",
    "# Part 13: Generate Inference from Base Fine-Tuned Model for testing purpose\n",
    "# ----------------------------- #\n",
    "    \n",
    "# Check if model variable exists in current session\n",
    "try:\n",
    "    max_seq_length\n",
    "    print(\"max_seq_length already loaded in current session. Skipping loading.\")\n",
    "except (NameError, TypeError):\n",
    "    print(\"Loading model...\")\n",
    "    max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    import torch\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model_pum\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    \n",
    "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: \n",
    "concept_name: {}\n",
    "detailed_explanation: {}\n",
    "Given the concept in concept_name variable and its detailed explanation in detailed_explanation variable, provide an example scenario that illustrates the concept.\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    instruction_prompt.format(\n",
    "        \"Hero Archetype\", # concept_name\n",
    "        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "model.config.torch_dtype = torch.bfloat16 \n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512) # using repetition_penalty of 0.1 leads to repetition of text and high values lead to wierd grammer issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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 --add-to-git-credential\n",
    "if False:\n",
    "    model.push_to_hub(\"olabs-ai/qLeap_base_v02\", token = os.getenv(\"HUGGINGFACE_TOKEN\")) # Online saving\n",
    "    tokenizer.push_to_hub(\"olabs-ai/qLeap_base_v02\", token = os.getenv(\"HUGGINGFACE_TOKEN\")) # Online saving\n",
    "    model.push_to_hub_gguf(\"olabs-ai/qLeap_base_v02\", tokenizer, quantization_method = \"q4_k_m\", token = os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 1: Import Libraries\n",
    "# ----------------------------- #\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "from datasets import Dataset\n",
    "from transformers import TextStreamer\n",
    "from unsloth import (\n",
    "    FastLanguageModel,\n",
    "    UnslothTrainer,\n",
    "    UnslothTrainingArguments,\n",
    "    is_bfloat16_supported,\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='transformation_errors.log',\n",
    "    filemode='w',\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Define paths\n",
    "INPUT_CSV_PATH = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv'\n",
    "OUTPUT_JSON_PATH = '/root/qLeap-fft/data/input/Instruction_Data/transformed_data.json'\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 1.2: Create Main Variables\n",
    "# ----------------------------- #\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "base_model_slug = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
    "base_model_name = \"lora_model_pum\"\n",
    "chunks_max_length = max_seq_length\n",
    "overlap_size = 1\n",
    "# Define your parameters\n",
    "batchSize = 2\n",
    "ga = 8\n",
    "maxSteps = 120\n",
    "lRate = 5e-5\n",
    "embLRate = 1e-5\n",
    "optim = \"adamw_8bit\"\n",
    "lrSchedule = \"linear\"\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 2: Load and Clean the Text Data\n",
    "# ----------------------------- #\n",
    "\n",
    "def read_csv_data(input_csv_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Read and validate the input CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(input_csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            return list(reader)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def transform_data(original_data: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Transform the original data by expanding example scenarios.\"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    for idx, entry in enumerate(original_data, start=1):\n",
    "        concept_name = entry.get('concept_name', '').strip()\n",
    "        detailed_explanation = entry.get('detailed_explanation', '').strip()\n",
    "        example_scenario_str = entry.get('example_scenario', '').strip()\n",
    "\n",
    "        if not all([concept_name, detailed_explanation, example_scenario_str]):\n",
    "            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            example_scenarios = json.loads(example_scenario_str)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                example_scenarios = ast.literal_eval(example_scenario_str)\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}') has invalid example_scenario: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not isinstance(example_scenarios, list):\n",
    "            logging.error(f\"Entry {idx} ('{concept_name}'): example_scenario is not a list\")\n",
    "            continue\n",
    "\n",
    "        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n",
    "            if not isinstance(scenario, str):\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}'): non-string scenario at position {scenario_idx}\")\n",
    "                continue\n",
    "\n",
    "            new_data.append({\n",
    "                'concept_name': concept_name,\n",
    "                'detailed_explanation': detailed_explanation,\n",
    "                'example_scenario': scenario.strip()\n",
    "            })\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# Process and save the data\n",
    "original_data = read_csv_data(INPUT_CSV_PATH)\n",
    "transformed_data = transform_data(original_data)\n",
    "\n",
    "# Save transformed data\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)\n",
    "with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Processed {len(transformed_data)} examples\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 3: Create Instruction Prompt Template and Process Data\n",
    "# ----------------------------- #\n",
    "\n",
    "# Import Jinja2 for template rendering\n",
    "from jinja2 import Template\n",
    "\n",
    "# Define the instruction template using Jinja2 syntax\n",
    "instruction_template = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ user_message }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ assistant_response }}<|eot_id|>\"\"\"\n",
    "\n",
    "def create_instruction_dataset(transformed_data: List[Dict[str, str]]) -> Dataset:\n",
    "    \"\"\"Create an instruction dataset from transformed data.\"\"\"\n",
    "    template = Template(instruction_template)\n",
    "\n",
    "    def instruction_prompt_func(examples):\n",
    "        prompts = []\n",
    "        for cn, de, es in zip(\n",
    "            examples[\"concept_name\"],\n",
    "            examples[\"detailed_explanation\"],\n",
    "            examples[\"example_scenario\"]\n",
    "        ):\n",
    "            # Prepare the user message\n",
    "            user_message = f\"Explain the concept of {cn} and provide an example.\"\n",
    "\n",
    "            # Prepare the assistant's response\n",
    "            assistant_response = f\"{de}\\n\\nExample:\\n{es}\"\n",
    "\n",
    "            # Render the prompt using the template\n",
    "            rendered_prompt = template.render(\n",
    "                user_message=user_message,\n",
    "                assistant_response=assistant_response\n",
    "            )\n",
    "            prompts.append(rendered_prompt)\n",
    "        return {\"text\": prompts}\n",
    "\n",
    "    dataset = Dataset.from_list(transformed_data)\n",
    "    return dataset.map(instruction_prompt_func, batched=True)\n",
    "\n",
    "# Create the dataset\n",
    "instruction_dataset = create_instruction_dataset(transformed_data)\n",
    "\n",
    "# Print a sample to verify\n",
    "print(\"\\nSample processed example:\")\n",
    "print(instruction_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------- #\n",
    "# Part 4: Load the Tokenizer and Model\n",
    "# ----------------------------- #\n",
    "\n",
    "# Check if model variable exists in current session\n",
    "try:\n",
    "    model\n",
    "    print(\"Model already loaded in current session. Skipping model loading.\")\n",
    "except (NameError, TypeError):\n",
    "    print(\"Loading model...\")\n",
    "    # Empty CUDA cache before loading new model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_name,  # Base model slug \n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "# Define special tokens\n",
    "special_tokens = [\n",
    "    \"<|begin_of_text|>\",\n",
    "    \"<|end_of_text|>\",\n",
    "    \"<|start_header_id|>\",\n",
    "    \"<|end_header_id|>\",\n",
    "    \"<|eot_id|>\",\n",
    "    \"user\",\n",
    "    \"assistant\"\n",
    "]\n",
    "# we will first check if any of the special_tokens are not there in the tokenizer, if not then we will add, otherwise we will not do anything\n",
    "tokens_not_in_vocab = []\n",
    "for i in special_tokens:\n",
    "    if i not in tokenizer.get_vocab():\n",
    "        tokens_not_in_vocab.append(i)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "# Add special tokens to the tokenizer\n",
    "if len(tokens_not_in_vocab) > 0:\n",
    "    special_tokens_dict = {'additional_special_tokens': tokens_not_in_vocab}\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    print(f\"Added {num_added_toks} special tokens to the tokenizer.\")\n",
    "else:\n",
    "    num_added_toks = 0\n",
    "\n",
    "# Resize model embeddings if new tokens were added\n",
    "if num_added_toks > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Resized model embeddings to {len(tokenizer)} tokens.\")\n",
    "\n",
    "# Set eos_token_id and pad_token_id\n",
    "tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Use eos_token as pad_token\n",
    "\n",
    "# Update model configuration\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Configure model\n",
    "model.config.torch_dtype = torch.bfloat16\n",
    "\n",
    "# Prepare the model for training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=128,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        \"embed_tokens\", \"lm_head\",\n",
    "    ],  # Add for continual pretraining\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=True,   # We support rank stabilized LoRA\n",
    "    loftq_config=None, # And LoftQ\n",
    ")\n",
    "\n",
    "print(transformed_data[0])\n",
    "\n",
    "for i in special_tokens:\n",
    "    if i in tokenizer.get_vocab():\n",
    "        print(f\"token {i} in tokenizer.get_vocab\")\n",
    "        \n",
    "print(instruction_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ----------------------------- #\n",
    "# Part 5: Configure Training Arguments\n",
    "# ----------------------------- #\n",
    "\n",
    "def setup_training(model, tokenizer, dataset,\n",
    "                   batch_size=2, gradient_accumulation=8, max_steps=120):\n",
    "    \"\"\"Setup the training configuration.\"\"\"\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    import wandb\n",
    "\n",
    "    # Get the current date and time in Indian Standard Time (IST)\n",
    "    ist = pytz.timezone('Asia/Kolkata')\n",
    "    current_datetime = datetime.now(ist)\n",
    "\n",
    "    # Format the datetime string\n",
    "    # Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create the run name with the current date and time\n",
    "    run_name = f\"\"\"Unsloth-CPT-Instruct-{formatted_datetime}-{base_model_slug}-{max_seq_length}_max_seq_length-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n",
    "\n",
    "    # Initialize Weights & Biases\n",
    "    # Set your W&B API key as an environment variable for security.\n",
    "    # Example: export WANDB_API_KEY=\"your_api_key\"\n",
    "    wandb.login(key=os.getenv(\"WANDB_API_KEY\"))  # Assumes API key is set in the environment variable\n",
    "    wandb.init(project=\"Unsloth-CPT\", name=run_name)\n",
    "\n",
    "    training_args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size=batchSize,\n",
    "        gradient_accumulation_steps=ga,\n",
    "        max_steps=maxSteps,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=lRate,\n",
    "        embedding_learning_rate=embLRate,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=optim,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=lrSchedule,\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=[\"tensorboard\", \"wandb\"],\n",
    "        logging_dir=\"./trel-fft-logs\"\n",
    "    )\n",
    "\n",
    "    return UnslothTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        args=training_args\n",
    "    )\n",
    "\n",
    "# Setup trainer\n",
    "trainer = setup_training(model, tokenizer, instruction_dataset)\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 6: Save the Instruction Fine-Tuned Model\n",
    "# ----------------------------- #\n",
    "\n",
    "model.save_pretrained(\"lora_model_pum_instruct\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model_pum_instruct\")\n",
    "\n",
    "# Hugging Face authentication token should be set via environment variable or login\n",
    "\n",
    "# Uncomment and set to True if you wish to push to Hugging Face Hub\n",
    "if False:\n",
    "    model.push_to_hub(\"olabs-ai/qLeap_instruct_v03\", token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "    tokenizer.push_to_hub(\"olabs-ai/qLeap_instruct_v03\", token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "    model.push_to_hub_gguf(\"olabs-ai/qLeap_instruct_v03\", tokenizer, quantization_method=\"q4_k_m\", token=os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 7: Generate Inference from Instruction Fine-Tuned Model\n",
    "# ----------------------------- #\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_model_for_inference(\n",
    "    max_seq_length=1024,\n",
    "    dtype=None, \n",
    "    load_in_4bit=True,\n",
    "    model_name=\"lora_model_pum_instruct\"\n",
    "):\n",
    "    # Check if model variable exists in current session\n",
    "    try:\n",
    "        model\n",
    "        print(\"Model already loaded in current session. Skipping model loading.\")\n",
    "        return model, tokenizer\n",
    "    except (NameError, TypeError):\n",
    "        print(\"Loading model...\")\n",
    "        # Empty CUDA cache before loading new model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Initialize model and tokenizer\n",
    "        from unsloth import FastLanguageModel\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,  # Base model slug \n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "        )\n",
    "        \n",
    "        # Prepare the model for inference\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "        # Set model dtype\n",
    "        model.config.torch_dtype = torch.bfloat16\n",
    "        \n",
    "        return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this before generation to validate:\n",
    "def validate_tokens(tokenizer, prompt):\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(prompt)\n",
    "    # Print each token for inspection\n",
    "    for token in tokens:\n",
    "        print(f\"Token: {token}\")\n",
    "    # Check special tokens\n",
    "    special_tokens = [\"<|begin_of_text|>\", \"<|start_header_id|>\", \n",
    "                     \"user\", \"<|end_header_id|>\", \"<|eot_id|>\"]\n",
    "    for st in special_tokens:\n",
    "        if st not in tokenizer.get_vocab():\n",
    "            print(f\"Warning: {st} not in vocabulary!\")\n",
    "            \n",
    "def generate_response(input_text, model, tokenizer):\n",
    "    # Prepare the user message\n",
    "    user_message = f\"{input_text}\"\n",
    "    # Fix the template rendering\n",
    "    from jinja2 import Template\n",
    "    \n",
    "    # Use the exact same template as training\n",
    "    template = Template(\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ user_message }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\")  # Note: No assistant_response or final <|eot_id|> as we want the model to generate these\n",
    "    \n",
    "    prompt = template.render(user_message=user_message)\n",
    "    \n",
    "    validate_tokens(tokenizer, prompt)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Initialize text streamer\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    # Add stop sequences\n",
    "    stop_sequences = [\n",
    "        \"<|begin_of_text|>\",  # Stop if it tries to start a new conversation\n",
    "        \"<|end_of_text|>\"     # Stop at proper end\n",
    "    ]\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    # Decode and clean up the response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Clean up multiple special tokens if they occur\n",
    "    generated_text = generated_text.replace(\"<|begin_of_text|><|begin_of_text|>\", \"<|begin_of_text|>\")\n",
    "    \n",
    "    # Split at any new prompt and take only the first response\n",
    "    if \"<|begin_of_text|>\" in generated_text[len(prompt):]:\n",
    "        generated_text = generated_text[:generated_text[len(prompt):].find(\"<|begin_of_text|>\") + len(prompt)]\n",
    "    \n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1,tokenizer1 = load_model_for_inference(model_name = \"lora_model_pum\")\n",
    "model2,tokenizer2 = load_model_for_inference(model_name = \"lora_model_pum_instruct\")\n",
    "# model3,tokenizer3 = load_model_for_inference(model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\")\n",
    "# model2,tokenizer4 = load_model_for_inference(model_name = \"unsloth/Llama-3.2-1B-bnb-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response for model1:\n",
      "Token: <|begin_of_text|>\n",
      "Token: <|start_header_id|>\n",
      "Token: user\n",
      "Token: <|end_header_id|>\n",
      "Token: ĊĊ\n",
      "Token: What\n",
      "Token: Ġis\n",
      "Token: Ġsem\n",
      "Token: iotics\n",
      "Token: <|eot_id|>\n",
      "Token: <|start_header_id|>\n",
      "Token: assistant\n",
      "Token: <|end_header_id|>\n",
      "Token: Ċ\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is semiotics<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "A vast and complex topic! Semiotics is the study of signs and symbols, their meanings, and how they are used to communicate. It's a branch of linguistics that explores the relationship between signifier (the word or symbol) and signified (the meaning) in order to understand how language functions as a system for communication. Here's a brief introduction:\n",
      "\n",
      "**Key concepts:**\n",
      "\n",
      "1. **Sign**: A thing or concept understood as having a name, function, or identity; it has an inherent meaning.\n",
      "2. **Signifier**: The physical object or sound that carries the sign; the material substance of the sign.\n",
      "3. **Signified**: The idea or concept represented by the sign; its intended meaning.\n",
      "4. **Symbolic representation**: An interpretation of a sign which makes it intelligible, and allows us to refer back to the original signifier.\n",
      "5. **Signification**: The process of making something appear to have a meaning through a sign.\n",
      "\n",
      "**Theories of Semantics:**\n",
      "\n",
      "There are several theories of semantics, but some key ones include:\n",
      "\n",
      "1. **Freudian theory**: This was developed by Sigmund Freud, who believed that words were derived from unconscious wish-fulfilments, and thus had no real meaning. He saw the word as a symbol for the individual unconscious wish, and that our perception of reality was distorted by this wish.\n",
      "2. **Structuralism**: Developed by Roland Barthes, this theory sees meaning as a system created through relations between signs, where each element contributes to a larger whole.\n",
      "3. **Post-structuralism**: This theory challenges traditional notions of meaning by questioning the idea of fixed meanings. It argues that there is no objective truth, and that all meaning is relative and context-dependent.\n",
      "\n",
      "**Applications of Semantics:**\n",
      "\n",
      "Semantics has been applied in various fields, including:\n",
      "\n",
      "1. **Linguistics**: Analyzing language structure and use.\n",
      "2. **Philosophy**: Examining the nature of reference and meaning.\n",
      "3. **Art criticism**: Understanding the creative decisions behind art works.\n",
      "4. **Anthropology**: Studying how culture shapes perceptions and meanings.\n",
      "5. **Psychology**: Exploring the development and functioning of thought.\n",
      "6. **Social sciences**: Researching human behavior, attitudes, and social interactions.\n",
      "7. **Communication studies**: Understanding how messages are conveyed and interpreted.\n",
      "\n",
      "In conclusion, semiotics is a rich and diverse field that seeks to uncover the underlying mechanisms of meaning-making in all aspects of human experience. Its insights can\n",
      "\n",
      "Generated Response for model2:\n",
      "Token: <|begin_of_text|>\n",
      "Token: <|start_header_id|>\n",
      "Token: user\n",
      "Token: <|end_header_id|>\n",
      "Token: ĊĊ\n",
      "Token: What\n",
      "Token: Ġis\n",
      "Token: Ġsem\n",
      "Token: iotics\n",
      "Token: <|eot_id|>\n",
      "Token: <|start_header_id|>\n",
      "Token: assistant\n",
      "Token: <|end_header_id|>\n",
      "Token: Ċ\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is semiotics<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "Semiotics is the study of signs, symbols, and meanings within cultural contexts. It is an interdisciplinary field that draws on linguistics, anthropology, sociology, philosophy, and other disciplines to understand how people create and negotiate meaning through language, images, objects, and behaviors. Semiotics examines how signs and symbols convey information, construct identity, and shape social relationships. It involves analyzing the ways in which meaning is created and interpreted within specific cultural contexts, including language, visual arts, fashion, music, and architecture. There are several types of semiotics, including: (1) Structural semantics, which focuses on the underlying structures and rules that govern signification; (2) Symbolic semantics, which examines the role of symbols and icons in communication; and (3) Cultural semantics, which explores the relationship between culture and meaning. Within these types, there are subtypes such as: (a) Lexical semantics, which studies the meaning of words and language; (b) Phonetics and phonology, which examines the sounds and sound systems of languages; (c) Anthropometric semantics, which analyzes the physical characteristics of objects and bodies; and (d) Visual semantics, which studies the meaning of images and visual representations.\n",
      "\n",
      "Example:\n",
      "A marketing team uses semiotics to develop a new advertising campaign for a luxury car brand. They analyze the symbolism behind the brand's logo, color palette, and advertising imagery to create a message that resonates with its target audience. By examining the cultural associations and historical context of the brand, they identify opportunities to tap into the emotional resonance of the target market, ultimately increasing brand recognition and sales. For instance, the company may use a combination of traditional and modern design elements, such as sleek lines, bold typography, and high-end visuals, to evoke feelings of power, sophistication, and exclusivity, while also highlighting the car's advanced technology and performance capabilities. This approach allows the brand to create a unique identity that speaks to both its target audience's aspirational values and its own premium positioning. By using semiotics to develop a cohesive brand image, the company can differentiate itself from competitors and build a loyal customer base.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"What is semiotics\"\n",
    "\n",
    "print(\"\\nGenerated Response for model1:\")\n",
    "print(generate_response(text, model1, tokenizer1))\n",
    "\n",
    "print(\"\\nGenerated Response for model2:\")\n",
    "print(generate_response(text, model2, tokenizer2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response for model1:\n",
      "Token: <|begin_of_text|>\n",
      "Token: <|start_header_id|>\n",
      "Token: user\n",
      "Token: <|end_header_id|>\n",
      "Token: ĊĊ\n",
      "Token: What\n",
      "Token: Ġis\n",
      "Token: Ġblue\n",
      "Token: Ġocean\n",
      "Token: Ġstrategy\n",
      "Token: <|eot_id|>\n",
      "Token: <|start_header_id|>\n",
      "Token: assistant\n",
      "Token: <|end_header_id|>\n",
      "Token: Ċ\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is blue ocean strategy<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\"Blue Ocean Strategy\" (BOS) is a business strategy model developed by Roger Martin, a management consultant and professor at the University of Queensland. The concept was popularized in 2008 through his book \"Blue Ocean Strategy: How to Create Uncompeting and Profitable Markets.” It offers an alternative approach to traditional blue ocean thinking which emphasizes the importance of understanding the customer’s perception of your product or service as much as that of the company itself.\n",
      "\n",
      "The basic idea behind BOS is to analyze how the perceived value proposition of the product or service created for the target market by the company has changed over time, and to then find opportunities to create new products or services that offer even more valuable perceptions to the same customers. This could be done with lower prices, different packaging, better marketing, etc. Here are some key aspects of Blue Ocean Strategy:\n",
      "\n",
      "1. **Identifying New Perceived Value**: A significant component of Blue Ocean Strategy involves identifying new ways to make a current product or service more appealing to the target market. For example, perhaps there is a product that can satisfy both the existing demand and offer something new.\n",
      "2. **New Products and Services**: The objective here would be to create something new that meets the changing needs of the market. These can include various versions of a similar product, special editions, new technologies, etc.\n",
      "3. **Changing the Perception of the Company**: Another goal is to change the perception of the company itself. This might involve making significant changes in the brand image, rebranding, or creating entirely new companies from scratch. However, it should not be too drastic, as this can have negative effects on the financial situation of the company.\n",
      "4. **Creating Competitive Advantage**: The last but not least, the aim is always to gain a competitive advantage over your competitors. By finding a way to meet the evolving needs and desires of your customers you can acquire a position where you no longer have to compete with them.\n",
      "\n",
      "One of the most influential concepts underlying Blue Ocean Strategy is called “the unobtrusive competitor” or the “invisible competitor,” who does not directly compete with us but creates a perception of superiority among our customers, thus indirectly creating a gap between the two. This invisible competitor is the one we do not see, nor understand, nor know about, nor think of; it is the desired perception of the customer. Therefore, the success of the strategy relies heavily on the creation of such an impression.<|eot_id|>\n",
      "\n",
      "Generated Response for model2:\n",
      "Token: <|begin_of_text|>\n",
      "Token: <|start_header_id|>\n",
      "Token: user\n",
      "Token: <|end_header_id|>\n",
      "Token: ĊĊ\n",
      "Token: What\n",
      "Token: Ġis\n",
      "Token: Ġblue\n",
      "Token: Ġocean\n",
      "Token: Ġstrategy\n",
      "Token: <|eot_id|>\n",
      "Token: <|start_header_id|>\n",
      "Token: assistant\n",
      "Token: <|end_header_id|>\n",
      "Token: Ċ\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is blue ocean strategy<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "Blue Ocean Strategy, also known as Blue Ocean Approach or Blue Ocean Concept, is a strategic management methodology developed by Peter Drucker and Ken Blanchard in the 1990s. It is an alternative to traditional Green Belt and White Belt roles within quality management, focusing on creating new business models and strategies that go beyond traditional product-focused approaches. The concept emphasizes collaboration between cross-functional teams, including sales, marketing, and operations, to develop innovative solutions that appeal to customers' needs and desires. Blue Ocean Strategy involves a systematic approach to identify, analyze, and capitalize on new opportunities, rather than just focusing on incremental improvements to existing products or services. It encourages companies to think outside the box, take risks, and challenge conventional wisdom, often through user experience (UX) research, customer journey mapping, and other qualitative methods. Blue Ocean Strategy has been applied in various industries, including retail, finance, and healthcare, and has been successful in generating new business models, such as subscription-based services, freemium models, and experiential retail. However, it has also been criticized for its lack of standardization and lack of clear guidelines, which can make it challenging to implement effectively. Nevertheless, Blue Ocean Strategy remains an important concept in qualitative market research, ethnography, and innovation methodologies, as it encourages companies to think creatively and experiment with new approaches to stay competitive in a rapidly changing market landscape.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"What is blue ocean strategy\"\n",
    "\n",
    "print(\"\\nGenerated Response for model1:\")\n",
    "print(generate_response(text, model1, tokenizer1))\n",
    "\n",
    "print(\"\\nGenerated Response for model2:\")\n",
    "print(generate_response(text, model2, tokenizer2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
