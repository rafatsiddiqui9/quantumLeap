{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Git clone qLeap-fft repo inside `/root/` directory\n","## Ensure to have the latest branch\n","## Switch to quantumLeap directory"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Current directory: /home/ubuntu/quantumLeap\n","Already in correct directory\n","Working directory: /home/ubuntu/quantumLeap\n"]}],"source":["import os\n","\n","# Set these environment variables before importing torch-related modules\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","\n","from pathlib import Path\n","\n","def ensure_working_directory():\n","    \"\"\"\n","    Check if we're in the correct working directory, if not switch to it.\n","    Creates the directory if it doesn't exist.\n","    \"\"\"\n","    target_dir = '/home/ubuntu/quantumLeap'\n","    current_dir = os.getcwd()\n","    \n","    # Print current directory\n","    print(f\"Current directory: {current_dir}\")\n","    \n","    # Check if we need to switch directories\n","    if current_dir != target_dir:\n","        # Create directory if it doesn't exist\n","        Path(target_dir).mkdir(parents=True, exist_ok=True)\n","        \n","        try:\n","            # Change to target directory\n","            os.chdir(target_dir)\n","            print(f\"Successfully switched to: {target_dir}\")\n","        except Exception as e:\n","            print(f\"Error switching to directory: {str(e)}\")\n","            raise\n","    else:\n","        print(\"Already in correct directory\")\n","    \n","    # Verify current directory\n","    print(f\"Working directory: {os.getcwd()}\")\n","\n","# Call the function before your main code\n","ensure_working_directory()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ----------------------------- #\n","# Part 1.1: Install and Setup Libraries - for Ola Krutrim Cloud Instance\n","# ----------------------------- #\n","\n","# # if executing below in terminal with virtual env, do not need to add --system tag\n","# pip install uv #install this in the virtual environment where you want to execute the notebook.\n","# pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121 # as on 07Nov2024, xformers is compatible with torch=2.4.0 only; uv doesnt work for installing torch\n","# uv pip install packaging ninja\n","# uv pip install flash-attn --no-build-isolation\n","# uv pip install unsloth\n","# python -m xformers.info\n","# uv pip install wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets tqdm Iprogress ipywidgets python-dotenv tensorboard -q\n","\n","# # restart once you have installed all of the above\n","# !nvidia-smi\n","# !nvcc --version\n","# import torch\n","# print(torch.__version__)          # Should reflect 2.5.0+cu124\n","# print(torch.version.cuda)         # Should output 12.4\n","# print(torch.cuda.is_available())  # Should return True"]},{"cell_type":"markdown","metadata":{},"source":["# Restart again so that all the libraries are properly initialized"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:33.365845Z","iopub.status.busy":"2024-11-04T08:49:33.365559Z","iopub.status.idle":"2024-11-04T08:49:47.786343Z","shell.execute_reply":"2024-11-04T08:49:47.785092Z","shell.execute_reply.started":"2024-11-04T08:49:33.365811Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","punkt was already available.\n","en_core_web_sm was already available.\n"]}],"source":["# ----------------------------- #\n","# Part 1.2: Import Necessary Libraries\n","# ----------------------------- #\n","\n","# General Libraries\n","import os\n","import json\n","import sys\n","import subprocess\n","import argparse\n","import logging\n","import math\n","import random\n","from datetime import datetime\n","import re\n","import gc\n","import weakref\n","import multiprocessing\n","\n","# Torch related\n","import torch\n","from torch import nn\n","import torch.distributed as dist\n","\n","# Transformers related\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForLanguageModeling,\n","    Adafactor\n",")\n","\n","# Huggingface TRL for full finetune\n","from trl import SFTTrainer, SFTConfig\n","\n","# General huggingface libraries\n","import huggingface_hub\n","from datasets import load_dataset, Dataset\n","from accelerate import Accelerator\n","\n","\n","# Unsloth specificic libraries\n","import unsloth\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n","\n","# Other Libraries\n","from peft import LoraConfig\n","import wandb\n","import nltk\n","import spacy\n","# from galore_torch import GaLoreAdamW, GaLoreAdafactor, GaLoreAdamW8bit\n","\n","# Check and import NLTK and spacy modules\n","# Ensure NLTK's punkt tokenizer is available\n","import nltk\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","    print('punkt was already available.')\n","except LookupError:\n","    nltk.download('punkt')\n","    print('punkt was not available. It has been downloaded')\n","\n","# Initialize spaCy English model\n","try:\n","    nlp = spacy.load('en_core_web_sm')\n","    print('en_core_web_sm was already available.')\n","except OSError:\n","    print(\"SpaCy English model not found. Downloading...\")\n","    os.system('python -m spacy download en_core_web_sm')\n","    nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.535557Z","iopub.status.busy":"2024-11-04T08:49:54.535142Z","iopub.status.idle":"2024-11-04T08:49:54.549789Z","shell.execute_reply":"2024-11-04T08:49:54.549048Z","shell.execute_reply.started":"2024-11-04T08:49:54.535509Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 2: Load and Clean the Text Data\n","# ----------------------------- #\n","\n","def load_and_clean_text(file_path):\n","    \"\"\"\n","    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n","    \"\"\"\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    # # Remove Project Gutenberg's license text and headers/footers\n","    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","\n","    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n","    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n","    return text.strip()\n","\n","# Replace 'psychology_of_unconscious.txt' with your actual file path\n","file_path = '/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'\n","clean_text = load_and_clean_text(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install t"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting text processing\n","Created 71 initial chunks\n","Processing chunk 1/71\n","Error in LLM request: Connection error.\n","Processing chunk 2/71\n","Error in LLM request: Connection error.\n","Processing chunk 3/71\n","Error in LLM request: Connection error.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 210\u001b[0m\n\u001b[1;32m    207\u001b[0m     print_chunk_summary(chunker\u001b[38;5;241m.\u001b[39mlog_dir)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[1], line 200\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Process text\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m sections \u001b[38;5;241m=\u001b[39m \u001b[43mchunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    203\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(chunker\u001b[38;5;241m.\u001b[39mlog_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic_sections.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[1], line 164\u001b[0m, in \u001b[0;36mSemanticChunker.process_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_chunk_log(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, chunk, raw_response, sections)\n\u001b[1;32m    163\u001b[0m     all_sections\u001b[38;5;241m.\u001b[39mextend(sections)\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Rate limiting\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete. Total sections created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_sections\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import tiktoken\n","from openai import OpenAI\n","import json\n","import time\n","from typing import List, Dict\n","import numpy as np\n","import os\n","from datetime import datetime\n","\n","class SemanticChunker:\n","    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n","        \"\"\"Initialize the semantic chunker with model configuration\"\"\"\n","        self.client = OpenAI(\n","            base_url=\"http://localhost:8000/v1\",\n","            api_key=\"dummy\"\n","        )\n","        self.model_name = model_name\n","        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n","        self.max_tokens = 3000\n","        \n","        # Set up logging directory with timestamp\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        self.log_dir = f\"/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/Psychology_Of_Unconscious_Mind/chunks_{timestamp}\"\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        \n","        # Set up logging file for processing summary\n","        self.log_file = os.path.join(self.log_dir, \"processing_log.txt\")\n","        \n","    def log_message(self, message: str):\n","        \"\"\"Write log message with timestamp\"\"\"\n","        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        with open(self.log_file, 'a', encoding='utf-8') as f:\n","            f.write(f\"[{timestamp}] {message}\\n\")\n","        print(message)\n","        \n","    def count_tokens(self, text: str) -> int:\n","        \"\"\"Count tokens in text using tiktoken\"\"\"\n","        return len(self.encoding.encode(text))\n","    \n","    def create_initial_chunks(self, text: str) -> List[str]:\n","        \"\"\"Create initial chunks of approximately max_tokens size\"\"\"\n","        paragraphs = text.split('\\n\\n')\n","        chunks = []\n","        current_chunk = []\n","        current_tokens = 0\n","        \n","        for paragraph in paragraphs:\n","            para_tokens = self.count_tokens(paragraph)\n","            \n","            if current_tokens + para_tokens > self.max_tokens:\n","                # Join current chunk and add to chunks\n","                chunks.append('\\n\\n'.join(current_chunk))\n","                current_chunk = [paragraph]\n","                current_tokens = para_tokens\n","            else:\n","                current_chunk.append(paragraph)\n","                current_tokens += para_tokens\n","        \n","        # Add the last chunk if it exists\n","        if current_chunk:\n","            chunks.append('\\n\\n'.join(current_chunk))\n","            \n","        self.log_message(f\"Created {len(chunks)} initial chunks\")\n","        return chunks\n","    \n","    def save_chunk_log(self, chunk_num: int, original_chunk: str, llm_response: str, parsed_sections: List[Dict]):\n","        \"\"\"Save intermediate chunks and responses\"\"\"\n","        log_file = os.path.join(self.log_dir, f\"chunk_{chunk_num:04d}.json\")\n","        log_data = {\n","            \"chunk_number\": chunk_num,\n","            \"original_text\": original_chunk,\n","            \"llm_raw_response\": llm_response,\n","            \"parsed_sections\": parsed_sections,\n","            \"token_count\": self.count_tokens(original_chunk)\n","        }\n","        with open(log_file, 'w', encoding='utf-8') as f:\n","            json.dump(log_data, f, indent=2, ensure_ascii=False)\n","    \n","    def get_semantic_sections(self, chunk: str) -> tuple[List[Dict], str]:\n","        \"\"\"Send chunk to LLM for semantic sectioning with structured JSON output\"\"\"\n","\n","        # Define the JSON schema\n","        schema = {\n","            \"type\": \"object\",\n","            \"properties\": {\n","                \"sections\": {\n","                    \"type\": \"array\",\n","                    \"items\": {\n","                        \"type\": \"object\",\n","                        \"properties\": {\n","                            \"topic\": {\"type\": \"string\"},\n","                            \"content\": {\"type\": \"string\"},\n","                            \"key_concepts\": {\n","                                \"type\": \"array\",\n","                                \"items\": {\"type\": \"string\"}\n","                            }\n","                        },\n","                        \"required\": [\"topic\", \"content\", \"key_concepts\"],\n","                        \"additionalProperties\": False\n","                    }\n","                }\n","            },\n","            \"required\": [\"sections\"],\n","            \"additionalProperties\": False\n","        }\n","\n","        try:\n","            response = self.client.chat.completions.create(\n","                model=self.model_name,\n","                messages=[\n","                    {\n","                        \"role\": \"system\",\n","                        \"content\": \"You are a text analysis expert. Break the given text into coherent sections by topic.\"\n","                    },\n","                    {\n","                        \"role\": \"user\",\n","                        \"content\": f\"Analyze this text and break it into coherent sections:\\n\\n{chunk}\"\n","                    }\n","                ],\n","                max_tokens=self.max_tokens,\n","                temperature=0.2,\n","                response_format={\n","                    \"type\": \"json_schema\",\n","                    \"json_schema\": {\n","                        \"name\": \"text_sections\",\n","                        \"schema\": schema,\n","                        \"strict\": True\n","                    }\n","                }\n","            )\n","            \n","            result = response.choices[0].message.content\n","            \n","            try:\n","                parsed = json.loads(result)\n","                self.log_message(f\"Successfully parsed JSON with {len(parsed['sections'])} sections\")\n","                return parsed['sections'], result\n","            except json.JSONDecodeError as e:\n","                self.log_message(f\"JSON parsing error: {str(e)}\")\n","                self.log_message(f\"Raw response: {result}\")\n","                return [], result\n","                \n","        except Exception as e:\n","            self.log_message(f\"Error in LLM request: {str(e)}\")\n","            return [], str(e)\n","    \n","    def process_text(self, text: str) -> List[Dict]:\n","        \"\"\"Process entire text into semantic sections\"\"\"\n","        self.log_message(\"Starting text processing\")\n","        \n","        # Create initial chunks\n","        initial_chunks = self.create_initial_chunks(text)\n","        \n","        # Process each chunk\n","        all_sections = []\n","        for i, chunk in enumerate(initial_chunks):\n","            self.log_message(f\"Processing chunk {i+1}/{len(initial_chunks)}\")\n","            sections, raw_response = self.get_semantic_sections(chunk)\n","            \n","            # Save intermediate results\n","            self.save_chunk_log(i+1, chunk, raw_response, sections)\n","            \n","            all_sections.extend(sections)\n","            time.sleep(1)  # Rate limiting\n","            \n","        self.log_message(f\"Processing complete. Total sections created: {len(all_sections)}\")\n","        return all_sections\n","\n","    def save_sections(self, sections: List[Dict], output_file: str):\n","        \"\"\"Save processed sections to JSON file\"\"\"\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","            json.dump({'sections': sections}, f, indent=2, ensure_ascii=False)\n","        self.log_message(f\"Saved sections to {output_file}\")\n","\n","def print_chunk_summary(log_dir: str):\n","    \"\"\"Print summary of processed chunks\"\"\"\n","    print(\"\\nChunk Processing Summary:\")\n","    print(\"-\" * 50)\n","    \n","    for file in sorted(os.listdir(log_dir)):\n","        if file.endswith(\".json\") and file != \"processing_log.txt\":\n","            with open(os.path.join(log_dir, file), 'r') as f:\n","                data = json.load(f)\n","                print(f\"\\nChunk {data['chunk_number']}:\")\n","                print(f\"Token count: {data['token_count']}\")\n","                print(f\"Sections created: {len(data['parsed_sections'])}\")\n","                for section in data['parsed_sections']:\n","                    print(f\"- {section['topic']}\")\n","\n","def main():\n","    # Initialize chunker\n","    chunker = SemanticChunker()\n","    \n","    # Read input file\n","    input_file = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    \n","    # Process text\n","    sections = chunker.process_text(text)\n","    \n","    # Save results\n","    output_file = os.path.join(chunker.log_dir, \"semantic_sections.json\")\n","    chunker.save_sections(sections, output_file)\n","    \n","    # Print chunk summary\n","    print_chunk_summary(chunker.log_dir)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[2024-11-10 01:49:52] Starting processing of /home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\n","[2024-11-10 01:49:52] Input text: 845623 chars, 199664 tokens\n","[2024-11-10 01:49:52] Starting text processing\n","[2024-11-10 01:49:52] \n","Processing chunk 1\n","[2024-11-10 01:49:52] Starting get_complete_paragraphs with 845623 chars of text\n","[2024-11-10 01:49:52] Found 2957 sections\n","[2024-11-10 01:49:52] Processing section 1: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 2: SectionType.HEADER, 20 tokens\n","[2024-11-10 01:49:52] Processing section 3: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 5: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 7: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 9: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 11: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 13: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 15: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 17: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 18: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 20: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 21: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 23: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 25: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 26: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 28: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 30: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 32: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 33: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 35: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 36: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 38: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 39: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 41: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 43: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 44: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 45: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 47: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 49: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 51: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 53: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 55: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 57: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 59: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 61: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 63: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 65: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 67: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 69: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 71: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 73: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 75: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 77: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 79: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 80: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 82: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 84: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 86: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 88: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 90: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 91: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 93: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 95: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 96: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 98: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 100: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 102: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 104: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 105: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 107: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 109: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 111: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 113: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 114: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 116: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 117: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 119: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 121: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 123: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 124: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 126: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 128: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 129: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 131: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 133: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 135: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 137: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 139: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 141: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 143: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 145: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 147: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 149: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 151: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 153: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 155: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 157: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 159: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 161: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 163: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 165: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 166: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 168: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 170: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 171: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 173: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 174: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 175: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 177: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 178: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 180: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 181: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 182: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 183: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:52] Processing section 185: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 186: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 188: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 190: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 192: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 193: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 195: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 197: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 199: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 201: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 203: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 205: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 207: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 208: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 209: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 210: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 212: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 213: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 215: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 217: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 219: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 221: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 223: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 224: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 226: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 228: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 230: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 232: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 234: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 236: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 238: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 240: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 242: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 244: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 246: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:52] Processing section 248: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 250: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 252: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 254: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 256: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 258: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 260: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:52] Processing section 262: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 264: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 266: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 268: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 270: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 272: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 273: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 275: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 277: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 279: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 281: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 283: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 285: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 287: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 289: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 291: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 293: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 295: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 297: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 299: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 301: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 303: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 305: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 307: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 309: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:52] Processing section 311: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 313: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 315: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 317: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 319: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 321: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 322: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 323: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 324: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 326: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 327: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 329: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 331: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 332: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 334: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 336: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 337: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 338: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 340: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 341: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 342: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 343: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 345: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 347: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 349: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 351: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 353: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 355: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 357: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 358: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 360: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 361: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 363: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 365: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 367: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 369: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:52] Processing section 371: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 373: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 375: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 377: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 379: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 381: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 383: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 385: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 386: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 387: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 389: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 390: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 392: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 393: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 395: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 397: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 399: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 401: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 403: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 405: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 407: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 409: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 410: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 412: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 414: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 416: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 417: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 418: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 420: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 422: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 424: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 426: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 428: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 430: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 432: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 433: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 435: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 437: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 439: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 441: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 443: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 445: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 446: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 447: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 449: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 451: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 452: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 454: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 455: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 457: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 459: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 461: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 462: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 464: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 465: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 467: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 469: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 471: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 473: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 474: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 476: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 478: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 479: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 481: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 483: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 485: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:52] Processing section 487: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 489: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 491: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 492: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 494: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 496: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 497: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 499: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 500: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 501: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 502: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 504: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 506: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 508: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 510: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 512: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 514: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 516: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 517: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 519: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 521: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 523: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 524: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 526: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 528: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 529: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 531: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 533: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 535: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:52] Processing section 537: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 538: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 539: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 541: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 543: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 545: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 547: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 549: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 551: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 553: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 554: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 555: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 557: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 559: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 561: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 563: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 565: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 567: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 569: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 570: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 572: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 574: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:52] Processing section 576: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:52] Processing section 577: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 579: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 581: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 583: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:52] Processing section 585: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:52] Processing section 587: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:52] Processing section 588: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:52] Processing section 590: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:52] Processing section 591: SectionType.HEADER, 2 tokens\n","[2024-11-10 01:49:52] Processing section 593: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 595: SectionType.HEADER, 18 tokens\n","[2024-11-10 01:49:52] Processing section 597: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 599: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 601: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:52] Processing section 603: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:52] Processing section 605: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 607: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:52] Processing section 608: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:52] Processing section 610: SectionType.HEADER, 19 tokens\n","[2024-11-10 01:49:52] Processing section 612: SectionType.HEADER, 19 tokens\n","[2024-11-10 01:49:52] Processing section 613: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:52] Processing section 615: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 617: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 619: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 621: SectionType.HEADER, 18 tokens\n","[2024-11-10 01:49:53] Processing section 623: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 625: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 627: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 629: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 631: SectionType.HEADER, 18 tokens\n","[2024-11-10 01:49:53] Processing section 633: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 635: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 637: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 639: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 641: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 644: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 646: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 648: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 649: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 651: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 653: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 655: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 657: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 658: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 660: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 662: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 664: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 666: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 668: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 670: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 672: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 674: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 676: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 678: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 680: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 682: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 683: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 685: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 687: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 688: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 690: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 692: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 694: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 695: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 697: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 699: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 701: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 703: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 705: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 707: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 709: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 711: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 713: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 715: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 716: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 718: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 720: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 722: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 724: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 726: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 728: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 730: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 732: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 734: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 736: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 738: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 740: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 742: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 744: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 746: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 748: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 750: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 752: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 754: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 756: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 758: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 759: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 760: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 762: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 764: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 766: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 768: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 770: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 772: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 774: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 776: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 778: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 780: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 782: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 784: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 786: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 788: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 790: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 792: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 794: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 796: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 798: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 800: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 802: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 803: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 805: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 807: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 809: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 811: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 813: SectionType.HEADER, 2 tokens\n","[2024-11-10 01:49:53] Processing section 815: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 817: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 819: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 821: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 823: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 825: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 827: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 829: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 831: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 833: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 835: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 837: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 838: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 840: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 842: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 844: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 845: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 847: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 849: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 851: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 853: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 855: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 857: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 859: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 861: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 862: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 864: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 865: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 867: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 869: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 871: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 873: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 875: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 877: SectionType.ATTRIBUTION, 117 tokens\n","[2024-11-10 01:49:53] After processing: tokens=632/3000, sections=877\n","[2024-11-10 01:49:53] Processing section 878: SectionType.ATTRIBUTION, 218 tokens\n","[2024-11-10 01:49:53] After processing: tokens=850/3000, sections=878\n","[2024-11-10 01:49:53] Processing section 879: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 881: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 883: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 885: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 886: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 887: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 889: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 891: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 893: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 895: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 897: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 898: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 900: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 902: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 903: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 905: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 907: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 909: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 911: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 913: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 914: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 916: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 918: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 920: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 922: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 924: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 926: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 928: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 930: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 932: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 934: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 936: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 938: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 940: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 942: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 944: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 945: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 947: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 949: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 951: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 953: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 955: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 957: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 959: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 961: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 963: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 965: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 967: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 969: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 971: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 973: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 975: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 977: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 979: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 981: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 983: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 985: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 987: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 988: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 990: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 992: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 994: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 996: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 998: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 1000: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1002: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1004: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1006: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1008: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1010: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1012: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1014: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1016: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1017: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1019: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1021: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1023: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1025: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1027: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1029: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1031: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1033: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1035: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1037: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1039: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1041: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1042: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1044: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1046: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1048: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1050: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1051: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1053: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1055: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1057: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1058: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1059: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1061: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1063: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1065: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1067: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1069: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1071: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1073: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1075: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1077: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1079: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1080: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1082: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1084: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1085: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1087: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1089: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1091: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1093: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1095: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1096: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1098: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1100: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1102: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1104: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1106: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1108: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1110: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1112: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1114: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1116: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 1117: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1119: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1121: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1123: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1125: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1127: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1129: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1131: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1133: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1135: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1137: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1139: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1141: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1143: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1145: SectionType.HEADER, 19 tokens\n","[2024-11-10 01:49:53] Processing section 1147: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1149: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1151: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1153: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1155: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1157: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1159: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1161: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1163: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1165: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1166: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1168: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1170: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1172: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1174: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1176: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1178: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1180: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1182: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1184: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1186: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1188: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1190: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1192: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1194: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1196: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1198: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1200: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1202: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1203: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1205: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1207: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1209: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1211: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1213: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1215: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1217: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1218: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1219: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1220: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1222: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1224: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1226: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1228: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1230: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1232: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1234: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1236: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1238: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1240: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1242: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1244: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1246: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1248: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1250: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1252: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1254: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1256: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1258: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1260: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1262: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1264: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1266: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1268: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1270: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1272: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 1274: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1276: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1278: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 1280: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 1282: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 1284: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1286: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1288: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 1290: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 1291: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 1293: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 1295: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1297: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1299: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1301: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 1302: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1304: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 1306: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 1308: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1310: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 1311: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 1312: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1314: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1316: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1318: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1320: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1322: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1324: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1326: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1328: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1330: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1332: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1334: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1336: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1338: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1340: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1341: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1342: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1344: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1346: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1348: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1350: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1352: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1354: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1356: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1357: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1358: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1360: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1361: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1363: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 1364: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1365: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1367: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1369: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1371: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1373: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1374: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1376: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1378: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1380: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1382: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1384: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1386: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1388: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1390: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1392: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1394: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1395: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1397: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1399: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1401: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1403: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1404: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1406: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1408: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1410: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1412: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1414: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1416: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1418: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1419: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1421: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1422: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1424: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1426: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1428: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1429: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1431: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1432: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1433: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1435: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1437: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1439: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1441: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1442: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1444: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1446: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1448: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1450: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1452: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1454: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1455: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1457: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1458: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1460: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1462: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1463: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1465: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1467: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1469: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1471: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1473: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1475: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1477: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1479: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1481: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1483: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1485: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1487: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1489: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1491: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1492: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1494: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1496: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1497: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1499: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1501: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1503: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1504: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1506: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1507: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1509: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1511: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1513: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1515: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1516: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1518: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1520: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1522: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1524: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1526: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1528: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1530: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1532: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1534: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1536: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 1538: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 1539: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1541: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1543: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1545: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1547: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1549: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1551: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1553: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1554: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1556: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1558: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1560: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1562: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1564: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1566: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1568: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1570: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1572: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1574: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1576: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1578: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1580: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1581: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1583: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1585: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1587: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1589: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1591: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1593: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1595: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1597: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1599: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1601: SectionType.HEADER, 19 tokens\n","[2024-11-10 01:49:53] Processing section 1603: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1605: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1607: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1609: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1611: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1613: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1615: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1617: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1618: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1620: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1622: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1623: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1625: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1627: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1629: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1631: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1633: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1635: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1637: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1639: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1641: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1643: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1645: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1647: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1648: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1650: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1652: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1653: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1655: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1657: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1659: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1661: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1663: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1665: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1667: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1669: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1670: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1672: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 1674: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1676: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1678: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1680: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1682: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1684: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1686: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1688: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1690: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1692: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1694: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1696: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1698: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1700: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1702: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1704: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1706: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1708: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1710: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1712: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 1713: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1715: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1717: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1719: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1720: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1722: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1723: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1724: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1726: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1728: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1730: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1732: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1734: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1736: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1738: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1740: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1742: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1744: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1746: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1748: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1750: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1752: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1754: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1756: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1758: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1760: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1762: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1764: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1765: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1767: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1769: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1771: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1773: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1775: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 1777: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1778: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1780: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1782: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1784: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1786: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1788: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1790: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1792: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1794: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1796: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1798: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1800: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1802: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1804: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1806: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1808: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1810: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1812: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1814: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1816: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1818: SectionType.QUOTE, 282 tokens\n","[2024-11-10 01:49:53] After processing: tokens=353/3000, sections=1818\n","[2024-11-10 01:49:53] Processing section 1819: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1821: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1822: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1824: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1826: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1828: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1830: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1832: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1834: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1836: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1838: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1840: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1842: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1844: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1846: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1847: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1849: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1851: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1853: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1855: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1857: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1859: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1861: SectionType.HEADER, 18 tokens\n","[2024-11-10 01:49:53] Processing section 1863: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1865: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1867: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1869: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1871: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1873: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1875: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1877: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1879: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1881: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1883: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1885: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1887: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1889: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1891: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 1892: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1893: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1895: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1897: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1899: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1901: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1903: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1904: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1906: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1908: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1910: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1912: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1914: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1916: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1918: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1920: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1922: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1924: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 1926: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1928: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1930: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1932: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1933: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1935: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1937: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1939: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1941: SectionType.HEADER, 18 tokens\n","[2024-11-10 01:49:53] Processing section 1943: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1945: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1947: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1949: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 1951: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1953: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1955: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1957: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1959: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1961: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1963: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 1965: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1967: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 1969: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1971: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1973: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 1975: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1977: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1979: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 1981: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1983: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1985: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1987: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1989: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 1991: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1993: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 1995: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 1996: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 1998: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2000: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2002: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2004: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2006: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2008: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2010: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2012: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2014: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2016: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2018: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2020: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2022: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2024: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2026: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2028: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2030: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2031: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2033: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2035: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2037: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2039: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2041: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2043: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2045: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2047: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2049: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2051: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2053: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2055: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2057: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2058: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2060: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2061: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2063: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2066: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2068: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2070: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2072: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2074: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2076: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2078: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2080: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2082: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2084: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2086: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2088: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2090: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2092: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2094: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2096: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2098: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2100: SectionType.HEADER, 18 tokens\n","[2024-11-10 01:49:53] Processing section 2102: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2104: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2106: SectionType.ATTRIBUTION, 11 tokens\n","[2024-11-10 01:49:53] After processing: tokens=43/3000, sections=2106\n","[2024-11-10 01:49:53] Processing section 2107: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2109: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2111: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2113: SectionType.QUOTE, 755 tokens\n","[2024-11-10 01:49:53] After processing: tokens=914/3000, sections=2113\n","[2024-11-10 01:49:53] Processing section 2114: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2116: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2118: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2120: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2122: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2124: SectionType.QUOTE, 181 tokens\n","[2024-11-10 01:49:53] After processing: tokens=438/3000, sections=2124\n","[2024-11-10 01:49:53] Processing section 2125: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2127: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2129: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2131: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2133: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2135: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2137: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2139: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2141: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2142: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2144: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2146: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2148: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2150: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2152: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2154: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2156: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2158: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2160: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2162: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2164: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2166: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2168: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2170: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2172: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2174: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2176: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2178: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2180: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2181: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2183: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2185: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2187: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2189: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2191: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2193: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2195: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2197: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2199: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2201: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2203: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2205: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2206: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2208: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 2209: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2211: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2213: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2215: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2217: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2219: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2221: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2223: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2225: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2227: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2229: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2231: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2233: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2235: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2237: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2239: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2241: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2242: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2244: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2246: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2248: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2250: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2252: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2254: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2256: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2258: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2260: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2262: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2264: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2266: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2268: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2270: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2272: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2274: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2276: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2278: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2280: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2282: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2284: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 2286: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2288: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2290: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2292: SectionType.HEADER, 2 tokens\n","[2024-11-10 01:49:53] Processing section 2293: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2294: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2296: SectionType.HEADER, 2 tokens\n","[2024-11-10 01:49:53] Processing section 2297: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 2298: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2299: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2301: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2302: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 2303: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2304: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2306: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2308: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2310: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2312: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2314: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2316: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2318: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2320: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2322: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2324: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2326: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2328: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2330: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2332: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2334: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2336: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2338: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 2339: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2341: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2343: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2345: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2348: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2350: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2351: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2353: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2355: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2357: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 2358: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2360: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2362: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2364: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2366: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2368: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2369: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2371: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2373: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2375: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2377: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2379: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2381: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2383: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2385: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2387: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2389: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2391: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2393: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2395: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2397: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2399: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2401: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2402: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2404: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2406: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2408: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2410: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2412: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2414: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2416: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2418: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2420: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2422: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2424: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2426: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2428: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2430: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2432: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2434: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2436: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2438: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2440: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2442: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2444: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2446: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2448: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2450: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2452: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2454: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2456: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2457: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2459: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2461: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2463: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2465: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2467: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2468: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2470: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2472: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2474: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2476: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2478: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2480: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2482: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2484: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2486: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2488: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2490: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2492: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2494: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2496: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2498: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2500: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2502: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2504: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2506: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2508: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2510: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2512: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2514: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2516: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2518: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2520: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2522: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2524: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2526: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2527: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2529: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2531: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2533: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2535: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2537: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2539: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2541: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2543: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2545: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 2547: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2549: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2551: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 2553: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2555: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2557: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2559: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2561: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2563: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2565: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2567: SectionType.HEADER, 18 tokens\n","[2024-11-10 01:49:53] Processing section 2569: SectionType.HEADER, 21 tokens\n","[2024-11-10 01:49:53] Processing section 2571: SectionType.HEADER, 18 tokens\n","[2024-11-10 01:49:53] Processing section 2573: SectionType.HEADER, 20 tokens\n","[2024-11-10 01:49:53] Processing section 2575: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2577: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2579: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2581: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2583: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2585: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2587: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2589: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2591: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2593: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2594: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2596: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2598: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2600: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2602: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2604: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2605: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2607: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2609: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 2611: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2613: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2615: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2617: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2619: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2621: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2623: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2625: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2627: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2629: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2631: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2633: SectionType.ATTRIBUTION, 54 tokens\n","[2024-11-10 01:49:53] After processing: tokens=372/3000, sections=2633\n","[2024-11-10 01:49:53] Processing section 2634: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2635: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2637: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2639: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2641: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2643: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2645: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2647: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2649: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 2651: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2653: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2655: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2657: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2659: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2661: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2663: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2664: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2666: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2668: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2670: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2672: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2674: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2675: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 2677: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2679: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2681: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2683: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2685: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2687: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2689: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2690: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2692: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 2693: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2695: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2697: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2699: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2701: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2703: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2705: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2707: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2709: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2711: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2713: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2715: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2716: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2718: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2720: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2722: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2724: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2726: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2728: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2730: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2732: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2734: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2736: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2738: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2740: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2741: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2743: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2745: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2747: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2749: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2751: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2753: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2755: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2757: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2759: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2761: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2763: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2765: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2767: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2770: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2772: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2774: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2776: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2778: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2781: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2782: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2784: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2786: SectionType.HEADER, 5 tokens\n","[2024-11-10 01:49:53] Processing section 2788: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2790: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2792: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 2794: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 2796: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2798: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2800: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2802: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2804: SectionType.HEADER, 8 tokens\n","[2024-11-10 01:49:53] Processing section 2806: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2808: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2810: SectionType.HEADER, 4 tokens\n","[2024-11-10 01:49:53] Processing section 2811: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2812: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2814: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2816: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2818: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2820: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2822: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2824: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2826: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2828: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2830: SectionType.HEADER, 7 tokens\n","[2024-11-10 01:49:53] Processing section 2832: SectionType.HEADER, 3 tokens\n","[2024-11-10 01:49:53] Processing section 2834: SectionType.HEADER, 6 tokens\n","[2024-11-10 01:49:53] Processing section 2836: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processing section 2838: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2840: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2842: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2844: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2846: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2848: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2850: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2852: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2854: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2856: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2858: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2860: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2862: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2864: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2866: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2868: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2870: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2871: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2873: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2874: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2876: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2878: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2880: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2882: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2884: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2886: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2888: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2890: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2892: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2894: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2896: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2898: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2900: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2902: SectionType.HEADER, 17 tokens\n","[2024-11-10 01:49:53] Processing section 2904: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2906: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2908: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2910: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2912: SectionType.HEADER, 10 tokens\n","[2024-11-10 01:49:53] Processing section 2914: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2916: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2918: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2920: SectionType.HEADER, 11 tokens\n","[2024-11-10 01:49:53] Processing section 2922: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2924: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2926: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2928: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2930: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2932: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2934: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2936: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2938: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2940: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2942: SectionType.HEADER, 13 tokens\n","[2024-11-10 01:49:53] Processing section 2944: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2946: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2948: SectionType.HEADER, 12 tokens\n","[2024-11-10 01:49:53] Processing section 2949: SectionType.HEADER, 15 tokens\n","[2024-11-10 01:49:53] Processing section 2951: SectionType.HEADER, 16 tokens\n","[2024-11-10 01:49:53] Processing section 2953: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2954: SectionType.HEADER, 14 tokens\n","[2024-11-10 01:49:53] Processing section 2956: SectionType.HEADER, 9 tokens\n","[2024-11-10 01:49:53] Processed 2957 sections, 0 remaining\n","[2024-11-10 01:49:53] Created chunk 1 (199726 tokens)\n","[2024-11-10 01:49:53] Chunk preview:\n","Psychology of the Unconscious by Jung\n","\n","\n","\n","       AN INTRODUCTION TO PSYCHOANALYSIS AND ANALYTIC PSYCHOLOGY\n","\n","\n","\n","When Professor Freud of Vienna made his early discoveries in the realm\n","\n","\n","\n","of the neuroses, and announced that the basis and origin of the various\n","\n","\n","symptoms grouped under the terms hysteria and neuroses lay in\n","\n","\n","\n","unfulfilled desires and wishes, unexpressed and unknown to the patient\n","for the most part, and concerned chiefly with the sexual instinct, it\n","was not realized what far-reaching in...\n","[2024-11-10 01:49:53] Created 1 initial chunks\n","[2024-11-10 01:49:53] Processing limited to first 3 chunks\n","[2024-11-10 01:49:53] \n","Processing chunk 1/1\n","[2024-11-10 01:49:53] Chunk size: 852674 chars, 199726 tokens\n","\n","====================================================================================================\n","INPUT CHUNK\n","====================================================================================================\n","Chunk 1:\n","================================================================================\n","Psychology of the Unconscious by Jung\n","\n","\n","\n","       AN INTRODUCTION TO PSYCHOANALYSIS AND ANALYTIC PSYCHOLOGY\n","\n","\n","\n","When Professor Freud of Vienna made his early discoveries in the realm\n","\n","\n","\n","of the neuroses, and announced that the basis and origin of the various\n","\n","\n","symptoms grouped under the terms hysteria and neuroses lay in\n","\n","\n","\n","unfulfilled desires and wishes, unexpressed and unknown to the patient\n","for the most part, and concerned chiefly with the sexual instinct, it\n","was not realized what far-reaching influence this unpopular and bitterly\n","\n","\n","attacked theory would exert on the understanding of human life in\n","\n","\n","\n","general.\n","\n","\n","For this theory has so widened in its scope that its application has now\n","\n","\n","\n","extended beyond a particular group of pathologic states. It has in fact\n","led to a new evaluation of the whole conduct of human life; a new\n","\n","\n","comprehension has developed which explains those things which formerly\n","\n","\n","\n","were unexplained, and there is offered an understanding not only of the\n","\n","\n","symptoms of a neur...\n","================================================================================\n","[2024-11-10 01:49:53] Sending request to LLM (input tokens: 199726)\n","[2024-11-10 01:49:55] Error in LLM request: Connection error.\n","\n","====================================================================================================\n","SEMANTIC SECTIONS\n","====================================================================================================\n","\n","====================================================================================================\n","PROCESSING METRICS\n","====================================================================================================\n","{}\n","[2024-11-10 01:49:56] Processing complete. Created 0 semantic chunks\n","[2024-11-10 01:49:56] Saved 0 semantic chunks to /home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/Psychology_Of_Unconscious_Mind/chunks_20241110_014952/semantic_chunks\n","Error in main execution: division by zero\n"]},{"ename":"ZeroDivisionError","evalue":"division by zero","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 828\u001b[0m\n\u001b[1;32m    826\u001b[0m     test_structure_analysis()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 739\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# Generate processing summary\u001b[39;00m\n\u001b[1;32m    728\u001b[0m summary_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(chunker\u001b[38;5;241m.\u001b[39mlog_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing_summary.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    729\u001b[0m summary \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39misoformat(),\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_file,\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchars\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text),\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunker\u001b[38;5;241m.\u001b[39mcount_tokens(text)\n\u001b[1;32m    735\u001b[0m     },\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(semantic_chunks),\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28msum\u001b[39m(chunker\u001b[38;5;241m.\u001b[39mcount_tokens(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m semantic_chunks),\n\u001b[0;32m--> 739\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_chunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msemantic_chunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msemantic_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m     },\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_directory\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_dir\n\u001b[1;32m    742\u001b[0m }\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(summary_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    745\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(summary, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}],"source":["# Import statements and data structures\n","import tiktoken\n","from openai import OpenAI\n","import json\n","import time\n","from typing import List, Dict, Tuple, Optional\n","import numpy as np\n","import os\n","from datetime import datetime\n","from pprint import pprint\n","import re\n","from dataclasses import dataclass\n","from enum import Enum\n","\n","# Core data structures\n","class SectionType(Enum):\n","    HEADER = \"header\"\n","    CONTENT = \"content\"\n","    QUOTE = \"quote\"\n","    ATTRIBUTION = \"attribution\"\n","    LIST = \"list\"\n","    FRONT_MATTER = \"front_matter\"\n","    TABLE_OF_CONTENTS = \"table_of_contents\"\n","    \n","@dataclass\n","class Section:\n","    text: str\n","    type: SectionType\n","    level: int = 0\n","    metadata: Dict = None\n","\n","# Main class initialization\n","class SemanticChunker:\n","    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n","        \"\"\"Initialize the semantic chunker with model configuration\"\"\"\n","        self.client = OpenAI(\n","            base_url=\"http://localhost:8000/v1\",\n","            api_key=\"dummy\"\n","        )\n","        self.model_name = model_name\n","        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n","        self.max_tokens = 3000\n","        \n","        # Set up logging directory with timestamp\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        self.log_dir = f\"/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/Psychology_Of_Unconscious_Mind/chunks_{timestamp}\"\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        \n","        # Set up logging file for processing summary\n","        self.log_file = os.path.join(self.log_dir, \"processing_log.txt\")\n","        \n","        # Initialize state variables\n","        self.missed_text = \"\"  # Store text not included in LLM output\n","\n","    def log_message(self, message: str):\n","        \"\"\"Write log message with timestamp and print to console\"\"\"\n","        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        log_entry = f\"[{timestamp}] {message}\"\n","        with open(self.log_file, 'a', encoding='utf-8') as f:\n","            f.write(log_entry + \"\\n\")\n","        print(log_entry)\n","    \n","    def print_separator(self, message: str = \"\"):\n","        \"\"\"Print a separator line with optional message\"\"\"\n","        print(f\"\\n{'='*100}\")\n","        if message:\n","            print(f\"{message}\")\n","            print('='*100)\n","    \n","    def count_tokens(self, text: str) -> int:\n","        \"\"\"Count tokens in text using tiktoken\"\"\"\n","        return len(self.encoding.encode(text))\n","    \n","    # Continuing the SemanticChunker class...\n","\n","    def is_chapter_heading(self, text: str) -> Tuple[bool, int]:\n","        \"\"\"\n","        Enhanced chapter heading detection with level identification.\n","        Returns (is_heading, level).\n","        \"\"\"\n","        text = text.strip()\n","        if not text:\n","            return False, 0\n","            \n","        # Chapter patterns\n","        chapter_patterns = [\n","            (r'^CHAPTER\\s+[IVXL]+', 1),  # Main chapter headers\n","            (r'^[IVX]+\\.\\s*â€”\\s*', 2),    # Sub-chapter headers\n","            (r'^\\d+\\.\\s*â€”\\s*', 2),       # Numbered sections\n","            (r'^\\s*[A-Z][A-Z\\s]+$', 1),  # ALL CAPS lines\n","            (r'^\\s*[IVX]+\\.\\s+[A-Z]', 2) # Roman numeral sections\n","        ]\n","        \n","        for pattern, level in chapter_patterns:\n","            if re.match(pattern, text, re.I):\n","                return True, level\n","        \n","        # Check for centered text formatting\n","        line_length = len(text)\n","        leading_spaces = len(text) - len(text.lstrip())\n","        trailing_spaces = len(text) - len(text.rstrip())\n","        \n","        # Improved centered text detection\n","        is_centered = (abs(leading_spaces - trailing_spaces) <= 2 and \n","                      leading_spaces > 5 and\n","                      text.strip())  # Must have content\n","        is_caps = text.isupper()\n","        reasonable_length = 10 < len(text.strip()) < 100\n","        \n","        if is_centered:\n","            if is_caps and reasonable_length:\n","                return True, 1  # Main header\n","            elif reasonable_length:\n","                return True, 2  # Sub header\n","                \n","        return False, 0\n","    \n","    def analyze_text_structure(self, text: str) -> List[Section]:\n","        \"\"\"\n","        Enhanced text structure analysis with improved section detection.\n","        \"\"\"\n","        sections = []\n","        lines = text.split('\\n')\n","        current_section = []\n","        current_type = None\n","        current_level = 0\n","        \n","        def flush_section():\n","            nonlocal current_section, current_type, current_level\n","            if current_section:\n","                # Skip empty sections but preserve intentional spacing\n","                content = '\\n'.join(current_section).strip()\n","                if content or current_type in {SectionType.HEADER, SectionType.FRONT_MATTER}:\n","                    sections.append(Section(\n","                        text='\\n'.join(current_section),\n","                        type=current_type or SectionType.CONTENT,\n","                        level=current_level\n","                    ))\n","                current_section = []\n","                current_type = None\n","                current_level = 0\n","        \n","        i = 0\n","        while i < len(lines):\n","            line = lines[i]\n","            next_line = lines[i + 1] if i + 1 < len(lines) else \"\"\n","            \n","            # Detect centered headers\n","            if line.strip() and line.strip().isupper():\n","                leading_spaces = len(line) - len(line.lstrip())\n","                if leading_spaces > 10:  # Likely centered\n","                    flush_section()\n","                    current_type = SectionType.HEADER\n","                    current_level = 1\n","                    current_section = [line]\n","                    # Include following blank lines\n","                    while i + 1 < len(lines) and not lines[i + 1].strip():\n","                        current_section.append(lines[i + 1])\n","                        i += 1\n","                    flush_section()\n","                    i += 1\n","                    continue\n","            \n","            # Detect Table of Contents\n","            if re.match(r'^\\s*CONTENTS\\s*$', line, re.I):\n","                flush_section()\n","                current_type = SectionType.TABLE_OF_CONTENTS\n","                current_section = [line]\n","                # Include following blank lines\n","                while i + 1 < len(lines) and not lines[i + 1].strip():\n","                    current_section.append(lines[i + 1])\n","                    i += 1\n","                i += 1\n","                continue\n","            \n","            # Detect Front Matter\n","            if re.match(r'^\\s*(?:AUTHOR\\'S\\s+NOTE|PREFACE|INTRODUCTION)\\s*$', line, re.I):\n","                flush_section()\n","                current_type = SectionType.FRONT_MATTER\n","                current_section = [line]\n","                # Include following blank lines\n","                while i + 1 < len(lines) and not lines[i + 1].strip():\n","                    current_section.append(lines[i + 1])\n","                    i += 1\n","                i += 1\n","                continue\n","            \n","            # Detect chapter headings\n","            is_heading, level = self.is_chapter_heading(line)\n","            if is_heading:\n","                flush_section()\n","                current_type = SectionType.HEADER\n","                current_level = level\n","                current_section = [line]\n","                # Include following blank lines\n","                while i + 1 < len(lines) and not lines[i + 1].strip():\n","                    current_section.append(lines[i + 1])\n","                    i += 1\n","                flush_section()\n","                i += 1\n","                continue\n","            \n","            # Detect quotes\n","            if ((line.startswith('\"') and len(line) > 50) or \n","                (line.startswith('_') and line.endswith('_'))):\n","                if current_type != SectionType.QUOTE:\n","                    flush_section()\n","                    current_type = SectionType.QUOTE\n","                \n","            # Detect attributions\n","            if re.match(r'^\\s*(?:â€”|--)\\s*[A-Z]', line):\n","                flush_section()\n","                current_type = SectionType.ATTRIBUTION\n","                current_section = [line]\n","                i += 1\n","                continue\n","                \n","            # Detect lists\n","            if re.match(r'^\\s{4,}(?:[\\w\\-]+\\.|\\-|\\*)\\s', line):\n","                if current_type != SectionType.LIST:\n","                    flush_section()\n","                    current_type = SectionType.LIST\n","            \n","            current_section.append(line)\n","            i += 1\n","            \n","            # Handle section transitions\n","            if i < len(lines):\n","                next_line = lines[i]\n","                # Detect section breaks by multiple blank lines\n","                if (not line.strip() and not next_line.strip() and \n","                    current_type not in {SectionType.HEADER, SectionType.FRONT_MATTER}):\n","                    flush_section()\n","        \n","        flush_section()  # Flush any remaining content\n","        \n","        # Filter and clean sections\n","        filtered_sections = []\n","        for section in sections:\n","            if section.text.strip() or section.type in {SectionType.HEADER, SectionType.FRONT_MATTER}:\n","                filtered_sections.append(section)\n","        \n","        return filtered_sections\n","    \n","    # Continuing the SemanticChunker class...\n","\n","    def get_complete_paragraphs(self, text: str, max_tokens: int) -> Tuple[str, str]:\n","        \"\"\"\n","        Get complete paragraphs up to max_tokens with improved error handling.\n","        \"\"\"\n","        try:\n","            # First, analyze the structure\n","            sections = self.analyze_text_structure(text)\n","            if not sections:\n","                raise ValueError(\"No sections found in text\")\n","                \n","            self.log_message(f\"Found {len(sections)} sections\")\n","            \n","            current_sections = []\n","            current_tokens = 0\n","            section_index = 0\n","            \n","            while section_index < len(sections):\n","                section = sections[section_index]\n","                section_tokens = max(1, self.count_tokens(section.text))  # Prevent zero tokens\n","                \n","                # Handle oversized sections\n","                if section_tokens > max_tokens:\n","                    if not current_sections:\n","                        # Split large section\n","                        split_text = section.text[:max_tokens*4].strip()  # Approximate chars\n","                        if split_text:\n","                            current_sections.append(\n","                                Section(split_text, section.type, section.level)\n","                            )\n","                        remaining_text = section.text[max_tokens*4:]\n","                        if remaining_text:\n","                            self.missed_text = remaining_text\n","                        break\n","                    else:\n","                        break\n","                        \n","                # Check if adding section would exceed limit\n","                if current_tokens + section_tokens > max_tokens:\n","                    break\n","                    \n","                # Add section\n","                current_sections.append(section)\n","                current_tokens += section_tokens\n","                section_index += 1\n","                \n","            # Combine sections with proper spacing\n","            processed_text = self.combine_sections(current_sections)\n","            remaining_text = self.combine_sections(sections[section_index:])\n","            \n","            return processed_text, remaining_text\n","            \n","        except Exception as e:\n","            self.log_message(f\"Error in get_complete_paragraphs: {str(e)}\")\n","            # Return empty chunk and original text on error\n","            return \"\", text\n","\n","    \n","    def create_initial_chunks(self, text: str) -> List[str]:\n","        \"\"\"\n","        Create initial chunks while preserving document structure.\n","        Returns empty list and logs error if no valid chunks can be created.\n","        \"\"\"\n","        chunks = []\n","        remaining_text = text\n","        chunk_number = 0\n","        \n","        try:\n","            while remaining_text.strip():\n","                chunk_number += 1\n","                self.log_message(f\"\\nProcessing chunk {chunk_number}\")\n","                \n","                # Handle missed text from previous chunk\n","                if self.missed_text:\n","                    self.log_message(\"Adding missed text from previous chunk\")\n","                    remaining_text = self.missed_text + '\\n\\n' + remaining_text\n","                    self.missed_text = \"\"\n","                \n","                # Get complete paragraphs up to token limit\n","                chunk_text, remaining_text = self.get_complete_paragraphs(\n","                    remaining_text, \n","                    self.max_tokens\n","                )\n","                \n","                if not chunk_text.strip():\n","                    self.log_message(\"Warning: Empty chunk produced\")\n","                    if len(remaining_text.strip()) < self.max_tokens:\n","                        # Handle last small chunk\n","                        if remaining_text.strip():\n","                            chunks.append(remaining_text.strip())\n","                        break\n","                    else:\n","                        # Force split if stuck\n","                        chunk_text = remaining_text[:self.max_tokens*4]  # Use characters as approximation\n","                        remaining_text = remaining_text[self.max_tokens*4:]\n","                \n","                token_count = self.count_tokens(chunk_text)\n","                if token_count > 0:  # Prevent empty chunks\n","                    self.log_message(f\"Created chunk {chunk_number} ({token_count} tokens)\")\n","                    chunks.append(chunk_text)\n","                    \n","                # Safeguards\n","                if not remaining_text.strip():\n","                    break\n","                if len(chunks) >= 100:  # Limit total chunks\n","                    self.log_message(\"Warning: Maximum chunk limit reached\")\n","                    break\n","                if chunk_number > 10 and not chunks:  # Detect processing failure\n","                    raise RuntimeError(\"Failed to create any valid chunks after multiple attempts\")\n","                    \n","        except Exception as e:\n","            self.log_message(f\"Error in chunk creation: {str(e)}\")\n","            if not chunks:  # Ensure at least one chunk is created\n","                self.log_message(\"Attempting emergency chunk creation\")\n","                try:\n","                    # Create a single chunk with first max_tokens characters\n","                    chunks = [text[:self.max_tokens*4].strip()]\n","                except:\n","                    self.log_message(\"Emergency chunk creation failed\")\n","                    \n","        finally:\n","            # Always save whatever chunks were created\n","            if chunks:\n","                os.makedirs(self.log_dir, exist_ok=True)\n","                for i, chunk in enumerate(chunks):\n","                    chunk_file = os.path.join(self.log_dir, f\"chunk_{i+1:04d}.txt\")\n","                    with open(chunk_file, 'w', encoding='utf-8') as f:\n","                        f.write(chunk)\n","                self.log_message(f\"Created {len(chunks)} initial chunks\")\n","            else:\n","                self.log_message(\"Warning: No chunks were created\")\n","                \n","            return chunks\n","\n","    def get_semantic_sections(self, chunk: str) -> Tuple[List[str], Dict]:\n","        \"\"\"\n","        Process chunks through LLM for semantic analysis with improved handling.\n","        \"\"\"\n","        try:\n","            self.log_message(f\"Sending request to LLM (input tokens: {self.count_tokens(chunk)})\")\n","            \n","            # Enhanced prompt for better structure preservation\n","            system_prompt = \"\"\"You are a text analysis expert. Your task is to:\n","            1. Maintain the original document structure exactly as provided\n","            2. Split the input into semantically coherent sections\n","            3. Preserve all formatting, spacing, and special characters\n","            4. Keep headers with their associated content\n","            5. Keep lists and quotes intact within their sections\n","            6. Mark sections using <START_SECTION> and <END_SECTION> tags\n","            7. Mark incomplete sections with <INCOMPLETE> tags\n","            8. Handle front matter, tables of contents, and chapter headings appropriately\n","            9. Preserve all original line breaks and paragraph spacing\n","\n","            Critical Rules:\n","            - Do not modify any text content\n","            - Preserve all original formatting\n","            - Keep structural elements together (headers with content)\n","            - Maintain document hierarchy\n","            - Include ALL text - do not skip anything\n","            \"\"\"\n","            \n","            response = self.client.chat.completions.create(\n","                model=self.model_name,\n","                messages=[\n","                    {\"role\": \"system\", \"content\": system_prompt},\n","                    {\"role\": \"user\", \"content\": f\"Split this text into sections, preserving ALL content and structure:\\n\\n{chunk}\"}\n","                ],\n","                max_tokens=self.max_tokens,\n","                temperature=0.2,\n","                timeout=60  # 1-minute timeout\n","            )\n","            \n","            result = response.choices[0].message.content\n","            \n","            # Extract sections with improved parsing\n","            sections = []\n","            section_pattern = r'<START_SECTION>(.*?)<END_SECTION>'\n","            for match in re.finditer(section_pattern, result, re.DOTALL):\n","                section_text = match.group(1).strip()\n","                if section_text:  # Keep even short sections if they're structural\n","                    if len(section_text) > 50 or any(marker in section_text.upper() \n","                        for marker in ['CHAPTER', 'CONTENTS', 'NOTE', 'INTRODUCTION']):\n","                        sections.append(section_text)\n","            \n","            # Handle incomplete sections\n","            incomplete_pattern = r'<INCOMPLETE>(.*?)</INCOMPLETE>'\n","            incomplete_match = re.search(incomplete_pattern, result, re.DOTALL)\n","            if incomplete_match:\n","                incomplete_text = incomplete_match.group(1).strip()\n","                if incomplete_text:\n","                    self.missed_text = incomplete_text\n","                    self.log_message(f\"Found incomplete section ({self.count_tokens(incomplete_text)} tokens)\")\n","            \n","            # Verify content preservation\n","            if not sections:\n","                self.log_message(\"Warning: No sections created by LLM\")\n","                self.missed_text = chunk\n","            elif not incomplete_match:\n","                missed_text = self.verify_output_completeness(chunk, sections)\n","                if missed_text:\n","                    self.missed_text = missed_text\n","                    self.log_message(f\"Found missed text ({self.count_tokens(missed_text)} tokens)\")\n","            \n","            # Collect metrics\n","            metrics = {\n","                \"completion_tokens\": response.usage.completion_tokens,\n","                \"prompt_tokens\": response.usage.prompt_tokens,\n","                \"total_tokens\": response.usage.total_tokens,\n","                \"finish_reason\": response.choices[0].finish_reason,\n","                \"sections_created\": len(sections),\n","                \"has_missed_text\": bool(self.missed_text),\n","                \"avg_section_length\": sum(len(s) for s in sections) / len(sections) if sections else 0,\n","                \"timestamp\": datetime.now().isoformat()\n","            }\n","            \n","            return sections, metrics\n","                \n","        except Exception as e:\n","            self.log_message(f\"Error in LLM request: {str(e)}\")\n","            return [], {}\n","\n","    def verify_output_completeness(self, input_text: str, output_sections: List[str]) -> str:\n","        \"\"\"\n","        Enhanced verification of content preservation with improved detection.\n","        \"\"\"\n","        # Normalize texts for comparison\n","        input_normalized = ' '.join(input_text.split())\n","        output_normalized = ' '.join(' '.join(output_sections).split())\n","        \n","        # Quick full-text comparison\n","        if input_normalized == output_normalized:\n","            return \"\"\n","        \n","        # Find missing content using sliding window\n","        words = input_normalized.split()\n","        missing_sequences = set()  # Use set to avoid duplicates\n","        \n","        # Use multiple window sizes for better detection\n","        for window_size in [5, 10, 15]:  # Try different window sizes\n","            i = 0\n","            while i < len(words) - window_size:\n","                sequence = ' '.join(words[i:i+window_size])\n","                if sequence not in output_normalized:\n","                    # Find complete missing phrase\n","                    start = i\n","                    while start > 0 and ' '.join(words[start-1:i+window_size]) not in output_normalized:\n","                        start -= 1\n","                    end = i + window_size\n","                    while end < len(words) and ' '.join(words[i:end+1]) not in output_normalized:\n","                        end += 1\n","                    missing_sequences.add(' '.join(words[start:end]))\n","                    i = end\n","                else:\n","                    i += 1\n","        \n","        return '\\n'.join(sorted(missing_sequences)) if missing_sequences else \"\"\n","\n","    def validate_chunk(self, chunk: str, original_sections: List[Section]) -> bool:\n","        \"\"\"\n","        Comprehensive chunk validation with detailed reporting.\n","        \"\"\"\n","        # Normalize texts for comparison\n","        chunk_text = ' '.join(chunk.split())\n","        \n","        # Track missing content by section type\n","        missing_by_type = {}\n","        \n","        for section in original_sections:\n","            section_text = ' '.join(section.text.split())\n","            \n","            # For headers and front matter, require exact matches\n","            if section.type in [SectionType.HEADER, SectionType.FRONT_MATTER]:\n","                if section_text not in chunk_text:\n","                    missing_by_type.setdefault(section.type, []).append(section.text)\n","                continue\n","            \n","            # For other content, use sliding window detection\n","            words = section_text.split()\n","            window_size = 5\n","            missing_chunks = set()\n","            \n","            i = 0\n","            while i < len(words) - window_size:\n","                sequence = ' '.join(words[i:i+window_size])\n","                if sequence not in chunk_text:\n","                    # Find complete phrase\n","                    start = i\n","                    while start > 0 and ' '.join(words[start-1:i+window_size]) not in chunk_text:\n","                        start -= 1\n","                    end = i + window_size\n","                    while end < len(words) and ' '.join(words[i:end+1]) not in chunk_text:\n","                        end += 1\n","                    missing_chunks.add(' '.join(words[start:end]))\n","                    i = end\n","                else:\n","                    i += 1\n","            \n","            if missing_chunks:\n","                missing_by_type.setdefault(section.type, []).extend(missing_chunks)\n","        \n","        # Report missing content by type\n","        if missing_by_type:\n","            self.log_message(\"Missing content detected:\")\n","            for section_type, missing_content in missing_by_type.items():\n","                self.log_message(f\"\\n{section_type.value}:\")\n","                for content in missing_content:\n","                    self.log_message(f\"  - {content[:100]}...\")\n","            return False\n","        \n","        return True\n","\n","    def combine_sections(self, sections: List[Section]) -> str:\n","        \"\"\"Safely combine sections with proper spacing.\"\"\"\n","        if not sections:\n","            return \"\"\n","            \n","        parts = []\n","        for section in sections:\n","            if parts:  # Add spacing between sections\n","                parts.append(\"\")\n","                if section.type == SectionType.HEADER:\n","                    parts.append(\"\")  # Extra space for headers\n","            parts.append(section.text.rstrip())\n","            \n","        return \"\\n\".join(parts)\n","\n","    def save_chunk_log(self, chunk_num: int, original_chunk: str, \n","                    sections: List[str], metrics: Dict):\n","        \"\"\"Save processing logs with proper error handling.\"\"\"\n","        try:\n","            log_file = os.path.join(self.log_dir, f\"chunk_{chunk_num:04d}.json\")\n","            \n","            # Safely calculate averages\n","            num_sections = len(sections) if sections else 1\n","            total_chars = sum(len(s) for s in sections) if sections else 0\n","            \n","            log_data = {\n","                \"chunk_number\": chunk_num,\n","                \"timestamp\": datetime.now().isoformat(),\n","                \"original_text\": {\n","                    \"content\": original_chunk[:1000],  # Limit size\n","                    \"length\": len(original_chunk),\n","                    \"tokens\": self.count_tokens(original_chunk)\n","                },\n","                \"sections\": {\n","                    \"count\": len(sections),\n","                    \"average_length\": total_chars / num_sections,\n","                    \"content\": [s[:1000] for s in sections]  # Limit size\n","                },\n","                \"metrics\": metrics\n","            }\n","            \n","            with open(log_file, 'w', encoding='utf-8') as f:\n","                json.dump(log_data, f, indent=2, ensure_ascii=False)\n","                \n","        except Exception as e:\n","            self.log_message(f\"Error saving chunk log: {str(e)}\")\n","            \n","    def process_text(self, text: str, max_chunks: int = None) -> List[str]:\n","        \"\"\"\n","        Main text processing pipeline with enhanced error handling and logging.\n","        \"\"\"\n","        self.log_message(\"Starting text processing\")\n","        \n","        try:\n","            # Create initial chunks\n","            initial_chunks = self.create_initial_chunks(text)\n","            \n","            if max_chunks:\n","                initial_chunks = initial_chunks[:max_chunks]\n","                self.log_message(f\"Processing limited to first {max_chunks} chunks\")\n","            \n","            # Process each chunk\n","            semantic_chunks = []\n","            for i, chunk in enumerate(initial_chunks):\n","                self.log_message(f\"\\nProcessing chunk {i+1}/{len(initial_chunks)}\")\n","                \n","                # Detailed chunk analysis\n","                chunk_tokens = self.count_tokens(chunk)\n","                self.log_message(f\"Chunk size: {len(chunk)} chars, {chunk_tokens} tokens\")\n","                \n","                # Print input preview\n","                self.print_separator(\"INPUT CHUNK\")\n","                print(f\"Chunk {i+1}:\")\n","                print(\"=\"*80)\n","                print(chunk[:1000] + \"...\" if len(chunk) > 1000 else chunk)\n","                print(\"=\"*80)\n","                \n","                # Get semantic sections\n","                sections, metrics = self.get_semantic_sections(chunk)\n","                \n","                # Process and validate sections\n","                self.print_separator(\"SEMANTIC SECTIONS\")\n","                for j, section in enumerate(sections):\n","                    section_tokens = self.count_tokens(section)\n","                    print(f\"\\nSection {j+1} ({section_tokens} tokens):\")\n","                    print(\"-\"*40)\n","                    print(section[:500] + \"...\" if len(section) > 500 else section)\n","                    print(\"-\"*40)\n","                \n","                # Print metrics\n","                self.print_separator(\"PROCESSING METRICS\")\n","                pprint(metrics)\n","                \n","                if self.missed_text:\n","                    self.print_separator(\"MISSED CONTENT\")\n","                    print(self.missed_text)\n","                \n","                semantic_chunks.extend(sections)\n","                \n","                # Save detailed processing log\n","                self.save_chunk_log(i+1, chunk, sections, metrics)\n","                \n","                time.sleep(1)  # Rate limiting\n","            \n","            self.log_message(f\"Processing complete. Created {len(semantic_chunks)} semantic chunks\")\n","            return semantic_chunks\n","            \n","        except Exception as e:\n","            self.log_message(f\"Error in text processing: {str(e)}\")\n","            raise\n","\n","def main():\n","    \"\"\"\n","    Main execution function with enhanced error handling and reporting.\n","    \"\"\"\n","    try:\n","        # Initialize chunker\n","        chunker = SemanticChunker()\n","        \n","        # Read input file\n","        input_file = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","        \n","        if not os.path.exists(input_file):\n","            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n","        \n","        with open(input_file, 'r', encoding='utf-8') as f:\n","            text = f.read()\n","        \n","        chunker.log_message(f\"Starting processing of {input_file}\")\n","        chunker.log_message(f\"Input text: {len(text)} chars, {chunker.count_tokens(text)} tokens\")\n","        \n","        # Process text with limit for testing\n","        semantic_chunks = chunker.process_text(text, max_chunks=3)\n","        \n","        # Save final chunks\n","        output_dir = os.path.join(chunker.log_dir, \"semantic_chunks\")\n","        os.makedirs(output_dir, exist_ok=True)\n","        \n","        for i, chunk in enumerate(semantic_chunks):\n","            output_file = os.path.join(output_dir, f\"semantic_chunk_{i+1:04d}.txt\")\n","            with open(output_file, 'w', encoding='utf-8') as f:\n","                f.write(chunk)\n","        \n","        chunker.log_message(f\"Saved {len(semantic_chunks)} semantic chunks to {output_dir}\")\n","        \n","        # Generate processing summary\n","        summary_file = os.path.join(chunker.log_dir, \"processing_summary.json\")\n","        summary = {\n","            \"timestamp\": datetime.now().isoformat(),\n","            \"input_file\": input_file,\n","            \"input_stats\": {\n","                \"chars\": len(text),\n","                \"tokens\": chunker.count_tokens(text)\n","            },\n","            \"output_stats\": {\n","                \"total_chunks\": len(semantic_chunks),\n","                \"total_tokens\": sum(chunker.count_tokens(c) for c in semantic_chunks),\n","                \"avg_chunk_size\": sum(len(c) for c in semantic_chunks) / len(semantic_chunks)\n","            },\n","            \"output_directory\": output_dir\n","        }\n","        \n","        with open(summary_file, 'w', encoding='utf-8') as f:\n","            json.dump(summary, f, indent=2)\n","        \n","    except Exception as e:\n","        print(f\"Error in main execution: {str(e)}\")\n","        raise\n","\n","def test_structure_analysis():\n","    \"\"\"\n","    Enhanced test function with detailed validation and reporting.\n","    \"\"\"\n","    chunker = SemanticChunker()\n","    \n","    test_text = \"\"\"\n","                             AUTHOR'S NOTE\n","\n","My task in this work has been to investigate an individual phantasy\n","system, and in the doing of it problems of such magnitude have been\n","uncovered, that my endeavor to grasp them in their entirety has\n","necessarily meant only a superficial orientation toward those paths, the\n","opening and exploration of which may possibly crown the work of future\n","investigators with success.\n","\n","                                CONTENTS\n","\n","        INTRODUCTION                                                     3\n","        \n","        Relation of the Incest Phantasy to the Oedipus Legendâ€”Moral\n","        revulsion over such a discovery\n","\n"," I.â€”    CONCERNING THE TWO KINDS OF THINKING                             8\n","\"\"\"\n","    \n","    try:\n","        print(\"\\nTesting structural analysis...\")\n","        sections = chunker.analyze_text_structure(test_text)\n","        \n","        print(\"\\nIdentified sections:\")\n","        for i, section in enumerate(sections, 1):\n","            print(f\"\\nSection {i}:\")\n","            print(\"=\"*80)\n","            print(f\"Type: {section.type}\")\n","            print(f\"Level: {section.level}\")\n","            print(f\"Length: {len(section.text)} chars, {chunker.count_tokens(section.text)} tokens\")\n","            print(\"-\"*40)\n","            print(section.text)\n","            print(\"=\"*80)\n","        \n","        print(\"\\nTesting chunking with structure preservation...\")\n","        chunks = chunker.create_initial_chunks(test_text)\n","        \n","        print(\"\\nResulting chunks:\")\n","        for i, chunk in enumerate(chunks, 1):\n","            print(f\"\\nChunk {i}:\")\n","            print(\"=\"*80)\n","            print(chunk)\n","            print(\"=\"*80)\n","            \n","        print(\"\\nValidating chunk content...\")\n","        for i, chunk in enumerate(chunks, 1):\n","            print(f\"\\nValidating chunk {i}:\")\n","            is_valid = chunker.validate_chunk(chunk, sections)\n","            print(f\"Chunk {i} validation: {'PASSED' if is_valid else 'FAILED'}\")\n","            \n","        # Generate test summary\n","        test_summary = {\n","            \"timestamp\": datetime.now().isoformat(),\n","            \"sections_identified\": len(sections),\n","            \"chunks_created\": len(chunks),\n","            \"section_types\": {str(s.type): sum(1 for sec in sections if sec.type == s.type) for s in sections},\n","            \"validation_results\": [chunker.validate_chunk(c, sections) for c in chunks]\n","        }\n","        \n","        print(\"\\nTest Summary:\")\n","        pprint(test_summary)\n","            \n","    except Exception as e:\n","        print(f\"Error during testing: {str(e)}\")\n","        raise\n","\n","if __name__ == \"__main__\":\n","    if os.environ.get(\"SEMANTIC_CHUNKER_TEST\"):\n","        test_structure_analysis()\n","    else:\n","        main()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.825136Z","iopub.status.busy":"2024-11-04T08:49:54.824806Z","iopub.status.idle":"2024-11-04T08:49:54.833600Z","shell.execute_reply":"2024-11-04T08:49:54.832541Z","shell.execute_reply.started":"2024-11-04T08:49:54.825103Z"},"trusted":true},"outputs":[],"source":["# # ----------------------------- #\n","# # Part 3: Parse Text into Discourse Units\n","# # ----------------------------- #\n","\n","def parse_discourse_units(text, overwrite=False):\n","    \"\"\"\n","    Parses text into discourse units using spaCy.\n","    Currently splits text into sentences.\n","    \"\"\"\n","    paragraphs = text.split('\\n\\n')\n","    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n","\n","    discourse_units = []\n","    for para in paragraphs:\n","        doc = nlp(para)\n","        sentences = [sent.text for sent in doc.sents]\n","        discourse_units.extend(sentences)\n","\n","    output_path = '/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_discourse_units.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Discourse Units: {len(discourse_units)}\")\n","    return discourse_units"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.987822Z","iopub.status.busy":"2024-11-04T08:49:54.987546Z","iopub.status.idle":"2024-11-04T08:49:54.997464Z","shell.execute_reply":"2024-11-04T08:49:54.996587Z","shell.execute_reply.started":"2024-11-04T08:49:54.987793Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 4: Create Chunks Using Hybrid Strategy\n","# ----------------------------- #\n","\n","def create_chunks(discourse_units, tokenizer, max_length=4096, overlap_size=1, overwrite=False):\n","    \"\"\"\n","    Creates chunks from discourse units using a sliding window with overlapping chunks.\n","    Optimized to work directly with token IDs and utilize efficient list operations.\n","    \"\"\"\n","    chunks = []\n","    current_chunk_tokens = []\n","    current_length = 0\n","\n","    for unit in discourse_units:\n","        unit_tokens = tokenizer.encode(unit, add_special_tokens=True)\n","        unit_length = len(unit_tokens)\n","\n","        if current_length + unit_length <= max_length:\n","            current_chunk_tokens.extend(unit_tokens)\n","            current_length += unit_length\n","        else:\n","            # Decode and append the current chunk\n","            chunk_text = tokenizer.decode(\n","                current_chunk_tokens, skip_special_tokens=True)\n","            chunks.append(chunk_text)\n","\n","            # Prepare overlap tokens\n","            overlap_tokens = current_chunk_tokens[-overlap_size:]\n","            current_chunk_tokens = overlap_tokens + unit_tokens\n","            current_length = len(current_chunk_tokens)\n","\n","    # Append any remaining tokens as the last chunk\n","    if current_chunk_tokens:\n","        chunk_text = tokenizer.decode(\n","            current_chunk_tokens, skip_special_tokens=True)\n","        chunks.append(chunk_text)\n","\n","    # Write or read chunks as before\n","    output_path = '/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_chunks.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Chunks Created: {len(chunks)}\")\n","    return chunks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:56.853343Z","iopub.status.busy":"2024-11-04T08:49:56.852589Z","iopub.status.idle":"2024-11-04T08:49:56.862517Z","shell.execute_reply":"2024-11-04T08:49:56.861550Z","shell.execute_reply.started":"2024-11-04T08:49:56.853301Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 5: Create and Tokenize Dataset\n","# ----------------------------- #\n","\n","# To Do - make book titles and prompt generic so\n","def create_tokenized_dataset(chunks, tokenizer, max_length=1024, num_proc=2):\n","\n","    # Create a Dataset object from chunks\n","\n","    book_title = 'Psychology of the Unconscious by C. G. Jung'\n","    wikipedia_prompt = \"\"\"\n","    Psychology Book\n","\n","    ### Title: {}\n","\n","    ### Article: {}\n","    \"\"\"\n","\n","    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n","\n","    def formatting_prompts_func(examples):\n","        titles = book_title\n","        texts = examples[\"text\"]\n","        outputs = []\n","        for title, text in zip([book_title]*len(chunks), texts):\n","            text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n","            outputs.append(text)\n","        return {\"text\": outputs, }\n","    pass\n","\n","    # convert chunks variable to huggingface dataset\n","\n","    from datasets import Dataset\n","\n","    dataset = Dataset.from_dict({\"text\": chunks})\n","\n","    dataset = dataset.map(formatting_prompts_func,\n","                          batched=True, num_proc=num_proc)\n","    # Split the dataset into training and validation sets\n","    split = dataset.train_test_split(test_size=0.1, seed=42)\n","    train_dataset = split['train']\n","    eval_dataset = split['test']\n","\n","    print(len(dataset))\n","    # Find the maximum length of the text field in the entire dataset\n","    max_length = max(len(text) for text in dataset['text'])\n","    print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","#     print(f\"First 5 rows of training dataset: {train_dataset[:5]}\")\n","    print(f\"Validation Dataset Size: {len(eval_dataset)}\")\n","#     print(f\"First 5 rows of validation dataset: {eval_dataset[:5]}\")\n","    return train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:50:01.810305Z","iopub.status.busy":"2024-11-04T08:50:01.809400Z","iopub.status.idle":"2024-11-04T08:50:01.844994Z","shell.execute_reply":"2024-11-04T08:50:01.844131Z","shell.execute_reply.started":"2024-11-04T08:50:01.810262Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 6: Set up environment and other important utilities\n","# ----------------------------- #\n","\n","def setup_environment():\n","    \"\"\"\n","    Initializes the Accelerator for distributed training.\n","    \"\"\"\n","    return Accelerator()\n","\n","\n","def get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n","    \"\"\"\n","    Defines a custom learning rate scheduler with warmup, constant, and linear annealing phases.\n","    \"\"\"\n","    def lr_lambda(current_step):\n","        if current_step < num_warmup_steps:\n","            return current_step / num_warmup_steps  # Linear warmup\n","        elif current_step < initial_phase_steps:\n","            return 1.0  # Constant learning rate for initial phase\n","        else:\n","            # Linear annealing for the remaining steps\n","            return 1.0 - ((current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps))\n","\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","\n","def setup_training_steps(initial_rows, annealing_rows, batch_size, gradient_accumulation_steps, num_epochs):\n","    \"\"\"\n","    Calculates total and initial training steps based on dataset size and training parameters.\n","    \"\"\"\n","    total_rows = initial_rows + annealing_rows\n","    total_steps = (total_rows * num_epochs) // (batch_size *\n","                                                gradient_accumulation_steps)\n","    initial_steps = (initial_rows * num_epochs) // (batch_size *\n","                                                    gradient_accumulation_steps)\n","    return max(1, total_steps), max(1, initial_steps)\n","\n","\n","def print_memory_usage(step_desc):\n","    \"\"\"\n","    Prints the CUDA memory summary if CUDA is available.\n","    \"\"\"\n","    if torch.cuda.is_available():\n","        print(f\"Memory Usage at {step_desc}:\")\n","        print(torch.cuda.memory_summary())\n","        print(\"\\n\")\n","    else:\n","        print(f\"No CUDA available at {step_desc}.\\n\")\n","\n","\n","def inference(model, tokenizer):\n","    \"\"\"\n","    Runs inference using the trained model.\n","    \"\"\"\n","    # Define sample prompts\n","    prompts = [\n","        \"Explain what is medical anthropology and its importance in elevating the quality of life?\",\n","        \"Explain what are the types of Jungian archetypes and how they manifest in the human psyche?\"\n","    ]\n","\n","    for prompt in prompts:\n","        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, max_length=256)\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n","        \n","def compute_metrics(eval_pred):\n","    \"\"\"\n","    Computes perplexity based on model predictions and labels.\n","    \"\"\"\n","    logits, labels = eval_pred\n","    # Convert to torch tensors\n","    logits = torch.tensor(logits)\n","    labels = torch.tensor(labels)\n","    \n","    # Ensure shapes match\n","    if logits.shape[:2] != labels.shape:\n","        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n","    \n","    # Shift logits and labels\n","    shift_logits = logits[:, :-1, :].contiguous()\n","    shift_labels = labels[:, 1:].contiguous()\n","\n","    # Check label values\n","    if shift_labels.max() >= model.config.vocab_size:\n","        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n","    \n","    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n","    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","    perplexity = torch.exp(loss).item()\n","    return {\"perplexity\": perplexity}\n","\n","#  Login to Huggingface\n","from huggingface_hub import login\n","import os\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","def setup_huggingface_access():\n","    \"\"\"Setup Hugging Face access with token from environment or manual input\"\"\"\n","    # First try to get token from environment variable\n","    token = os.getenv('HUGGINGFACE_TOKEN')\n","    \n","    if not token:\n","        # If not in environment, prompt for token\n","        token = input(\"Enter your Hugging Face token: \")\n","        \n","    if token:\n","        try:\n","            login(token, add_to_git_credential=True)\n","            print(\"Successfully logged in to Hugging Face!\")\n","        except Exception as e:\n","            print(f\"Failed to log in to Hugging Face: {str(e)}\")\n","            return False\n","    else:\n","        print(\"No Hugging Face token provided\")\n","        return False\n","    \n","    return True"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:51:23.895754Z","iopub.status.busy":"2024-11-04T08:51:23.894928Z","iopub.status.idle":"2024-11-04T08:51:23.904037Z","shell.execute_reply":"2024-11-04T08:51:23.903118Z","shell.execute_reply.started":"2024-11-04T08:51:23.895714Z"},"trusted":true},"outputs":[],"source":["def load_model_and_tokenizer(base_model_slug, max_seq_length=4096, dtype=None, load_in_4bit=True):\n","    \"\"\"\n","    Load and configure the model and tokenizer with specified parameters on a single GPU.\n","    \"\"\"\n","    import torch\n","    import os\n","\n","    # Force CUDA if available\n","    if torch.cuda.is_available():\n","        print(\"CUDA is available.\")\n","        print(f\"Using GPU: {torch.cuda.get_device_properties(0).name}\")\n","        device = torch.device(\"cuda:0\")\n","        device_map = {\"\": 0}  # Force everything to GPU 0\n","    else:\n","        print(\"WARNING: CUDA is not available. Using CPU.\")\n","        device = torch.device(\"cpu\")\n","        device_map = \"cpu\"\n","\n","    # Print initial GPU memory\n","    if torch.cuda.is_available():\n","        print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n","\n","    try:\n","        # Load base model and tokenizer\n","        model, tokenizer = FastLanguageModel.from_pretrained(\n","            model_name=base_model_slug,\n","            max_seq_length=max_seq_length,\n","            dtype=dtype,\n","            load_in_4bit=load_in_4bit,\n","            device_map=device_map,\n","            token=os.getenv('HUGGINGFACE_TOKEN'),\n","        )\n","        \n","        print(f\"Model device after loading: {next(model.parameters()).device}\")\n","        \n","        # Configure PEFT model\n","        model = FastLanguageModel.get_peft_model(\n","            model,\n","            r=128,\n","            target_modules=[\n","                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                \"gate_proj\", \"up_proj\", \"down_proj\",\n","                \"embed_tokens\", \"lm_head\",\n","            ],\n","            lora_alpha=32,\n","            lora_dropout=0,\n","            bias=\"none\",\n","            use_gradient_checkpointing=\"unsloth\",\n","            random_state=3407,\n","            use_rslora=True,\n","            loftq_config=None,\n","        )\n","        \n","        # Ensure model is on GPU after PEFT configuration\n","        if torch.cuda.is_available():\n","            model = model.to(device)\n","            \n","        # Verify final device placement\n","        print(f\"Final model device: {next(model.parameters()).device}\")\n","        \n","        # Print GPU memory usage\n","        if torch.cuda.is_available():\n","            print(f\"\\nGPU Memory After Complete Setup: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n","            \n","    except Exception as e:\n","        print(f\"Error in model loading/configuration: {str(e)}\")\n","        raise\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:51:27.038467Z","iopub.status.busy":"2024-11-04T08:51:27.038068Z","iopub.status.idle":"2024-11-04T08:51:27.044121Z","shell.execute_reply":"2024-11-04T08:51:27.043250Z","shell.execute_reply.started":"2024-11-04T08:51:27.038425Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","# Set the environment variable\n","os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n","os.environ[\"WANDB_API_KEY\"] = \"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\"\n","# Verify it's set correctly\n","print(os.getenv(\"HUGGINGFACE_TOKEN\"))\n","print(os.getenv(\"WANDB_API_KEY\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:53:40.380684Z","iopub.status.busy":"2024-11-04T08:53:40.380279Z","iopub.status.idle":"2024-11-04T08:54:30.243475Z","shell.execute_reply":"2024-11-04T08:54:30.242381Z","shell.execute_reply.started":"2024-11-04T08:53:40.380645Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 2: Configure Environment Variables & Create Main Variables\n","# ----------------------------- #\n","\n","# Unsloth modell initialization variables\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","max_length = max_seq_length\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","# device_map = \"auto\"\n","base_model_slug = \"Qwen/Qwen2.5-7B-Instruct\"\n","base_model_name = \"lora_model_pum\"\n","chunks_max_length = max_seq_length\n","overlap_size = 1\n","# Define your parameters\n","batchSize = 2\n","ga = 8\n","maxSteps = 10\n","warmupSteps = 10\n","numTrainEpochs = 1\n","lRate = 5e-5\n","embLRate = 1e-5\n","optim = \"adamw_8bit\"\n","lrSchedule = \"linear\"\n","dataset_slug = \"psychology_of_unconscious\"\n","\n","from datetime import datetime\n","import pytz\n","import wandb\n","# Get the current date and time in Indian Standard Time (IST)\n","ist = pytz.timezone('Asia/Kolkata')\n","current_datetime = datetime.now(ist)\n","\n","# Format the datetime string\n","# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n","formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Define Run Name\n","run_name = f\"\"\"Kaggle-quantumLeap-{formatted_datetime}-{base_model_slug}-{dataset_slug}-{max_seq_length}_maxSeqLength-{max_length}_maxLength-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{numTrainEpochs}_numTrainEpochs-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n","\n","# Initialize Weights & Biases\n","# It's recommended to set your W&B API key as an environment variable for security.\n","wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n","wandb.init(project=\"OLA-quantumLeap\", name=run_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:55:17.806518Z","iopub.status.busy":"2024-11-04T08:55:17.806143Z","iopub.status.idle":"2024-11-04T08:55:20.860165Z","shell.execute_reply":"2024-11-04T08:55:20.859337Z","shell.execute_reply.started":"2024-11-04T08:55:17.806483Z"},"trusted":true},"outputs":[],"source":["\n","# ----------------------------- #\n","# Part 9: Data Processing\n","# ----------------------------- #\n","\n","# # Perform Inference Before Training\n","# inference(model, tokenizer)\n","\n","# Set number of processes to use for data loading\n","num_cpus = multiprocessing.cpu_count()\n","num_proc = max(num_cpus-2, 2)  # Adjust based on prior recommendations\n","print(f\"Number of CPU cores: {num_cpus}\")\n","print(f\"Number of processes: {num_proc}\")\n","\n","# Login to Hugging Face\n","if not setup_huggingface_access():\n","    raise Exception(\"Failed to setup Hugging Face access. Please check your token.\")\n","\n","# Load Model and Tokenizer\n","model, tokenizer = load_model_and_tokenizer(base_model_slug)\n","print(f\"Model Device: {model.device}\")\n","\n","# Load and Clean Text Data\n","file_path = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","clean_text = load_and_clean_text(file_path)\n","\n","# Parse Discourse Units\n","discourse_units = parse_discourse_units(clean_text, overwrite=True)\n","\n","# Create Chunks\n","chunks = create_chunks(\n","    discourse_units,\n","    tokenizer,\n","    max_length=max_length,\n","    overlap_size=overlap_size,\n","    overwrite=True,\n",")\n","\n","# Create Tokenized Dataset\n","train_dataset, eval_dataset = create_tokenized_dataset(\n","    chunks, tokenizer, max_length)\n","\n","# Save datasets as Hugging Face `datasets`\n","train_dataset.save_to_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","eval_dataset.save_to_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')\n","\n","### To Do - Make the below as dynamic and as a functio\n","# # Uncomment following if you want to just load the data from temp directory\n","# from datasets import load_from_disk\n","\n","# train_dataset = load_from_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","# eval_dataset = load_from_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import IntervalStrategy\n","from transformers.integrations import TensorBoardCallback\n","\n","import wandb\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_dataset,\n","    eval_dataset = eval_dataset,  # Use 10% of data for evaluation\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = batchSize,\n","        gradient_accumulation_steps = ga,\n","\n","        # Set both max_steps and num_train_epochs\n","        max_steps = maxSteps,\n","        num_train_epochs = numTrainEpochs,\n","\n","        # Use a single learning rate for all parameters\n","        learning_rate = lRate,\n","\n","        # Warmup strategy from successful runs\n","        warmup_steps = warmupSteps,\n","        # warmup_ratio = 0,\n","\n","        # Explicitly set precision based on hardware support\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        \n","        logging_steps = 1,\n","        \n","        optim = optim,\n","        weight_decay = 0.01,\n","        lr_scheduler_type = lrSchedule,\n","        \n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        \n","        report_to=[\"tensorboard\", \"wandb\"],\n","        logging_dir=f\"./trel-fft-logs/{run_name}\",\n","        \n","        # Set both save and evaluation strategies to 'steps'\n","        # save_strategy = IntervalStrategy.STEPS,\n","        # eval_strategy = IntervalStrategy.STEPS,\n","        # save_steps = 1,  # Save checkpoint every 20 steps\n","        # eval_steps = 1,  # Evaluate every 20 steps (matching save_steps)\n","        \n","        # load_best_model_at_end = True,\n","        # metric_for_best_model = \"eval_loss\",\n","    ),\n","    # compute_metrics = compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %%time\n","\n","# instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","# ### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","# concept_name: {}\n","# detailed_explanation: {}\n","\n","# ### Response:\n","# {}\"\"\"\n","\n","# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","# outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n","# tokenizer.batch_decode(outputs)\n","\n","\n","# %%time\n","# # Text Streaming goes into a loop and doesnt adher to EOS\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)\n","\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:55:41.075256Z","iopub.status.busy":"2024-11-04T08:55:41.074858Z","iopub.status.idle":"2024-11-04T10:16:26.590311Z","shell.execute_reply":"2024-11-04T10:16:26.589250Z","shell.execute_reply.started":"2024-11-04T08:55:41.075219Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 11: Start Training\n","# ----------------------------- #\n","\n","#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","trainer_stats = trainer.train()\n","\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import wandb\n","from pprint import pprint\n","\n","def get_run_config(project_name, run_id):\n","    try:\n","        # Initialize the wandb API\n","        api = wandb.Api()\n","\n","        # Access the specific run\n","        run = api.run(f\"{project_name}/{run_id}\")\n","\n","        # Get the full configuration\n","        config = run.config\n","\n","        # Filter for trainer-specific configuration\n","        trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n","\n","        return trainer_config\n","\n","    except wandb.errors.CommError:\n","        print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n","        return None\n","    except Exception as e:\n","        print(f\"An error occurred: {str(e)}\")\n","        return None\n","\n","# Usage\n","project_name = \"olabs-asia-olabs-pro/OLA-quantumLeap\"\n","run_id = \"we4axhd1\"\n","\n","trainer_config = get_run_config(project_name, run_id)\n","\n","if trainer_config:\n","    print(f\"Trainer configuration for run {run_id}:\")\n","    pprint(trainer_config)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %%time\n","\n","# instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","# ### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","# concept_name: {}\n","# detailed_explanation: {}\n","\n","# ### Response:\n","# {}\"\"\"\n","\n","# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","# outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n","# tokenizer.batch_decode(outputs)\n","\n","\n","# %%time\n","# # Text Streaming goes into a loop and doesnt adher to EOS\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)\n","\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","import os\n","\n","# Create timestamp\n","timestamp = int(time.time())\n","\n","# Create directory if it doesn't exist\n","save_dir = f\"/root/quantumLeap/models/qLeap_model_v0_{timestamp}\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# Save functions with explicit paths\n","def save_model_versions(model, tokenizer, timestamp, token):\n","    \"\"\"\n","    Save model in different formats with proper error handling\n","    \"\"\"\n","    try:\n","        # Save base model locally\n","        print(\"Saving base model locally...\")\n","        # model.save_pretrained(f\"{save_dir}/base\")\n","        # tokenizer.save_pretrained(f\"{save_dir}/base\")\n","        \n","        # Save 8-bit Q8_0 version\n","        print(\"Saving 8-bit Q8_0 version...\")\n","        try:\n","            model.push_to_hub_gguf(\n","                f\"olabs-ai/qLeap_model_v0_8bit_Q8_{timestamp}\",\n","                tokenizer,\n","                token=token,\n","                quantization_method=\"q8_0\"\n","            )\n","            print(\"Successfully saved 8-bit model\")\n","        except Exception as e:\n","            print(f\"Error saving 8-bit model: {str(e)}\")\n","            \n","        # Optional: Save 16-bit version\n","        print(\"Saving 16-bit version...\")\n","        try:\n","            model.push_to_hub_gguf(\n","                f\"olabs-ai/qLeap_model_v0_16bit_GGUF_{timestamp}\",\n","                tokenizer,\n","                quantization_method=\"f16\",\n","                token=token\n","            )\n","            print(\"Successfully saved 16-bit model\")\n","        except Exception as e:\n","            print(f\"Error saving 16-bit model: {str(e)}\")\n","            \n","    except Exception as e:\n","        print(f\"Error in save process: {str(e)}\")\n","        raise\n","\n","# Call the save function\n","huggingface_token = \"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\"\n","save_model_versions(model, tokenizer, timestamp, huggingface_token)"]},{"cell_type":"markdown","metadata":{},"source":["### if the loss from earlier training is too high try training arguments from unsloth colab notebook \"Llama-3.1 8b + Unsloth 2x faster finetuning.ipynb\". URL below\n","### https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=95_Nn-89DhsL"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset creation based on the book itself using AugmenToolkit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Instruction  Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n","\n","import json\n","import ast\n","import logging\n","\n","import csv\n","\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n","    reader = csv.DictReader(f)\n","    data = list(reader)\n","    \n","type(data)\n","\n","\n","# Configure logging\n","logging.basicConfig(\n","    filename='transformation_errors.log',\n","    filemode='w',\n","    level=logging.ERROR,\n","    format='%(levelname)s:%(message)s'\n",")\n","\n","# Sample original data\n","original_data = data\n","\n","def transform_data(original_data):\n","    \"\"\"\n","    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n","\n","    Parameters:\n","        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n","\n","    Returns:\n","        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n","    \"\"\"\n","    new_data = []\n","\n","    for idx, entry in enumerate(original_data, start=1):\n","        concept_name = entry.get('concept_name', '').strip()\n","        detailed_explanation = entry.get('detailed_explanation', '').strip()\n","        example_scenario_str = entry.get('example_scenario', '').strip()\n","\n","        if not concept_name or not detailed_explanation or not example_scenario_str:\n","            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n","            continue\n","\n","        # Attempt to parse with json.loads\n","        try:\n","            example_scenarios = json.loads(example_scenario_str)\n","            if not isinstance(example_scenarios, list):\n","                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","        except json.JSONDecodeError:\n","            # Fallback to ast.literal_eval\n","            try:\n","                example_scenarios = ast.literal_eval(example_scenario_str)\n","                if not isinstance(example_scenarios, list):\n","                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","            except (ValueError, SyntaxError) as e:\n","                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n","                continue\n","\n","        # Iterate through each scenario and create a new entry\n","        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n","            if not isinstance(scenario, str):\n","                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n","                continue\n","\n","            new_entry = {\n","                'concept_name': concept_name,\n","                'detailed_explanation': detailed_explanation,\n","                'example_scenario': scenario.strip()\n","            }\n","            new_data.append(new_entry)\n","\n","    return new_data\n","\n","# Transform the data\n","transformed_data = transform_data(original_data)\n","\n","# Optional: Save the transformed data to a JSON file\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n","    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n","\n","print(f\"Transformation complete. {len(transformed_data)} entries created.\")\n","print(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n","\n","print(len(transformed_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","\n","EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","\n","def instruction_prompt_func(examples):\n","    concept_name = examples[\"concept_name\"]\n","    detailed_explanation = examples[\"detailed_explanation\"]\n","    example_scenario = examples[\"example_scenario\"]\n","    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\n","pass\n","\n","\n","# convert transformed_data to a huggingface dataset\n","instruction_dataset = Dataset.from_dict(transformed_data)\n","instruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n","\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = instruction_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 8,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 8,\n","\n","        # Use num_train_epochs and warmup_ratio for longer runs!\n","        max_steps = 120,\n","        warmup_steps = 10,\n","        # warmup_ratio = 0.1,\n","        # num_train_epochs = 1,\n","\n","        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n","        learning_rate = 5e-5,\n","        embedding_learning_rate = 1e-5,\n","\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.00,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")\n","trainer_stats = trainer.train()\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# add current timestamp to model name\n","model.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\n","tokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\n","model.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","tokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","        \n","# Save to 8bit GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_GGUF_{int(time.time())}\", tokenizer,quantization_method = \"q8_0\", token = \"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\")\n","\n","# Save to 16bit GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n","\n","# Save to q4_k_m GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")\n","\n","\n","\n","# # Merge to 16bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n","\n","# # Merge to 4bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n","\n","# # Just LoRA adapters\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
