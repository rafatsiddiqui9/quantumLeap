{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "from datasets import Dataset\n",
    "from transformers import TextStreamer\n",
    "from unsloth import (\n",
    "    FastLanguageModel,\n",
    "    UnslothTrainer,\n",
    "    UnslothTrainingArguments,\n",
    "    is_bfloat16_supported\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='transformation_errors.log',\n",
    "    filemode='w',\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Define paths\n",
    "INPUT_CSV_PATH = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv'\n",
    "OUTPUT_JSON_PATH = '/root/qLeap-fft/data/input/Instruction_Data/transformed_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data(input_csv_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Read and validate the input CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(input_csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            return list(reader)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def transform_data(original_data: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Transform the original data by expanding example scenarios.\"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    for idx, entry in enumerate(original_data, start=1):\n",
    "        concept_name = entry.get('concept_name', '').strip()\n",
    "        detailed_explanation = entry.get('detailed_explanation', '').strip()\n",
    "        example_scenario_str = entry.get('example_scenario', '').strip()\n",
    "\n",
    "        if not all([concept_name, detailed_explanation, example_scenario_str]):\n",
    "            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            example_scenarios = json.loads(example_scenario_str)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                example_scenarios = ast.literal_eval(example_scenario_str)\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}') has invalid example_scenario: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not isinstance(example_scenarios, list):\n",
    "            logging.error(f\"Entry {idx} ('{concept_name}'): example_scenario is not a list\")\n",
    "            continue\n",
    "\n",
    "        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n",
    "            if not isinstance(scenario, str):\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}'): non-string scenario at position {scenario_idx}\")\n",
    "                continue\n",
    "\n",
    "            new_data.append({\n",
    "                'concept_name': concept_name,\n",
    "                'detailed_explanation': detailed_explanation,\n",
    "                'example_scenario': scenario.strip()\n",
    "            })\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# Process and save the data\n",
    "original_data = read_csv_data(INPUT_CSV_PATH)\n",
    "transformed_data = transform_data(original_data)\n",
    "\n",
    "# Save transformed data\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)\n",
    "with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Processed {len(transformed_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instruction template\n",
    "instruction_template = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\"\"\"\n",
    "\n",
    "def create_instruction_dataset(transformed_data: List[Dict[str, str]]) -> Dataset:\n",
    "    \"\"\"Create an instruction dataset from transformed data.\"\"\"\n",
    "    def instruction_prompt_func(examples):\n",
    "        return {\n",
    "            \"text\": [\n",
    "                instruction_template.format(\n",
    "                    f\"Explain the concept of {cn} and provide an example.\",\n",
    "                    f\"{de}\\n\\nExample:\\n{es}\"\n",
    "                )\n",
    "                for cn, de, es in zip(\n",
    "                    examples[\"concept_name\"],\n",
    "                    examples[\"detailed_explanation\"],\n",
    "                    examples[\"example_scenario\"]\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    dataset = Dataset.from_list(transformed_data)\n",
    "    return dataset.map(instruction_prompt_func, batched=True)\n",
    "\n",
    "# Create the dataset\n",
    "instruction_dataset = create_instruction_dataset(transformed_data)\n",
    "\n",
    "# Print a sample to verify\n",
    "print(\"\\nSample processed example:\")\n",
    "print(instruction_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization parameters\n",
    "max_seq_length = 1024\n",
    "model_name = \"lora_model_pum\"\n",
    "load_in_4bit = True\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")\n",
    "\n",
    "# Check for special tokens\n",
    "special_tokens = [\n",
    "    \"<|start_header_id|>\",\n",
    "    \"<|end_header_id|>\",\n",
    "    \"<|eot_id|>\",\n",
    "    \"system\",\n",
    "    \"user\",\n",
    "    \"assistant\"\n",
    "]\n",
    "\n",
    "for token in special_tokens:\n",
    "    if token not in tokenizer.get_vocab():\n",
    "        print(f\"Warning: {token} not in vocabulary!\")\n",
    "\n",
    "# Configure model\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.config.torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(model, tokenizer, dataset, \n",
    "                  batch_size=2, gradient_accumulation=8, max_steps=120):\n",
    "    \"\"\"Setup the training configuration.\"\"\"\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    import wandb\n",
    "\n",
    "    # Define your parameters\n",
    "    batchSize = 2\n",
    "    ga = 8\n",
    "    maxSteps = 120\n",
    "    lRate = 5e-5\n",
    "    embLRate = 1e-5\n",
    "    optim = \"adamw_8bit\"\n",
    "    lrSchedule = \"linear\"\n",
    "\n",
    "    # Assume these variables are defined elsewhere in your code\n",
    "    base_model_slug = \"your_base_model_slug\"  # Replace with your actual value\n",
    "    max_seq_length = 512  # Replace with your actual value\n",
    "\n",
    "    # Get the current date and time in Indian Standard Time (IST)\n",
    "    ist = pytz.timezone('Asia/Kolkata')\n",
    "    current_datetime = datetime.now(ist)\n",
    "\n",
    "    # Format the datetime string\n",
    "    # Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create the run name with the current date and time\n",
    "    run_name = f\"\"\"Unsloth-CPT-Base-{formatted_datetime}-28Octv1-{base_model_slug}-{max_seq_length}max_seq_length-{batchSize}batchSize-{ga}ga-{maxSteps}maxSteps-{lRate}lRate-{embLRate}embLRate-{optim}optim-{lrSchedule}lrSchedule\"\"\"\n",
    "\n",
    "    # Initialize Weights & Biases\n",
    "    # It's recommended to set your W&B API key as an environment variable for security.\n",
    "    # Example: export WANDB_API_KEY=\"your_api_key\"\n",
    "    wandb.login(key=\"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\")  # Consider using environment variables for security\n",
    "    wandb.init(project=\"Unsloth-CPT\", name=run_name)\n",
    "    \n",
    "    training_args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        max_steps=max_steps,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        embedding_learning_rate=1e-5,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=[\"tensorboard\", \"wandb\"],\n",
    "        logging_dir=\"./tel-fft-logs\"\n",
    "    )\n",
    "\n",
    "    return UnslothTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        args=training_args\n",
    "    )\n",
    "\n",
    "# Setup trainer\n",
    "trainer = setup_training(model, tokenizer, instruction_dataset)\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model_pum_instruct\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model_pum_instruct\")\n",
    "\n",
    "!huggingface-cli login --token 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 --add-to-git-credential\n",
    "if True:\n",
    "    model.push_to_hub(\"olabs-ai/qLeap_v07_instruct\", token = \"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\") # Online saving\n",
    "    tokenizer.push_to_hub(\"olabs-ai/qLeap_v07_instruct\", token = \"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\") # Online saving\n",
    "    model.push_to_hub_gguf(\"olabs-ai/qLeap_v07_instruct\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model initialization parameters\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Enable faster inference\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model_pum\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Instruction prompt matching the fine-tuning template\n",
    "instruction_prompt = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Explain the concept of {} and provide an example.<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Set model dtype\n",
    "model.config.torch_dtype = torch.bfloat16\n",
    "\n",
    "# Example usage\n",
    "concept_name = \"Semiotics\"\n",
    "\n",
    "# Format input\n",
    "inputs = tokenizer(\n",
    "    [instruction_prompt.format(concept_name)],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate output with modified parameters\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.get_vocab().get(\"<|eot_id|>\", tokenizer.eos_token_id),  # Use <|eot_id|> if available\n",
    "    min_length=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Optional: Print the full response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanation(model, tokenizer, concept_name: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"Generate explanation for a given concept.\"\"\"\n",
    "    prompt = instruction_template.format(\n",
    "        f\"Explain the concept of {concept_name} and provide an example.\",\n",
    "        \"\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.get_vocab().get(\"<|eot_id|>\", tokenizer.eos_token_id),\n",
    "        min_length=50,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate example\n",
    "concept = \"Defense Mechanisms\"\n",
    "response = generate_explanation(model, tokenizer, concept)\n",
    "print(f\"\\nGenerated explanation for {concept}:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
