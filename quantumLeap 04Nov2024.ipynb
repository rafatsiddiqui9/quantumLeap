{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Git clone qLeap-fft repo inside `/kaggle/working/` directory\n","## Ensure to have the latest branch\n","## Switch to quantumLeap directory"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Current directory: /root/quantumLeap\n","Successfully switched to: /kaggle/working/quantumLeap\n","Working directory: /kaggle/working/quantumLeap\n"]}],"source":["import os\n","from pathlib import Path\n","\n","def ensure_working_directory():\n","    \"\"\"\n","    Check if we're in the correct working directory, if not switch to it.\n","    Creates the directory if it doesn't exist.\n","    \"\"\"\n","    target_dir = '/kaggle/working/quantumLeap'\n","    current_dir = os.getcwd()\n","    \n","    # Print current directory\n","    print(f\"Current directory: {current_dir}\")\n","    \n","    # Check if we need to switch directories\n","    if current_dir != target_dir:\n","        # Create directory if it doesn't exist\n","        Path(target_dir).mkdir(parents=True, exist_ok=True)\n","        \n","        try:\n","            # Change to target directory\n","            os.chdir(target_dir)\n","            print(f\"Successfully switched to: {target_dir}\")\n","        except Exception as e:\n","            print(f\"Error switching to directory: {str(e)}\")\n","            raise\n","    else:\n","        print(\"Already in correct directory\")\n","    \n","    # Verify current directory\n","    print(f\"Working directory: {os.getcwd()}\")\n","\n","# Call the function before your main code\n","ensure_working_directory()"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["# # ----------------------------- #\n","# # Part 1.1: Install and Setup Libraries\n","# # ----------------------------- #\n","\n","# # !python -m xformers.info\n","# import torch\n","# print(f\"PyTorch version: {torch.__version__}\")\n","# print(f\"CUDA available: {torch.cuda.is_available()}\")\n","# print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n","# %pip install uv\n","# !uv pip install -q unsloth wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets --system\n","# # Finally, install a compatible xformers version\n","# !uv pip install -q xformers==0.0.27 --system  # This version is compatible with PyTorch 2.4.x\n","# !python -m xformers.info# # !python -m xformers.info"]},{"cell_type":"markdown","metadata":{},"source":["## Restart the session and check if unsloth is installed"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["# import unsloth"]},{"cell_type":"markdown","metadata":{},"source":["## Install Flash Attention and check version"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["# !uv pip install flash_attn --no-build-isolation -q --system\n","# !pip show flash_attn"]},{"cell_type":"markdown","metadata":{},"source":["# Restart again so that all the libraries are properly initialized"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","punkt was already available.\n","en_core_web_sm was already available.\n"]}],"source":["# ----------------------------- #\n","# Part 1.2: Import Necessary Libraries\n","# ----------------------------- #\n","\n","# General Libraries\n","import os\n","import json\n","import sys\n","import subprocess\n","import argparse\n","import logging\n","import math\n","import random\n","from datetime import datetime\n","import re\n","import gc\n","import weakref\n","import multiprocessing\n","\n","# Torch related\n","import torch\n","from torch import nn\n","import torch.distributed as dist\n","\n","# Transformers related\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForLanguageModeling,\n","    Adafactor\n",")\n","\n","# Huggingface TRL for full finetune\n","from trl import SFTTrainer, SFTConfig\n","\n","# General huggingface libraries\n","import huggingface_hub\n","from datasets import load_dataset, Dataset\n","from accelerate import Accelerator\n","\n","\n","# Unsloth specificic libraries\n","import unsloth\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n","\n","# Other Libraries\n","from peft import LoraConfig\n","import wandb\n","import nltk\n","import spacy\n","# from galore_torch import GaLoreAdamW, GaLoreAdafactor, GaLoreAdamW8bit\n","\n","# Check and import NLTK and spacy modules\n","# Ensure NLTK's punkt tokenizer is available\n","import nltk\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","    print('punkt was already available.')\n","except LookupError:\n","    nltk.download('punkt')\n","    print('punkt was not available. It has been downloaded')\n","\n","# Initialize spaCy English model\n","try:\n","    nlp = spacy.load('en_core_web_sm')\n","    print('en_core_web_sm was already available.')\n","except OSError:\n","    print(\"SpaCy English model not found. Downloading...\")\n","    os.system('python -m spacy download en_core_web_sm')\n","    nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 2: Configure Environment Variables & Create Main Variables\n","# ----------------------------- #\n","\n","# max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n","# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","# base_model_slug = \"meta-llama/Llama-3.2-1B-Instruct\"\n","# base_model_name = \"lora_model_pum\"\n","# chunks_max_length = max_seq_length\n","# overlap_size = 1\n","# # Define your parameters\n","# batchSize = 2\n","# ga = 8\n","# maxSteps = 120\n","# lRate = 5e-5\n","# embLRate = 1e-5\n","# optim = \"adamw_8bit\"\n","# lrSchedule = \"linear\"\n","# dataset_slug = \"psychology_of_unconscious\"\n","\n","# # Set environment variables before importing torch-related modules\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Replace 'psychology_of_unconscious.txt' with your actual file path\u001b[39;00m\n\u001b[1;32m     20\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 21\u001b[0m clean_text \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mload_and_clean_text\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_clean_text\u001b[39m(file_path):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Loads text from a file and removes Project Gutenberg's license and headers/footers.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# # Remove Project Gutenberg's license text and headers/footers\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\u001b[39;00m\n","File \u001b[0;32m~/miniconda/envs/ql/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'"]}],"source":["# ----------------------------- #\n","# Part 2: Load and Clean the Text Data\n","# ----------------------------- #\n","\n","def load_and_clean_text(file_path):\n","    \"\"\"\n","    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n","    \"\"\"\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    # # Remove Project Gutenberg's license text and headers/footers\n","    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","\n","    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n","    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n","    return text.strip()\n","\n","# Replace 'psychology_of_unconscious.txt' with your actual file path\n","file_path = '/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'\n","clean_text = load_and_clean_text(file_path)"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["# # ----------------------------- #\n","# # Part 3: Parse Text into Discourse Units\n","# # ----------------------------- #\n","\n","def parse_discourse_units(text, overwrite=False):\n","    \"\"\"\n","    Parses text into discourse units using spaCy.\n","    Currently splits text into sentences.\n","    \"\"\"\n","    paragraphs = text.split('\\n\\n')\n","    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n","\n","    discourse_units = []\n","    for para in paragraphs:\n","        doc = nlp(para)\n","        sentences = [sent.text for sent in doc.sents]\n","        discourse_units.extend(sentences)\n","\n","    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_discourse_units.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Discourse Units: {len(discourse_units)}\")\n","    return discourse_units"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 4: Create Chunks Using Hybrid Strategy\n","# ----------------------------- #\n","\n","def create_chunks(discourse_units, tokenizer, max_length=4096, overlap_size=1, overwrite=False):\n","    \"\"\"\n","    Creates chunks from discourse units using a sliding window with overlapping chunks.\n","    Optimized to work directly with token IDs and utilize efficient list operations.\n","    \"\"\"\n","    chunks = []\n","    current_chunk_tokens = []\n","    current_length = 0\n","\n","    for unit in discourse_units:\n","        unit_tokens = tokenizer.encode(unit, add_special_tokens=True)\n","        unit_length = len(unit_tokens)\n","\n","        if current_length + unit_length <= max_length:\n","            current_chunk_tokens.extend(unit_tokens)\n","            current_length += unit_length\n","        else:\n","            # Decode and append the current chunk\n","            chunk_text = tokenizer.decode(\n","                current_chunk_tokens, skip_special_tokens=True)\n","            chunks.append(chunk_text)\n","\n","            # Prepare overlap tokens\n","            overlap_tokens = current_chunk_tokens[-overlap_size:]\n","            current_chunk_tokens = overlap_tokens + unit_tokens\n","            current_length = len(current_chunk_tokens)\n","\n","    # Append any remaining tokens as the last chunk\n","    if current_chunk_tokens:\n","        chunk_text = tokenizer.decode(\n","            current_chunk_tokens, skip_special_tokens=True)\n","        chunks.append(chunk_text)\n","\n","    # Write or read chunks as before\n","    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_chunks.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Chunks Created: {len(chunks)}\")\n","    return chunks"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 5: Create and Tokenize Dataset\n","# ----------------------------- #\n","\n","# To Do - make book titles and prompt generic so\n","def create_tokenized_dataset(chunks, tokenizer, max_length=1024, num_proc=2):\n","\n","    # Create a Dataset object from chunks\n","\n","    book_title = 'Psychology of the Unconscious by C. G. Jung'\n","    wikipedia_prompt = \"\"\"\n","    Psychology Book\n","\n","    ### Title: {}\n","\n","    ### Article: {}\n","    \"\"\"\n","\n","    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n","\n","    def formatting_prompts_func(examples):\n","        titles = book_title\n","        texts = examples[\"text\"]\n","        outputs = []\n","        for title, text in zip([book_title]*len(chunks), texts):\n","            text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n","            outputs.append(text)\n","        return {\"text\": outputs, }\n","    pass\n","\n","    # convert chunks variable to huggingface dataset\n","\n","    from datasets import Dataset\n","\n","    dataset = Dataset.from_dict({\"text\": chunks})\n","\n","    dataset = dataset.map(formatting_prompts_func,\n","                          batched=True, num_proc=num_proc)\n","    # Split the dataset into training and validation sets\n","    split = dataset.train_test_split(test_size=0.1, seed=42)\n","    train_dataset = split['train']\n","    eval_dataset = split['test']\n","\n","    print(len(dataset))\n","    # Find the maximum length of the text field in the entire dataset\n","    max_length = max(len(text) for text in dataset['text'])\n","    print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","#     print(f\"First 5 rows of training dataset: {train_dataset[:5]}\")\n","    print(f\"Validation Dataset Size: {len(eval_dataset)}\")\n","#     print(f\"First 5 rows of validation dataset: {eval_dataset[:5]}\")\n","    return train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 6: Set up environment and other important utilities\n","# ----------------------------- #\n","\n","def setup_environment():\n","    \"\"\"\n","    Initializes the Accelerator for distributed training.\n","    \"\"\"\n","    return Accelerator()\n","\n","\n","def get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n","    \"\"\"\n","    Defines a custom learning rate scheduler with warmup, constant, and linear annealing phases.\n","    \"\"\"\n","    def lr_lambda(current_step):\n","        if current_step < num_warmup_steps:\n","            return current_step / num_warmup_steps  # Linear warmup\n","        elif current_step < initial_phase_steps:\n","            return 1.0  # Constant learning rate for initial phase\n","        else:\n","            # Linear annealing for the remaining steps\n","            return 1.0 - ((current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps))\n","\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","\n","def setup_training_steps(initial_rows, annealing_rows, batch_size, gradient_accumulation_steps, num_epochs):\n","    \"\"\"\n","    Calculates total and initial training steps based on dataset size and training parameters.\n","    \"\"\"\n","    total_rows = initial_rows + annealing_rows\n","    total_steps = (total_rows * num_epochs) // (batch_size *\n","                                                gradient_accumulation_steps)\n","    initial_steps = (initial_rows * num_epochs) // (batch_size *\n","                                                    gradient_accumulation_steps)\n","    return max(1, total_steps), max(1, initial_steps)\n","\n","\n","def print_memory_usage(step_desc):\n","    \"\"\"\n","    Prints the CUDA memory summary if CUDA is available.\n","    \"\"\"\n","    if torch.cuda.is_available():\n","        print(f\"Memory Usage at {step_desc}:\")\n","        print(torch.cuda.memory_summary())\n","        print(\"\\n\")\n","    else:\n","        print(f\"No CUDA available at {step_desc}.\\n\")\n","\n","\n","def inference(model, tokenizer):\n","    \"\"\"\n","    Runs inference using the trained model.\n","    \"\"\"\n","    # Define sample prompts\n","    prompts = [\n","        \"Explain what is medical anthropology and its importance in elevating the quality of life?\",\n","        \"Explain what are the types of Jungian archetypes and how they manifest in the human psyche?\"\n","    ]\n","\n","    for prompt in prompts:\n","        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, max_length=256)\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n","        \n","#  Login to Huggingface\n","from huggingface_hub import login\n","import os\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","def setup_huggingface_access():\n","    \"\"\"Setup Hugging Face access with token from environment or manual input\"\"\"\n","    # First try to get token from environment variable\n","    token = os.getenv('HUGGINGFACE_TOKEN')\n","    \n","    if not token:\n","        # If not in environment, prompt for token\n","        token = input(\"Enter your Hugging Face token: \")\n","        \n","    if token:\n","        try:\n","            login(token, add_to_git_credential=True)\n","            print(\"Successfully logged in to Hugging Face!\")\n","        except Exception as e:\n","            print(f\"Failed to log in to Hugging Face: {str(e)}\")\n","            return False\n","    else:\n","        print(\"No Hugging Face token provided\")\n","        return False\n","    \n","    return True"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 5: Load the Tokenizer and Model\n","# ----------------------------- #\n","\n","def load_model_and_tokenizer(base_model_slug, max_seq_length=4096, dtype=None, load_in_4bit=True, device_map = \"auto\"):\n","    \"\"\"\n","    Load and configure the model and tokenizer with specified parameters.\n","    \n","    Args:\n","        base_model_slug (str): The model identifier from HuggingFace\n","        max_seq_length (int): Maximum sequence length for the model\n","        dtype: Data type for model parameters\n","        load_in_4bit (bool): Whether to use 4-bit quantization\n","        \n","    Returns:\n","        tuple: (model, tokenizer)\n","    \"\"\"\n","    # Check CUDA is available\n","    import torch\n","    if not torch.cuda.is_available():\n","        print(\"WARNING: CUDA is not available. This might affect performance.\")\n","    else:\n","        print(\"CUDA available\")\n","        \n","    # Check available GPUs\n","    num_gpus = torch.cuda.device_count()\n","    print(f\"Number of available GPUs: {num_gpus}\")\n","    for i in range(num_gpus):\n","        print(f\"GPU {i}: {torch.cuda.get_device_properties(i).name}\")\n","\n","    # Determine optimal device map based on available GPUs\n","    device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"Using single device: {device_map}\")\n","\n","    # Model paths\n","    model_name = base_model_slug\n","    models_dir = os.path.join(os.path.dirname(\"~/\"), \"models\")\n","    model_path = os.path.join(models_dir, model_name)\n","\n","    # Create models directory if it doesn't exist\n","    if not os.path.exists(models_dir):\n","        os.makedirs(models_dir)\n","\n","    # Load or download model\n","    try:\n","        if os.path.exists(model_path):\n","            print(f\"Loading model from local path: {model_path}\")\n","            model, tokenizer = FastLanguageModel.from_pretrained(\n","                model_name=base_model_slug,\n","                max_seq_length=max_seq_length,\n","                dtype=dtype,\n","                load_in_4bit=load_in_4bit,\n","                token=os.getenv('HUGGINGFACE_TOKEN'),\n","            )\n","        else:\n","            print(f\"Downloading model from HuggingFace: {model_name}\")\n","            model, tokenizer = FastLanguageModel.from_pretrained(\n","                model_name=base_model_slug,\n","                max_seq_length=max_seq_length,\n","                dtype=dtype,\n","                load_in_4bit=load_in_4bit,\n","                token=os.getenv('HUGGINGFACE_TOKEN'),\n","            )\n","    except Exception as e:\n","        print(f\"Error loading model: {str(e)}\")\n","        raise\n","\n","    # Configure PEFT model\n","    model = FastLanguageModel.get_peft_model(\n","        model,\n","        r=128,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","        target_modules=[\n","            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","            \"gate_proj\", \"up_proj\", \"down_proj\",\n","            \"embed_tokens\", \"lm_head\",  # Add for continual pretraining\n","        ],\n","        lora_alpha=32,\n","        lora_dropout=0,  # Supports any, but = 0 is optimized\n","        bias=\"none\",    # Supports any, but = \"none\" is optimized\n","        use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n","        random_state=3407,\n","        use_rslora=True,   # We support rank stabilized LoRA\n","        loftq_config=None,  # And LoftQ\n","    )\n","\n","    print(\"Model and tokenizer loaded successfully!\")\n","    return model, tokenizer\n","\n","    # Print model device distribution\n","    if hasattr(model, 'hf_device_map'):\n","        print(\"\\nModel Device Distribution:\")\n","        for name, device in model.hf_device_map.items():\n","            print(f\"{name}: {device}\")\n","\n","    # Print memory usage per GPU\n","    print(\"\\nGPU Memory Usage after model loading:\")\n","    for i in range(num_gpus):\n","        memory_used = torch.cuda.memory_allocated(i) / (1024**3)\n","        memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n","        print(f\"GPU {i}: {memory_used:.2f}GB / {memory_total:.2f}GB\")"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["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\n","1ca3c5e9222c2504acbc07cf7f88267006ae68c4\n"]}],"source":["import os\n","\n","# Set the environment variable\n","os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n","os.environ[\"WANDB_API_KEY\"] = \"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\"\n","# Verify it's set correctly\n","print(os.getenv(\"HUGGINGFACE_TOKEN\"))\n","print(os.getenv(\"WANDB_API_KEY\"))"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molabs-asia\u001b[0m (\u001b[33molabs-asia-olabs-pro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.18.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/quantumLeap/wandb/run-20241105_045629-c84nqgyh</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/olabs-asia-olabs-pro/Trellis-FFT-v2/runs/c84nqgyh' target=\"_blank\">Kaggle-quantumLeap-20241105_102628-meta-llama/Llama-3.1-8B-Instruct-psychology_of_unconscious-4096_maxSeqLength-4096_maxLength-2_batchSize-16_ga-120_maxSteps-10_warmupSteps-1_numTrainEpochs-5e-05_lRate-1e-05_embLRate-adamw_8bit_optim-linear_lrSchedule</a></strong> to <a href='https://wandb.ai/olabs-asia-olabs-pro/Trellis-FFT-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/olabs-asia-olabs-pro/Trellis-FFT-v2' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/Trellis-FFT-v2</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/olabs-asia-olabs-pro/Trellis-FFT-v2/runs/c84nqgyh' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/Trellis-FFT-v2/runs/c84nqgyh</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# ----------------------------- #\n","# Part 2: Configure Environment Variables & Create Main Variables\n","# ----------------------------- #\n","\n","# Unsloth modell initialization variables\n","max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n","max_length = max_seq_length\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","base_model_slug = \"meta-llama/Llama-3.1-8B-Instruct\"\n","base_model_name = \"lora_model_pum\"\n","chunks_max_length = max_seq_length\n","overlap_size = 1\n","# Define your parameters\n","batchSize = 2\n","ga = 16\n","maxSteps = 120\n","warmupSteps = 10\n","numTrainEpochs = 1\n","lRate = 5e-5\n","embLRate = 1e-5\n","optim = \"adamw_8bit\"\n","lrSchedule = \"linear\"\n","dataset_slug = \"psychology_of_unconscious\"\n","\n","from datetime import datetime\n","import pytz\n","import wandb\n","# Get the current date and time in Indian Standard Time (IST)\n","ist = pytz.timezone('Asia/Kolkata')\n","current_datetime = datetime.now(ist)\n","\n","# Format the datetime string\n","# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n","formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Define Run Name\n","run_name = f\"\"\"Kaggle-quantumLeap-{formatted_datetime}-{base_model_slug}-{dataset_slug}-{max_seq_length}_maxSeqLength-{max_length}_maxLength-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{warmupSteps}_warmupSteps-{numTrainEpochs}_numTrainEpochs-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n","\n","# Initialize Weights & Biases\n","# It's recommended to set your W&B API key as an environment variable for security.\n","wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n","wandb.init(project=\"Trellis-FFT-v2\", name=run_name)\n","\n","# Set environment variables before importing torch-related modules\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of CPU cores: 256\n","Number of processes: 254\n","Successfully logged in to Hugging Face!\n","CUDA available\n","Number of available GPUs: 1\n","GPU 0: NVIDIA A100-SXM4-40GB\n","Using single device: cuda:0\n","Downloading model from HuggingFace: meta-llama/Llama-3.1-8B-Instruct\n","==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 8.0. CUDA Toolkit = 12.4.\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5189f9e47c7f40d780c6117457f5b875","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e61e9002a66e414788de790cedd2081c","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"818c0215d81345278a869f9ed1a96922","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0b8423b34da403584f96d0b23973218","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c2fdbe7d4864ffcbd172d1cb0a49c4a","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32787602c76b4e18b0678f6a839f34a4","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b2c9836cdfd48878bb1533f8dbcd20a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"949fa02dab924c83a6b90a741a9260c3","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6dcc869a93424e6c9af320f37f629043","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa549dbcc3ca40d69b7c4e15370ca67c","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1cdb0b201bb74d4e8a01e2c58d942666","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["meta-llama/Llama-3.1-8B-Instruct does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n","Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n","Please update transformers, TRL and unsloth via:\n","`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"]},{"name":"stdout","output_type":"stream","text":["Unsloth: Offloading input_embeddings to disk to save VRAM\n"]},{"name":"stderr","output_type":"stream","text":["/root/miniconda/envs/ql/lib/python3.11/site-packages/unsloth/models/_utils.py:902: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"]},{"name":"stdout","output_type":"stream","text":["Unsloth: Offloading output_embeddings to disk to save VRAM\n"]},{"name":"stderr","output_type":"stream","text":["Unsloth 2024.10.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]},{"name":"stdout","output_type":"stream","text":["Unsloth: Casting embed_tokens to float32\n","Unsloth: Casting lm_head to float32\n","Model and tokenizer loaded successfully!\n","Model Device: cpu\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Load and Clean Text Data\u001b[39;00m\n\u001b[1;32m     23\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m clean_text \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Parse Discourse Units\u001b[39;00m\n\u001b[1;32m     27\u001b[0m discourse_units \u001b[38;5;241m=\u001b[39m parse_discourse_units(clean_text, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mload_and_clean_text\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_clean_text\u001b[39m(file_path):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Loads text from a file and removes Project Gutenberg's license and headers/footers.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# # Remove Project Gutenberg's license text and headers/footers\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\u001b[39;00m\n","File \u001b[0;32m~/miniconda/envs/ql/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'"]}],"source":["\n","# ----------------------------- #\n","# Part 9: Data Processing\n","# ----------------------------- #\n","\n","# # Perform Inference Before Training\n","# inference(model, tokenizer)\n","\n","# Set number of processes to use for data loading\n","num_cpus = multiprocessing.cpu_count()\n","num_proc = max(num_cpus-2, 2)  # Adjust based on prior recommendations\n","print(f\"Number of CPU cores: {num_cpus}\")\n","print(f\"Number of processes: {num_proc}\")\n","\n","# Login to Hugging Face\n","if not setup_huggingface_access():\n","    raise Exception(\"Failed to setup Hugging Face access. Please check your token.\")\n","\n","# Load Model and Tokenizer\n","model, tokenizer = load_model_and_tokenizer(base_model_slug)\n","print(f\"Model Device: {model.device}\")\n","\n","# Load and Clean Text Data\n","file_path = \"/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","clean_text = load_and_clean_text(file_path)\n","\n","# Parse Discourse Units\n","discourse_units = parse_discourse_units(clean_text, overwrite=True)\n","\n","# Create Chunks\n","chunks = create_chunks(\n","    discourse_units,\n","    tokenizer,\n","    max_length=max_length,\n","    overlap_size=overlap_size,\n","    overwrite=True,\n",")\n","\n","# Create Tokenized Dataset\n","train_dataset, eval_dataset = create_tokenized_dataset(\n","    chunks, tokenizer, max_length)\n","\n","# Save datasets as Hugging Face `datasets`\n","train_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","eval_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')\n","\n","### To Do - Make the below as dynamic and as a functio\n","# # Uncomment following if you want to just load the data from temp directory\n","# from datasets import load_from_disk\n","\n","# train_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","# eval_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 8: Configure Training Arguments\n","# ----------------------------- #\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_dataset,\n","    eval_dataset = eval_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = batchSize,\n","        gradient_accumulation_steps = ga,\n","\n","        # Use warmup_ratio and num_train_epochs for longer runs!\n","        max_steps = maxSteps,\n","        warmup_steps = warmupSteps,\n","        # warmup_ratio = 0.1,\n","        num_train_epochs = numTrainEpochs,\n","\n","        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n","        learning_rate = lRate,\n","        embedding_learning_rate = embLRate,\n","\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = optim,\n","        weight_decay = 0.01,\n","        lr_scheduler_type = lrSchedule,\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","\n","        report_to=[\"tensorboard\", \"wandb\"],\n","        logging_dir=f\"./trel-fft-logs/{run_name}\",\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 11: Start Training\n","# ----------------------------- #\n","\n","#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","trainer_stats = trainer.train()\n","\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","\n","# add current timestamp to model name\n","model.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\n","tokenizer.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\")\n","model.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_base_pum_{int(time.time())}\", token = \"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\") # Online saving\n","tokenizer.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_pum_{int(time.time())}\", token = \"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\") # Online saving\n","\n","# # Merge to 16bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n","\n","# Merge to 4bit\n","if True: model.save_pretrained_merged(\"olabs-ai/qLeap_model_base_v0_pum_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n","if True: model.push_to_hub_merged(\"olabs-ai/qLeap_model_base_v0_pum_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n","\n","# # Just LoRA adapters\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n","    \n","    \n","# # Save to 8bit Q8_0\n","if True: model.save_pretrained_gguf(\"olabs-ai/qLeap_model_base_v0_pum_8bit_Q8_{int(time.time())}\", tokenizer,)\n","if True: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_base_v0_pum_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n","\n","# # Save to 16bit GGUF\n","# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n","\n","# # Save to q4_k_m GGUF\n","# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import wandb\n","# from pprint import pprint\n","\n","# def get_run_config(project_name, run_id):\n","#     try:\n","#         # Initialize the wandb API\n","#         api = wandb.Api()\n","\n","#         # Access the specific run\n","#         run = api.run(f\"{project_name}/{run_id}\")\n","\n","#         # Get the full configuration\n","#         config = run.config\n","\n","#         # Filter for trainer-specific configuration\n","#         trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n","\n","#         return trainer_config\n","\n","#     except wandb.errors.CommError:\n","#         print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n","#         return None\n","#     except Exception as e:\n","#         print(f\"An error occurred: {str(e)}\")\n","#         return None\n","\n","# # Usage\n","# project_name = \"olabs-asia-olabs-pro/huggingface\"\n","# run_id = \"ppqtwwmy\"\n","\n","# trainer_config = get_run_config(project_name, run_id)\n","\n","# if trainer_config:\n","#     print(f\"Trainer configuration for run {run_id}:\")\n","#     pprint(trainer_config)"]},{"cell_type":"markdown","metadata":{},"source":["# The Loss from earlier training was too high. We shall use training arguments from unsloth colab notebook \"Llama-3.1 8b + Unsloth 2x faster finetuning.ipynb\". URL below\n","### https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=95_Nn-89DhsL"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import IntervalStrategy\n","import wandb\n","\n","# Initialize wandb\n","wandb.init(project=\"huggingface\", name=\"refined_training_run\")\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    eval_dataset = dataset.select(range(len(dataset) // 10)),  # Use 10% of data for evaluation\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 8,\n","\n","        # Set both max_steps and num_train_epochs\n","        max_steps = 120,\n","        num_train_epochs = 3,\n","\n","        # Use a single learning rate for all parameters\n","        learning_rate = 5e-5,\n","\n","        # Warmup strategy from successful runs\n","        warmup_steps = 10,\n","        warmup_ratio = 0,\n","\n","        # Explicitly set precision based on hardware support\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        \n","        logging_steps = 1,\n","        \n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        \n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        \n","        report_to = \"wandb\",  # Enable Weights & Biases logging\n","        \n","        # Set both save and evaluation strategies to 'steps'\n","        save_strategy = IntervalStrategy.STEPS,\n","        eval_strategy = IntervalStrategy.STEPS,\n","        save_steps = 1,  # Save checkpoint every 20 steps\n","        eval_steps = 1,  # Evaluate every 20 steps (matching save_steps)\n","        \n","        load_best_model_at_end = True,\n","        metric_for_best_model = \"eval_loss\",\n","    ),\n","    compute_metrics = compute_metrics,\n",")\n","\n","# ... (rest of the code remains the same)\n","\n","# Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","# Start training\n","trainer_stats = trainer.train()\n","\n","# Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory / max_memory * 100, 3)\n","lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","\n","# Generation code (unchanged)\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Hero Archetype\", # concept_name\n","        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","from transformers import StoppingCriteria, StoppingCriteriaList\n","\n","class EndOfTextCriteria(StoppingCriteria):\n","    def __init__(self, eos_token_id):\n","        self.eos_token_id = eos_token_id\n","\n","    def __call__(self, input_ids, scores, **kwargs):\n","        return input_ids[0][-1] == self.eos_token_id\n","\n","stopping_criteria = StoppingCriteriaList([EndOfTextCriteria(tokenizer.eos_token_id)])\n","\n","outputs = model.generate(**inputs, \n","                         max_new_tokens=64, \n","                         stopping_criteria=stopping_criteria,\n","                         use_cache=True)\n","\n","print(tokenizer.batch_decode(outputs))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # delete previous trainer\n","# del trainer\n","\n","#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        # num_train_epochs = 1, # Set this for 1 full training run.\n","        max_steps = 60,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")\n","\n","trainer_stats = trainer.train()\n","#@title Show final memory and time stats\n","\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","\n","\n","\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Hero Archetype\", # concept_name\n","        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import time\n","\n","# add current timestamp to model name\n","model.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\n","tokenizer.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\")\n","model.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_base_{int(time.time())}\", token = \"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\") # Online saving\n","tokenizer.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_{int(time.time())}\", token = \"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\") # Online saving\n","\n","# # Merge to 16bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n","\n","# # Merge to 4bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n","\n","# # Just LoRA adapters\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n","    \n","    \n","# # Save to 8bit Q8_0\n","# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n","\n","# # Save to 16bit GGUF\n","# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n","\n","# # Save to q4_k_m GGUF\n","# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Hero Archetype\", # concept_name\n","        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"markdown","metadata":{},"source":["# Instruction  Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n","\n","import json\n","import ast\n","import logging\n","\n","import csv\n","\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n","    reader = csv.DictReader(f)\n","    data = list(reader)\n","    \n","type(data)\n","\n","\n","# Configure logging\n","logging.basicConfig(\n","    filename='transformation_errors.log',\n","    filemode='w',\n","    level=logging.ERROR,\n","    format='%(levelname)s:%(message)s'\n",")\n","\n","# Sample original data\n","original_data = data\n","\n","def transform_data(original_data):\n","    \"\"\"\n","    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n","\n","    Parameters:\n","        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n","\n","    Returns:\n","        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n","    \"\"\"\n","    new_data = []\n","\n","    for idx, entry in enumerate(original_data, start=1):\n","        concept_name = entry.get('concept_name', '').strip()\n","        detailed_explanation = entry.get('detailed_explanation', '').strip()\n","        example_scenario_str = entry.get('example_scenario', '').strip()\n","\n","        if not concept_name or not detailed_explanation or not example_scenario_str:\n","            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n","            continue\n","\n","        # Attempt to parse with json.loads\n","        try:\n","            example_scenarios = json.loads(example_scenario_str)\n","            if not isinstance(example_scenarios, list):\n","                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","        except json.JSONDecodeError:\n","            # Fallback to ast.literal_eval\n","            try:\n","                example_scenarios = ast.literal_eval(example_scenario_str)\n","                if not isinstance(example_scenarios, list):\n","                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","            except (ValueError, SyntaxError) as e:\n","                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n","                continue\n","\n","        # Iterate through each scenario and create a new entry\n","        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n","            if not isinstance(scenario, str):\n","                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n","                continue\n","\n","            new_entry = {\n","                'concept_name': concept_name,\n","                'detailed_explanation': detailed_explanation,\n","                'example_scenario': scenario.strip()\n","            }\n","            new_data.append(new_entry)\n","\n","    return new_data\n","\n","# Transform the data\n","transformed_data = transform_data(original_data)\n","\n","# Optional: Save the transformed data to a JSON file\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n","    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n","\n","print(f\"Transformation complete. {len(transformed_data)} entries created.\")\n","print(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n","\n","print(len(transformed_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","\n","EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","\n","def instruction_prompt_func(examples):\n","    concept_name = examples[\"concept_name\"]\n","    detailed_explanation = examples[\"detailed_explanation\"]\n","    example_scenario = examples[\"example_scenario\"]\n","    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\n","pass\n","\n","\n","# convert transformed_data to a huggingface dataset\n","instruction_dataset = Dataset.from_dict(transformed_data)\n","instruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n","\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = instruction_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 8,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 8,\n","\n","        # Use num_train_epochs and warmup_ratio for longer runs!\n","        max_steps = 120,\n","        warmup_steps = 10,\n","        # warmup_ratio = 0.1,\n","        # num_train_epochs = 1,\n","\n","        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n","        learning_rate = 5e-5,\n","        embedding_learning_rate = 1e-5,\n","\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.00,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")\n","trainer_stats = trainer.train()\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# add current timestamp to model name\n","model.save_pretrained(f\"qLeap_model_v0_{int(time.time())}\") # Local saving\n","tokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\n","model.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","tokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","\n","# # Merge to 16bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n","\n","# # Merge to 4bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n","\n","# # Just LoRA adapters\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n","    \n","    \n","# # Save to 8bit Q8_0\n","# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n","\n","# # Save to 16bit GGUF\n","# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n","\n","# # Save to q4_k_m GGUF\n","# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n","# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5930536,"sourceId":9698777,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":4}
