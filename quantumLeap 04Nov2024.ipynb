{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9698777,"sourceType":"datasetVersion","datasetId":5930536}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # !python -m xformers.info\n# import torch\n# print(f\"PyTorch version: {torch.__version__}\")\n# print(f\"CUDA available: {torch.cuda.is_available()}\")\n# print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n# %pip install uv\n# !uv pip install -q unsloth wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets --system\n# # Finally, install a compatible xformers version\n# !uv pip install -q xformers==0.0.27 --system  # This version is compatible with PyTorch 2.4.x\n# !python -m xformers.info","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:37:04.776241Z","iopub.execute_input":"2024-11-04T05:37:04.776530Z","iopub.status.idle":"2024-11-04T05:37:04.782686Z","shell.execute_reply.started":"2024-11-04T05:37:04.776495Z","shell.execute_reply":"2024-11-04T05:37:04.781873Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Git clone qLeap-fft repo inside `/kaggle/working/` directory\n## Ensure to have the latest branch\n## Switch to qLeap-fft directory","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\ndef ensure_working_directory():\n    \"\"\"\n    Check if we're in the correct working directory, if not switch to it.\n    Creates the directory if it doesn't exist.\n    \"\"\"\n    target_dir = '/kaggle/working/quantumLeap'\n    current_dir = os.getcwd()\n    \n    # Print current directory\n    print(f\"Current directory: {current_dir}\")\n    \n    # Check if we need to switch directories\n    if current_dir != target_dir:\n        # Create directory if it doesn't exist\n        Path(target_dir).mkdir(parents=True, exist_ok=True)\n        \n        try:\n            # Change to target directory\n            os.chdir(target_dir)\n            print(f\"Successfully switched to: {target_dir}\")\n        except Exception as e:\n            print(f\"Error switching to directory: {str(e)}\")\n            raise\n    else:\n        print(\"Already in correct directory\")\n    \n    # Verify current directory\n    print(f\"Working directory: {os.getcwd()}\")\n\n# Call the function before your main code\nensure_working_directory()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:41:27.038306Z","iopub.execute_input":"2024-11-04T05:41:27.038727Z","iopub.status.idle":"2024-11-04T05:41:27.048543Z","shell.execute_reply.started":"2024-11-04T05:41:27.038691Z","shell.execute_reply":"2024-11-04T05:41:27.047560Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Current directory: /kaggle/working\nSuccessfully switched to: /kaggle/working/quantumLeap\nWorking directory: /kaggle/working/quantumLeap\n","output_type":"stream"}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 1.1: Install and Setup Libraries\n# ----------------------------- #\n\n# !python -m xformers.info\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n%pip install uv\n!uv pip install -q unsloth wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets --system\n# Finally, install a compatible xformers version\n!uv pip install -q xformers==0.0.27 --system  # This version is compatible with PyTorch 2.4.x\n!python -m xformers.info# # !python -m xformers.info\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n%pip install uv\n!uv pip install -q unsloth wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets --system\n# Finally, install a compatible xformers version\n!uv pip install -q xformers==0.0.27 --system  # This version is compatible with PyTorch 2.4.x\n!python -m xformers.info","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:41:40.593541Z","iopub.execute_input":"2024-11-04T05:41:40.593937Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"PyTorch version: 2.4.0\nCUDA available: True\nCUDA version: 12.3\nCollecting uv\n  Downloading uv-0.4.29-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading uv-0.4.29-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport torch\nimport nltk\nimport spacy\nimport xformers\nimport bitsandbytes\nimport datasets\nimport huggingface_hub\nimport wandb\nimport ipywidgets\nimport unsloth\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom datasets import Dataset\nimport logging\nimport argparse","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:39:24.353195Z","iopub.execute_input":"2024-10-27T12:39:24.353585Z","iopub.status.idle":"2024-10-27T12:39:33.653370Z","shell.execute_reply.started":"2024-10-27T12:39:24.353549Z","shell.execute_reply":"2024-10-27T12:39:33.651946Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 1: Install and Setup Libraries\n# ----------------------------- #\n\n# Ensure NLTK's punkt tokenizer is available\nimport nltk\ntry:\n    nltk.data.find('tokenizers/punkt')\n    print('punkt was already available.')\nexcept LookupError:\n    nltk.download('punkt')\n    print('punkt was not available. It has been downloaded')\n\n# Initialize spaCy English model\ntry:\n    nlp = spacy.load('en_core_web_sm')\n    print('en_core_web_sm was already available.')\nexcept OSError:\n    print(\"SpaCy English model not found. Downloading...\")\n    os.system('python -m spacy download en_core_web_sm')\n    nlp = spacy.load('en_core_web_sm')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:39:33.655580Z","iopub.execute_input":"2024-10-27T12:39:33.657888Z","iopub.status.idle":"2024-10-27T12:39:34.801213Z","shell.execute_reply.started":"2024-10-27T12:39:33.657830Z","shell.execute_reply":"2024-10-27T12:39:34.800161Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"punkt was already available.\nen_core_web_sm was already available.\n","output_type":"stream"}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Load and Clean the Text Data\n# ----------------------------- #\n\ndef load_and_clean_text(file_path):\n    \"\"\"\n    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        text = f.read()\n    # # Remove Project Gutenberg's license text and headers/footers\n    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n\n    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n    return text.strip()\n\n# Replace 'psychology_of_unconscious.txt' with your actual file path\nfile_path = '/kaggle/input/psychologyofunconsciousmind/psychology_of_unconscious.txt'\nclean_text = load_and_clean_text(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:39:34.804950Z","iopub.execute_input":"2024-10-27T12:39:34.805263Z","iopub.status.idle":"2024-10-27T12:39:34.815213Z","shell.execute_reply.started":"2024-10-27T12:39:34.805228Z","shell.execute_reply":"2024-10-27T12:39:34.814481Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 3: Parse Text into Discourse Units\n# ----------------------------- #\n\n# def parse_discourse_units(text):\n#     \"\"\"\n#     Parses text into discourse units using spaCy.\n#     Currently splits text into sentences.\n#     \"\"\"\n#     paragraphs = text.split('\\n\\n')\n#     paragraphs = [para.strip() for para in paragraphs if para.strip()]\n    \n#     discourse_units = []\n#     for para in paragraphs:\n#         doc = nlp(para)\n#         sentences = [sent.text for sent in doc.sents]\n#         discourse_units.extend(sentences)\n#     return discourse_units\n\n# discourse_units = parse_discourse_units(clean_text)\n\n# # Save discourse_units to a JSON file\n# with open('/kaggle/working/discourse_units_final.json', 'w', encoding='utf-8') as f:\n#     json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n    \n# Load discourse_units from the JSON file\nwith open('/kaggle/working/discourse_units_final.json', 'r', encoding='utf-8') as f:\n    discourse_units = json.load(f)\n\nlen(discourse_units)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:39:34.816211Z","iopub.execute_input":"2024-10-27T12:39:34.816515Z","iopub.status.idle":"2024-10-27T12:39:34.836158Z","shell.execute_reply.started":"2024-10-27T12:39:34.816483Z","shell.execute_reply":"2024-10-27T12:39:34.835265Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"6175"},"metadata":{}}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 4: Create Chunks Using Hybrid Strategy\n# ----------------------------- #\n\ndef create_chunks(discourse_units, tokenizer, max_length=512, overlap_size=0):\n    \"\"\"\n    Creates chunks from discourse units using a sliding window with overlapping chunks.\n    \"\"\"\n    chunks = []\n    current_chunk = []\n    current_length = 0\n\n    for unit in discourse_units:\n        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n        unit_length = len(unit_tokens)\n\n        if current_length + unit_length <= max_length:\n            current_chunk.append(unit)\n            current_length += unit_length\n        else:\n            # Append the current chunk\n            chunks.append(' '.join(current_chunk))\n            # Create overlap\n            overlap_text = ' '.join(current_chunk)[-overlap_size:]\n            overlap_tokens = tokenizer.encode(overlap_text, add_special_tokens=False)\n            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n            # Start new chunk with overlap and current unit\n            current_chunk = [overlap_text, unit]\n            current_length = len(tokenizer.encode(overlap_text, add_special_tokens=False)) + unit_length\n\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n\n    return chunks","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:39:34.837560Z","iopub.execute_input":"2024-10-27T12:39:34.837939Z","iopub.status.idle":"2024-10-27T12:39:34.847329Z","shell.execute_reply.started":"2024-10-27T12:39:34.837893Z","shell.execute_reply":"2024-10-27T12:39:34.846538Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Load the Tokenizer and Model\n# ----------------------------- #\n\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\n\n                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n    lora_alpha = 32,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = True,   # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:39:34.848785Z","iopub.execute_input":"2024-10-27T12:39:34.849153Z","iopub.status.idle":"2024-10-27T12:43:28.526746Z","shell.execute_reply.started":"2024-10-27T12:39:34.849111Z","shell.execute_reply":"2024-10-27T12:43:28.525928Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   2%|2         | 52.4M/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5591ecaa14349fdb8d54861bcc7f505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35f7676333764df0928d22bd1ab14d4a"}},"metadata":{}},{"name":"stderr","text":"Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\nPlease update transformers, TRL and unsloth via:\n`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Offloading input_embeddings to disk to save VRAM\nUnsloth: Offloading output_embeddings to disk to save VRAM\n","output_type":"stream"},{"name":"stderr","text":"Unsloth 2024.10.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Casting embed_tokens to float32\nUnsloth: Casting lm_head to float32\n","output_type":"stream"}]},{"cell_type":"code","source":"# # ----------------------------- #\n# # Part 5: Load the Tokenizer and Model\n# # ----------------------------- #\n# import os\n# import re\n# import torch\n# import nltk\n# import spacy\n# from transformers import (\n#     AutoTokenizer,\n#     AutoModelForCausalLM,\n#     TrainingArguments,\n#     Trainer,\n#     DataCollatorForLanguageModeling,\n# )\n# from datasets import Dataset\n# import logging\n# import argparse\n# import wandb\n# from unsloth import FastLanguageModel\n\n# # Check available GPUs\n# num_gpus = torch.cuda.device_count()\n# print(f\"Number of available GPUs: {num_gpus}\")\n# for i in range(num_gpus):\n#     print(f\"GPU {i}: {torch.cuda.get_device_properties(i).name}\")\n\n# # Model configuration\n# max_seq_length = 512\n# dtype = None  # None for auto detection\n# load_in_4bit = True\n\n# # Determine optimal device map based on available GPUs\n# device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n# print(f\"Using single device: {device_map}\")\n\n# # Model paths\n# model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n# models_dir = os.path.join(os.path.dirname(os.getcwd()), \"models\")\n# model_path = os.path.join(models_dir, model_name)\n\n# # Create models directory if it doesn't exist\n# if not os.path.exists(models_dir):\n#     os.makedirs(models_dir)\n\n# # Load or download model\n# try:\n#     if os.path.exists(model_path):\n#         print(f\"Loading model from local path: {model_path}\")\n#         model, tokenizer = FastLanguageModel.from_pretrained(\n#             model_path,\n#             max_seq_length=max_seq_length,\n#             dtype=dtype,\n#             load_in_4bit=load_in_4bit,\n#             device_map=device_map,  # Add device map for multi-GPU support\n#         )\n#     else:\n#         print(f\"Downloading model from HuggingFace: {model_name}\")\n#         model, tokenizer = FastLanguageModel.from_pretrained(\n#             model_name,\n#             max_seq_length=max_seq_length,\n#             dtype=dtype,\n#             load_in_4bit=load_in_4bit,\n#             device_map=device_map,  # Add device map for multi-GPU support\n#             token=\"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\",\n#         )\n# except Exception as e:\n#     print(f\"Error loading model: {str(e)}\")\n#     raise\n\n# # Configure PEFT with optimizations for multi-GPU setup\n# model = FastLanguageModel.get_peft_model(\n#     model,\n#     r=128,\n#     target_modules=[\n#         \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n#         \"gate_proj\", \"up_proj\", \"down_proj\",\n#         \"embed_tokens\", \"lm_head\",\n#     ],\n#     lora_alpha=32,\n#     lora_dropout=0,\n#     bias=\"none\",\n#     use_gradient_checkpointing=\"unsloth\",\n#     random_state=3407,\n#     use_rslora=True,\n#     loftq_config=None,\n# )\n\n# # Print model device distribution\n# if hasattr(model, 'hf_device_map'):\n#     print(\"\\nModel Device Distribution:\")\n#     for name, device in model.hf_device_map.items():\n#         print(f\"{name}: {device}\")\n\n# # Print memory usage per GPU\n# print(\"\\nGPU Memory Usage after model loading:\")\n# for i in range(num_gpus):\n#     memory_used = torch.cuda.memory_allocated(i) / (1024**3)\n#     memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n#     print(f\"GPU {i}: {memory_used:.2f}GB / {memory_total:.2f}GB\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:28.528305Z","iopub.execute_input":"2024-10-27T12:43:28.528927Z","iopub.status.idle":"2024-10-27T12:43:28.536954Z","shell.execute_reply.started":"2024-10-27T12:43:28.528863Z","shell.execute_reply":"2024-10-27T12:43:28.535822Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 6: Create Chunks (After Tokenizer is Loaded)\n# ----------------------------- #\n\nchunks = create_chunks(discourse_units, tokenizer, max_length=2048, overlap_size=100)\n\n# Save chunks to a JSON file (Optional)\nwith open('/kaggle/working/chunks.json', 'w', encoding='utf-8') as f:\n    json.dump(chunks, f, ensure_ascii=False, indent=4)\n\n# # If you need to reload from JSON (Optional)\n# with open('/kaggle/working/chunks.json', 'r', encoding='utf-8') as f:\n#     chunks = json.load(f)\n    \nprint(len(chunks))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:28.538299Z","iopub.execute_input":"2024-10-27T12:43:28.539043Z","iopub.status.idle":"2024-10-27T12:43:29.421717Z","shell.execute_reply.started":"2024-10-27T12:43:28.538994Z","shell.execute_reply":"2024-10-27T12:43:29.420704Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"99\n","output_type":"stream"}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 7: Create and Tokenize Dataset\n# ----------------------------- #\n\n# Create a Dataset object from chunks\n\nbook_title = 'Psychology of the Unconscious by C. G. Jung'\nwikipedia_prompt = \"\"\"\n### Title: {}\n\n### Article: {}\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    titles = book_title\n    texts  = examples[\"text\"]\n    outputs = []\n    for title, text in zip([book_title]*len(chunks), texts):\n        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n        outputs.append(text)\n    return { \"text\" : outputs, }\npass\n\n# convert chunks variable to huggingface dataset\n\ndataset = Dataset.from_dict({\"text\": chunks})\n\n# dataset = dataset.train_test_split(test_size = 0.1)[\"train\"]\n\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n\nlen(dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:29.423112Z","iopub.execute_input":"2024-10-27T12:43:29.423603Z","iopub.status.idle":"2024-10-27T12:43:29.504471Z","shell.execute_reply.started":"2024-10-27T12:43:29.423556Z","shell.execute_reply":"2024-10-27T12:43:29.503491Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/99 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e39e557347d465abaea23dbe4de19f9"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"99"},"metadata":{}}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 8: Configure Training Arguments\n# ----------------------------- #\n\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Use warmup_ratio and num_train_epochs for longer runs!\n        max_steps = 120,\n        warmup_steps = 10,\n        # warmup_ratio = 0.1,\n        # num_train_epochs = 1,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = 5e-5,\n        embedding_learning_rate = 1e-5,\n\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:29.505741Z","iopub.execute_input":"2024-10-27T12:43:29.506070Z","iopub.status.idle":"2024-10-27T12:43:31.329753Z","shell.execute_reply.started":"2024-10-27T12:43:29.506035Z","shell.execute_reply":"2024-10-27T12:43:31.328952Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/99 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a290f1ac39be4d239c80a2aaa8ec3154"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"# # ----------------------------- #\n# # Part 8: Configure Training Arguments\n# # ----------------------------- #\n\n\n# from transformers import TrainingArguments\n# from unsloth import is_bfloat16_supported\n# from unsloth import UnslothTrainer, UnslothTrainingArguments\n\n# model = model.cuda(0)\n# trainer = UnslothTrainer(\n#     model = model,\n#     tokenizer = tokenizer,\n#     train_dataset = dataset,\n#     dataset_text_field = \"text\",\n#     max_seq_length = max_seq_length,\n#     dataset_num_proc = 2,\n\n#     args = UnslothTrainingArguments(\n#         per_device_train_batch_size = 2,\n#         gradient_accumulation_steps = 8,\n\n#         # Use warmup_ratio and num_train_epochs for longer runs!\n#         max_steps = 120,\n#         warmup_steps = 10,\n#         # warmup_ratio = 0.1,\n#         # num_train_epochs = 1,\n\n#         # Select a 2 to 10x smaller learning rate for the embedding matrices!\n#         learning_rate = 5e-5,\n#         embedding_learning_rate = 1e-5,\n\n#         fp16 = not is_bfloat16_supported(),\n#         bf16 = is_bfloat16_supported(),\n\n#         logging_steps = 1,\n#         optim = \"adamw_8bit\",\n#         weight_decay = 0.01,\n#         lr_scheduler_type = \"linear\",\n#         seed = 3407,\n#         output_dir = \"outputs\",\n#     ),\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:31.331398Z","iopub.execute_input":"2024-10-27T12:43:31.331977Z","iopub.status.idle":"2024-10-27T12:43:31.337797Z","shell.execute_reply.started":"2024-10-27T12:43:31.331924Z","shell.execute_reply":"2024-10-27T12:43:31.336868Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 9: Define Compute Metrics Function\n# ----------------------------- #\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Computes perplexity based on model predictions and labels.\n    \"\"\"\n    logits, labels = eval_pred\n    # Convert to torch tensors\n    logits = torch.tensor(logits)\n    labels = torch.tensor(labels)\n    \n    # Ensure shapes match\n    if logits.shape[:2] != labels.shape:\n        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n    \n    # Shift logits and labels\n    shift_logits = logits[:, :-1, :].contiguous()\n    shift_labels = labels[:, 1:].contiguous()\n\n    # Check label values\n    if shift_labels.max() >= model.config.vocab_size:\n        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n    \n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    perplexity = torch.exp(loss).item()\n    return {\"perplexity\": perplexity}","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:31.341960Z","iopub.execute_input":"2024-10-27T12:43:31.342303Z","iopub.status.idle":"2024-10-27T12:43:31.350915Z","shell.execute_reply.started":"2024-10-27T12:43:31.342258Z","shell.execute_reply":"2024-10-27T12:43:31.350075Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\n# instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n# ### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n# concept_name: {}\n# detailed_explanation: {}\n\n# ### Response:\n# {}\"\"\"\n\n# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n# inputs = tokenizer(\n# [\n#     instruction_prompt.format(\n#         \"Hero Archetype\", # concept_name\n#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n#         \"\", # output - leave this blank for generation!\n#     )\n# ], return_tensors = \"pt\").to(\"cuda\")\n\n# from transformers import StoppingCriteria, StoppingCriteriaList\n\n# class EndOfTextCriteria(StoppingCriteria):\n#     def __init__(self, eos_token_id):\n#         self.eos_token_id = eos_token_id\n\n#     def __call__(self, input_ids, scores, **kwargs):\n#         return input_ids[0][-1] == self.eos_token_id\n\n# stopping_criteria = StoppingCriteriaList([EndOfTextCriteria(tokenizer.eos_token_id)])\n\n# model.config.torch_dtype = torch.float16 \n# outputs = model.generate(**inputs, \n#                          max_new_tokens=64, \n#                          stopping_criteria=stopping_criteria,\n#                          use_cache=True)\n\n# # outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n# print(tokenizer.batch_decode(outputs))\n\n# # Text Streaming\n\n# # from transformers import TextStreamer\n# # text_streamer = TextStreamer(tokenizer)\n# # _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n\n# # inputs = tokenizer(\n# # [\n# #     instruction_prompt.format(\n# #         \"Hero Archetype\", # concept_name\n# #         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n# #         \"\", # output - leave this blank for generation!\n# #     )\n# # ], return_tensors = \"pt\").to(\"cuda\")\n\n\n\n# # from transformers import TextStreamer\n# # text_streamer = TextStreamer(tokenizer)\n# # _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n# #                    repetition_penalty = 0.1)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:31.352281Z","iopub.execute_input":"2024-10-27T12:43:31.353148Z","iopub.status.idle":"2024-10-27T12:43:31.363350Z","shell.execute_reply.started":"2024-10-27T12:43:31.353098Z","shell.execute_reply":"2024-10-27T12:43:31.362400Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 11: Start Training\n# ----------------------------- #\n\n#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:31.364697Z","iopub.execute_input":"2024-10-27T12:43:31.365348Z","iopub.status.idle":"2024-10-27T12:43:35.851027Z","shell.execute_reply.started":"2024-10-27T12:43:31.365302Z","shell.execute_reply":"2024-10-27T12:43:35.849608Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n6.982 GB of memory reserved.\n**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\nUnsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\nUnsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 99 | Num Epochs = 40\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 8\n\\        /    Total batch size = 32 | Total steps = 120\n \"-____-\"     Number of trainable parameters = 982,515,712\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpu_stats\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Max memory = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_memory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_gpu_memory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB of memory reserved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#@title Show final memory and time stats\u001b[39;00m\n\u001b[1;32m     13\u001b[0m used_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n","File \u001b[0;32m<string>:156\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n","File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule must have its parameters and buffers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m                            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (device_ids[0]) but found one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m                            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m inputs, module_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscatter(inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu"],"ename":"RuntimeError","evalue":"module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu","output_type":"error"}]},{"cell_type":"code","source":"import wandb\nfrom pprint import pprint\n\ndef get_run_config(project_name, run_id):\n    try:\n        # Initialize the wandb API\n        api = wandb.Api()\n\n        # Access the specific run\n        run = api.run(f\"{project_name}/{run_id}\")\n\n        # Get the full configuration\n        config = run.config\n\n        # Filter for trainer-specific configuration\n        trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n\n        return trainer_config\n\n    except wandb.errors.CommError:\n        print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None\n\n# Usage\nproject_name = \"olabs-asia-olabs-pro/huggingface\"\nrun_id = \"ppqtwwmy\"\n\ntrainer_config = get_run_config(project_name, run_id)\n\nif trainer_config:\n    print(f\"Trainer configuration for run {run_id}:\")\n    pprint(trainer_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:35.851869Z","iopub.status.idle":"2024-10-27T12:43:35.852223Z","shell.execute_reply.started":"2024-10-27T12:43:35.852052Z","shell.execute_reply":"2024-10-27T12:43:35.852070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom pprint import pprint\n\ndef get_run_config(project_name, run_id):\n    try:\n        # Initialize the wandb API\n        api = wandb.Api()\n\n        # Access the specific run\n        run = api.run(f\"{project_name}/{run_id}\")\n\n        # Get the full configuration\n        config = run.config\n\n        # Filter for trainer-specific configuration\n        trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n\n        return trainer_config\n\n    except wandb.errors.CommError:\n        print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None\n\n# Usage\nproject_name = \"olabs-asia-olabs-pro/huggingface\"\nrun_id = \"i7lm3hcl\"\n\ntrainer_config = get_run_config(project_name, run_id)\n\nif trainer_config:\n    print(f\"Trainer configuration for run {run_id}:\")\n    pprint(trainer_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T12:43:35.853661Z","iopub.status.idle":"2024-10-27T12:43:35.854089Z","shell.execute_reply.started":"2024-10-27T12:43:35.853856Z","shell.execute_reply":"2024-10-27T12:43:35.853896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Loss from earlier training was too high. We shall use training arguments from unsloth colab notebook \"Llama-3.1 8b + Unsloth 2x faster finetuning.ipynb\". URL below\n### https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=95_Nn-89DhsL","metadata":{}},{"cell_type":"code","source":"print(dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import IntervalStrategy\nimport wandb\n\n# Initialize wandb\nwandb.init(project=\"huggingface\", name=\"refined_training_run\")\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = dataset.select(range(len(dataset) // 10)),  # Use 10% of data for evaluation\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Set both max_steps and num_train_epochs\n        max_steps = 120,\n        num_train_epochs = 3,\n\n        # Use a single learning rate for all parameters\n        learning_rate = 5e-5,\n\n        # Warmup strategy from successful runs\n        warmup_steps = 10,\n        warmup_ratio = 0,\n\n        # Explicitly set precision based on hardware support\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        \n        logging_steps = 1,\n        \n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        \n        seed = 3407,\n        output_dir = \"outputs\",\n        \n        report_to = \"wandb\",  # Enable Weights & Biases logging\n        \n        # Set both save and evaluation strategies to 'steps'\n        save_strategy = IntervalStrategy.STEPS,\n        eval_strategy = IntervalStrategy.STEPS,\n        save_steps = 1,  # Save checkpoint every 20 steps\n        eval_steps = 1,  # Evaluate every 20 steps (matching save_steps)\n        \n        load_best_model_at_end = True,\n        metric_for_best_model = \"eval_loss\",\n    ),\n    compute_metrics = compute_metrics,\n)\n\n# ... (rest of the code remains the same)\n\n# Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\n# Start training\ntrainer_stats = trainer.train()\n\n# Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n# Generation code (unchanged)\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\nclass EndOfTextCriteria(StoppingCriteria):\n    def __init__(self, eos_token_id):\n        self.eos_token_id = eos_token_id\n\n    def __call__(self, input_ids, scores, **kwargs):\n        return input_ids[0][-1] == self.eos_token_id\n\nstopping_criteria = StoppingCriteriaList([EndOfTextCriteria(tokenizer.eos_token_id)])\n\noutputs = model.generate(**inputs, \n                         max_new_tokens=64, \n                         stopping_criteria=stopping_criteria,\n                         use_cache=True)\n\nprint(tokenizer.batch_decode(outputs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # delete previous trainer\n# del trainer\n\n#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\n\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n\n\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n\n# inputs = tokenizer(\n# [\n#     instruction_prompt.format(\n#         \"Hero Archetype\", # concept_name\n#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n#         \"\", # output - leave this blank for generation!\n#     )\n# ], return_tensors = \"pt\").to(\"cuda\")\n\n\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n#                    repetition_penalty = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\n# add current timestamp to model name\nmodel.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\ntokenizer.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\")\nmodel.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_base_{int(time.time())}\", token = \"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\") # Online saving\ntokenizer.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_{int(time.time())}\", token = \"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\") # Online saving\n\n# # Merge to 16bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n\n# # Merge to 4bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n\n# # Just LoRA adapters\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n    \n    \n# # Save to 8bit Q8_0\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n\n# # Save to 16bit GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n\n# # Save to q4_k_m GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n\n# inputs = tokenizer(\n# [\n#     instruction_prompt.format(\n#         \"Hero Archetype\", # concept_name\n#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n#         \"\", # output - leave this blank for generation!\n#     )\n# ], return_tensors = \"pt\").to(\"cuda\")\n\n\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n#                    repetition_penalty = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instruction  Tuning","metadata":{}},{"cell_type":"code","source":"\n# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n\nimport json\nimport ast\nimport logging\n\nimport csv\n\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    data = list(reader)\n    \ntype(data)\n\n\n# Configure logging\nlogging.basicConfig(\n    filename='transformation_errors.log',\n    filemode='w',\n    level=logging.ERROR,\n    format='%(levelname)s:%(message)s'\n)\n\n# Sample original data\noriginal_data = data\n\ndef transform_data(original_data):\n    \"\"\"\n    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n\n    Parameters:\n        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n\n    Returns:\n        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n    \"\"\"\n    new_data = []\n\n    for idx, entry in enumerate(original_data, start=1):\n        concept_name = entry.get('concept_name', '').strip()\n        detailed_explanation = entry.get('detailed_explanation', '').strip()\n        example_scenario_str = entry.get('example_scenario', '').strip()\n\n        if not concept_name or not detailed_explanation or not example_scenario_str:\n            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n            continue\n\n        # Attempt to parse with json.loads\n        try:\n            example_scenarios = json.loads(example_scenario_str)\n            if not isinstance(example_scenarios, list):\n                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n        except json.JSONDecodeError:\n            # Fallback to ast.literal_eval\n            try:\n                example_scenarios = ast.literal_eval(example_scenario_str)\n                if not isinstance(example_scenarios, list):\n                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n            except (ValueError, SyntaxError) as e:\n                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n                continue\n\n        # Iterate through each scenario and create a new entry\n        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n            if not isinstance(scenario, str):\n                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n                continue\n\n            new_entry = {\n                'concept_name': concept_name,\n                'detailed_explanation': detailed_explanation,\n                'example_scenario': scenario.strip()\n            }\n            new_data.append(new_entry)\n\n    return new_data\n\n# Transform the data\ntransformed_data = transform_data(original_data)\n\n# Optional: Save the transformed data to a JSON file\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n\nprint(f\"Transformation complete. {len(transformed_data)} entries created.\")\nprint(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n\nprint(len(transformed_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef instruction_prompt_func(examples):\n    concept_name = examples[\"concept_name\"]\n    detailed_explanation = examples[\"detailed_explanation\"]\n    example_scenario = examples[\"example_scenario\"]\n    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\npass\n\n\n# convert transformed_data to a huggingface dataset\ninstruction_dataset = Dataset.from_dict(transformed_data)\ninstruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = instruction_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 8,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Use num_train_epochs and warmup_ratio for longer runs!\n        max_steps = 120,\n        warmup_steps = 10,\n        # warmup_ratio = 0.1,\n        # num_train_epochs = 1,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = 5e-5,\n        embedding_learning_rate = 1e-5,\n\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.00,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add current timestamp to model name\nmodel.save_pretrained(f\"qLeap_model_v0_{int(time.time())}\") # Local saving\ntokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\nmodel.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\ntokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n\n# # Merge to 16bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n\n# # Merge to 4bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n\n# # Just LoRA adapters\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n    \n    \n# # Save to 8bit Q8_0\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n\n# # Save to 16bit GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n\n# # Save to q4_k_m GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}