{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Git clone qLeap-fft repo inside `/root/` directory\n","## Ensure to have the latest branch\n","## Switch to quantumLeap directory"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","\n","# Set these environment variables before importing torch-related modules\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","\n","from pathlib import Path\n","\n","def ensure_working_directory():\n","    \"\"\"\n","    Check if we're in the correct working directory, if not switch to it.\n","    Creates the directory if it doesn't exist.\n","    \"\"\"\n","    target_dir = '/home/ubuntu/quantumLeap'\n","    current_dir = os.getcwd()\n","    \n","    # Print current directory\n","    print(f\"Current directory: {current_dir}\")\n","    \n","    # Check if we need to switch directories\n","    if current_dir != target_dir:\n","        # Create directory if it doesn't exist\n","        Path(target_dir).mkdir(parents=True, exist_ok=True)\n","        \n","        try:\n","            # Change to target directory\n","            os.chdir(target_dir)\n","            print(f\"Successfully switched to: {target_dir}\")\n","        except Exception as e:\n","            print(f\"Error switching to directory: {str(e)}\")\n","            raise\n","    else:\n","        print(\"Already in correct directory\")\n","    \n","    # Verify current directory\n","    print(f\"Working directory: {os.getcwd()}\")\n","\n","# Call the function before your main code\n","ensure_working_directory()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ----------------------------- #\n","# Part 1.1: Install and Setup Libraries - for Ola Krutrim Cloud Instance\n","# ----------------------------- #\n","\n","# # if executing below in terminal with virtual env, do not need to add --system tag\n","# pip install uv #install this in the virtual environment where you want to execute the notebook.\n","# pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121 # as on 07Nov2024, xformers is compatible with torch=2.4.0 only; uv doesnt work for installing torch\n","# uv pip install packaging ninja\n","# uv pip install flash-attn --no-build-isolation\n","# uv pip install unsloth\n","# python -m xformers.info\n","# uv pip install wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets tqdm Iprogress ipywidgets python-dotenv tensorboard -q\n","\n","# restart once you have installed all of the above\n","!nvidia-smi\n","!nvcc --version\n","import torch\n","print(torch.__version__)          # Should reflect 2.5.0+cu124\n","print(torch.version.cuda)         # Should output 12.4\n","print(torch.cuda.is_available())  # Should return True"]},{"cell_type":"markdown","metadata":{},"source":["# Restart again so that all the libraries are properly initialized"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:33.365845Z","iopub.status.busy":"2024-11-04T08:49:33.365559Z","iopub.status.idle":"2024-11-04T08:49:47.786343Z","shell.execute_reply":"2024-11-04T08:49:47.785092Z","shell.execute_reply.started":"2024-11-04T08:49:33.365811Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 1.2: Import Necessary Libraries\n","# ----------------------------- #\n","\n","# General Libraries\n","import os\n","import json\n","import sys\n","import subprocess\n","import argparse\n","import logging\n","import math\n","import random\n","from datetime import datetime\n","import re\n","import gc\n","import weakref\n","import multiprocessing\n","\n","# Torch related\n","import torch\n","from torch import nn\n","import torch.distributed as dist\n","\n","# Transformers related\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForLanguageModeling,\n","    Adafactor\n",")\n","\n","# Huggingface TRL for full finetune\n","from trl import SFTTrainer, SFTConfig\n","\n","# General huggingface libraries\n","import huggingface_hub\n","from datasets import load_dataset, Dataset\n","from accelerate import Accelerator\n","\n","\n","# Unsloth specificic libraries\n","import unsloth\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n","\n","# Other Libraries\n","from peft import LoraConfig\n","import wandb\n","import nltk\n","import spacy\n","# from galore_torch import GaLoreAdamW, GaLoreAdafactor, GaLoreAdamW8bit\n","\n","# Check and import NLTK and spacy modules\n","# Ensure NLTK's punkt tokenizer is available\n","import nltk\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","    print('punkt was already available.')\n","except LookupError:\n","    nltk.download('punkt')\n","    print('punkt was not available. It has been downloaded')\n","\n","# Initialize spaCy English model\n","try:\n","    nlp = spacy.load('en_core_web_sm')\n","    print('en_core_web_sm was already available.')\n","except OSError:\n","    print(\"SpaCy English model not found. Downloading...\")\n","    os.system('python -m spacy download en_core_web_sm')\n","    nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.535557Z","iopub.status.busy":"2024-11-04T08:49:54.535142Z","iopub.status.idle":"2024-11-04T08:49:54.549789Z","shell.execute_reply":"2024-11-04T08:49:54.549048Z","shell.execute_reply.started":"2024-11-04T08:49:54.535509Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 2: Load and Clean the Text Data\n","# ----------------------------- #\n","\n","def load_and_clean_text(file_path):\n","    \"\"\"\n","    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n","    \"\"\"\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    # # Remove Project Gutenberg's license text and headers/footers\n","    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","\n","    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n","    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n","    return text.strip()\n","\n","# Replace 'psychology_of_unconscious.txt' with your actual file path\n","file_path = '/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'\n","clean_text = load_and_clean_text(file_path)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.825136Z","iopub.status.busy":"2024-11-04T08:49:54.824806Z","iopub.status.idle":"2024-11-04T08:49:54.833600Z","shell.execute_reply":"2024-11-04T08:49:54.832541Z","shell.execute_reply.started":"2024-11-04T08:49:54.825103Z"},"trusted":true},"outputs":[],"source":["# # ----------------------------- #\n","# # Part 3: Parse Text into Discourse Units\n","# # ----------------------------- #\n","\n","def parse_discourse_units(text, overwrite=False):\n","    \"\"\"\n","    Parses text into discourse units using spaCy.\n","    Currently splits text into sentences.\n","    \"\"\"\n","    paragraphs = text.split('\\n\\n')\n","    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n","\n","    discourse_units = []\n","    for para in paragraphs:\n","        doc = nlp(para)\n","        sentences = [sent.text for sent in doc.sents]\n","        discourse_units.extend(sentences)\n","\n","    output_path = '/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_discourse_units.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Discourse Units: {len(discourse_units)}\")\n","    return discourse_units"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.987822Z","iopub.status.busy":"2024-11-04T08:49:54.987546Z","iopub.status.idle":"2024-11-04T08:49:54.997464Z","shell.execute_reply":"2024-11-04T08:49:54.996587Z","shell.execute_reply.started":"2024-11-04T08:49:54.987793Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 4: Create Chunks Using Hybrid Strategy\n","# ----------------------------- #\n","\n","def create_chunks(discourse_units, tokenizer, max_length=4096, overlap_size=1, overwrite=False):\n","    \"\"\"\n","    Creates chunks from discourse units using a sliding window with overlapping chunks.\n","    Optimized to work directly with token IDs and utilize efficient list operations.\n","    \"\"\"\n","    chunks = []\n","    current_chunk_tokens = []\n","    current_length = 0\n","\n","    for unit in discourse_units:\n","        unit_tokens = tokenizer.encode(unit, add_special_tokens=True)\n","        unit_length = len(unit_tokens)\n","\n","        if current_length + unit_length <= max_length:\n","            current_chunk_tokens.extend(unit_tokens)\n","            current_length += unit_length\n","        else:\n","            # Decode and append the current chunk\n","            chunk_text = tokenizer.decode(\n","                current_chunk_tokens, skip_special_tokens=True)\n","            chunks.append(chunk_text)\n","\n","            # Prepare overlap tokens\n","            overlap_tokens = current_chunk_tokens[-overlap_size:]\n","            current_chunk_tokens = overlap_tokens + unit_tokens\n","            current_length = len(current_chunk_tokens)\n","\n","    # Append any remaining tokens as the last chunk\n","    if current_chunk_tokens:\n","        chunk_text = tokenizer.decode(\n","            current_chunk_tokens, skip_special_tokens=True)\n","        chunks.append(chunk_text)\n","\n","    # Write or read chunks as before\n","    output_path = '/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_chunks.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Chunks Created: {len(chunks)}\")\n","    return chunks"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:56.853343Z","iopub.status.busy":"2024-11-04T08:49:56.852589Z","iopub.status.idle":"2024-11-04T08:49:56.862517Z","shell.execute_reply":"2024-11-04T08:49:56.861550Z","shell.execute_reply.started":"2024-11-04T08:49:56.853301Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 5: Create and Tokenize Dataset\n","# ----------------------------- #\n","\n","# To Do - make book titles and prompt generic so\n","def create_tokenized_dataset(chunks, tokenizer, max_length=1024, num_proc=2):\n","\n","    # Create a Dataset object from chunks\n","\n","    book_title = 'Psychology of the Unconscious by C. G. Jung'\n","    wikipedia_prompt = \"\"\"\n","    Psychology Book\n","\n","    ### Title: {}\n","\n","    ### Article: {}\n","    \"\"\"\n","\n","    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n","\n","    def formatting_prompts_func(examples):\n","        titles = book_title\n","        texts = examples[\"text\"]\n","        outputs = []\n","        for title, text in zip([book_title]*len(chunks), texts):\n","            text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n","            outputs.append(text)\n","        return {\"text\": outputs, }\n","    pass\n","\n","    # convert chunks variable to huggingface dataset\n","\n","    from datasets import Dataset\n","\n","    dataset = Dataset.from_dict({\"text\": chunks})\n","\n","    dataset = dataset.map(formatting_prompts_func,\n","                          batched=True, num_proc=num_proc)\n","    # Split the dataset into training and validation sets\n","    split = dataset.train_test_split(test_size=0.1, seed=42)\n","    train_dataset = split['train']\n","    eval_dataset = split['test']\n","\n","    print(len(dataset))\n","    # Find the maximum length of the text field in the entire dataset\n","    max_length = max(len(text) for text in dataset['text'])\n","    print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","#     print(f\"First 5 rows of training dataset: {train_dataset[:5]}\")\n","    print(f\"Validation Dataset Size: {len(eval_dataset)}\")\n","#     print(f\"First 5 rows of validation dataset: {eval_dataset[:5]}\")\n","    return train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:50:01.810305Z","iopub.status.busy":"2024-11-04T08:50:01.809400Z","iopub.status.idle":"2024-11-04T08:50:01.844994Z","shell.execute_reply":"2024-11-04T08:50:01.844131Z","shell.execute_reply.started":"2024-11-04T08:50:01.810262Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 6: Set up environment and other important utilities\n","# ----------------------------- #\n","\n","def setup_environment():\n","    \"\"\"\n","    Initializes the Accelerator for distributed training.\n","    \"\"\"\n","    return Accelerator()\n","\n","\n","def get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n","    \"\"\"\n","    Defines a custom learning rate scheduler with warmup, constant, and linear annealing phases.\n","    \"\"\"\n","    def lr_lambda(current_step):\n","        if current_step < num_warmup_steps:\n","            return current_step / num_warmup_steps  # Linear warmup\n","        elif current_step < initial_phase_steps:\n","            return 1.0  # Constant learning rate for initial phase\n","        else:\n","            # Linear annealing for the remaining steps\n","            return 1.0 - ((current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps))\n","\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","\n","def setup_training_steps(initial_rows, annealing_rows, batch_size, gradient_accumulation_steps, num_epochs):\n","    \"\"\"\n","    Calculates total and initial training steps based on dataset size and training parameters.\n","    \"\"\"\n","    total_rows = initial_rows + annealing_rows\n","    total_steps = (total_rows * num_epochs) // (batch_size *\n","                                                gradient_accumulation_steps)\n","    initial_steps = (initial_rows * num_epochs) // (batch_size *\n","                                                    gradient_accumulation_steps)\n","    return max(1, total_steps), max(1, initial_steps)\n","\n","\n","def print_memory_usage(step_desc):\n","    \"\"\"\n","    Prints the CUDA memory summary if CUDA is available.\n","    \"\"\"\n","    if torch.cuda.is_available():\n","        print(f\"Memory Usage at {step_desc}:\")\n","        print(torch.cuda.memory_summary())\n","        print(\"\\n\")\n","    else:\n","        print(f\"No CUDA available at {step_desc}.\\n\")\n","\n","\n","def inference(model, tokenizer):\n","    \"\"\"\n","    Runs inference using the trained model.\n","    \"\"\"\n","    # Define sample prompts\n","    prompts = [\n","        \"Explain what is medical anthropology and its importance in elevating the quality of life?\",\n","        \"Explain what are the types of Jungian archetypes and how they manifest in the human psyche?\"\n","    ]\n","\n","    for prompt in prompts:\n","        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, max_length=256)\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n","        \n","def compute_metrics(eval_pred):\n","    \"\"\"\n","    Computes perplexity based on model predictions and labels.\n","    \"\"\"\n","    logits, labels = eval_pred\n","    # Convert to torch tensors\n","    logits = torch.tensor(logits)\n","    labels = torch.tensor(labels)\n","    \n","    # Ensure shapes match\n","    if logits.shape[:2] != labels.shape:\n","        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n","    \n","    # Shift logits and labels\n","    shift_logits = logits[:, :-1, :].contiguous()\n","    shift_labels = labels[:, 1:].contiguous()\n","\n","    # Check label values\n","    if shift_labels.max() >= model.config.vocab_size:\n","        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n","    \n","    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n","    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","    perplexity = torch.exp(loss).item()\n","    return {\"perplexity\": perplexity}\n","\n","#  Login to Huggingface\n","from huggingface_hub import login\n","import os\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","def setup_huggingface_access():\n","    \"\"\"Setup Hugging Face access with token from environment or manual input\"\"\"\n","    # First try to get token from environment variable\n","    token = os.getenv('HUGGINGFACE_TOKEN')\n","    \n","    if not token:\n","        # If not in environment, prompt for token\n","        token = input(\"Enter your Hugging Face token: \")\n","        \n","    if token:\n","        try:\n","            login(token, add_to_git_credential=True)\n","            print(\"Successfully logged in to Hugging Face!\")\n","        except Exception as e:\n","            print(f\"Failed to log in to Hugging Face: {str(e)}\")\n","            return False\n","    else:\n","        print(\"No Hugging Face token provided\")\n","        return False\n","    \n","    return True"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:51:23.895754Z","iopub.status.busy":"2024-11-04T08:51:23.894928Z","iopub.status.idle":"2024-11-04T08:51:23.904037Z","shell.execute_reply":"2024-11-04T08:51:23.903118Z","shell.execute_reply.started":"2024-11-04T08:51:23.895714Z"},"trusted":true},"outputs":[],"source":["def load_model_and_tokenizer(base_model_slug, max_seq_length=4096, dtype=None, load_in_4bit=True):\n","    \"\"\"\n","    Load and configure the model and tokenizer with specified parameters on a single GPU.\n","    \"\"\"\n","    import torch\n","    import os\n","\n","    # Force CUDA if available\n","    if torch.cuda.is_available():\n","        print(\"CUDA is available.\")\n","        print(f\"Using GPU: {torch.cuda.get_device_properties(0).name}\")\n","        device = torch.device(\"cuda:0\")\n","        device_map = {\"\": 0}  # Force everything to GPU 0\n","    else:\n","        print(\"WARNING: CUDA is not available. Using CPU.\")\n","        device = torch.device(\"cpu\")\n","        device_map = \"cpu\"\n","\n","    # Print initial GPU memory\n","    if torch.cuda.is_available():\n","        print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n","\n","    try:\n","        # Load base model and tokenizer\n","        model, tokenizer = FastLanguageModel.from_pretrained(\n","            model_name=base_model_slug,\n","            max_seq_length=max_seq_length,\n","            dtype=dtype,\n","            load_in_4bit=load_in_4bit,\n","            device_map=device_map,\n","            token=os.getenv('HUGGINGFACE_TOKEN'),\n","        )\n","        \n","        print(f\"Model device after loading: {next(model.parameters()).device}\")\n","        \n","        # Configure PEFT model\n","        model = FastLanguageModel.get_peft_model(\n","            model,\n","            r=128,\n","            target_modules=[\n","                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                \"gate_proj\", \"up_proj\", \"down_proj\",\n","                \"embed_tokens\", \"lm_head\",\n","            ],\n","            lora_alpha=32,\n","            lora_dropout=0,\n","            bias=\"none\",\n","            use_gradient_checkpointing=\"unsloth\",\n","            random_state=3407,\n","            use_rslora=True,\n","            loftq_config=None,\n","        )\n","        \n","        # Ensure model is on GPU after PEFT configuration\n","        if torch.cuda.is_available():\n","            model = model.to(device)\n","            \n","        # Verify final device placement\n","        print(f\"Final model device: {next(model.parameters()).device}\")\n","        \n","        # Print GPU memory usage\n","        if torch.cuda.is_available():\n","            print(f\"\\nGPU Memory After Complete Setup: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n","            \n","    except Exception as e:\n","        print(f\"Error in model loading/configuration: {str(e)}\")\n","        raise\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:51:27.038467Z","iopub.status.busy":"2024-11-04T08:51:27.038068Z","iopub.status.idle":"2024-11-04T08:51:27.044121Z","shell.execute_reply":"2024-11-04T08:51:27.043250Z","shell.execute_reply.started":"2024-11-04T08:51:27.038425Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","# Set the environment variable\n","os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n","os.environ[\"WANDB_API_KEY\"] = \"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\"\n","# Verify it's set correctly\n","print(os.getenv(\"HUGGINGFACE_TOKEN\"))\n","print(os.getenv(\"WANDB_API_KEY\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:53:40.380684Z","iopub.status.busy":"2024-11-04T08:53:40.380279Z","iopub.status.idle":"2024-11-04T08:54:30.243475Z","shell.execute_reply":"2024-11-04T08:54:30.242381Z","shell.execute_reply.started":"2024-11-04T08:53:40.380645Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 2: Configure Environment Variables & Create Main Variables\n","# ----------------------------- #\n","\n","# Unsloth modell initialization variables\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","max_length = max_seq_length\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","# device_map = \"auto\"\n","base_model_slug = \"Qwen/Qwen2.5-7B-Instruct\"\n","base_model_name = \"lora_model_pum\"\n","chunks_max_length = max_seq_length\n","overlap_size = 1\n","# Define your parameters\n","batchSize = 2\n","ga = 8\n","maxSteps = 10\n","warmupSteps = 10\n","numTrainEpochs = 1\n","lRate = 5e-5\n","embLRate = 1e-5\n","optim = \"adamw_8bit\"\n","lrSchedule = \"linear\"\n","dataset_slug = \"psychology_of_unconscious\"\n","\n","from datetime import datetime\n","import pytz\n","import wandb\n","# Get the current date and time in Indian Standard Time (IST)\n","ist = pytz.timezone('Asia/Kolkata')\n","current_datetime = datetime.now(ist)\n","\n","# Format the datetime string\n","# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n","formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Define Run Name\n","run_name = f\"\"\"Kaggle-quantumLeap-{formatted_datetime}-{base_model_slug}-{dataset_slug}-{max_seq_length}_maxSeqLength-{max_length}_maxLength-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{numTrainEpochs}_numTrainEpochs-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n","\n","# Initialize Weights & Biases\n","# It's recommended to set your W&B API key as an environment variable for security.\n","wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n","wandb.init(project=\"OLA-quantumLeap\", name=run_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:55:17.806518Z","iopub.status.busy":"2024-11-04T08:55:17.806143Z","iopub.status.idle":"2024-11-04T08:55:20.860165Z","shell.execute_reply":"2024-11-04T08:55:20.859337Z","shell.execute_reply.started":"2024-11-04T08:55:17.806483Z"},"trusted":true},"outputs":[],"source":["\n","# ----------------------------- #\n","# Part 9: Data Processing\n","# ----------------------------- #\n","\n","# # Perform Inference Before Training\n","# inference(model, tokenizer)\n","\n","# Set number of processes to use for data loading\n","num_cpus = multiprocessing.cpu_count()\n","num_proc = max(num_cpus-2, 2)  # Adjust based on prior recommendations\n","print(f\"Number of CPU cores: {num_cpus}\")\n","print(f\"Number of processes: {num_proc}\")\n","\n","# Login to Hugging Face\n","if not setup_huggingface_access():\n","    raise Exception(\"Failed to setup Hugging Face access. Please check your token.\")\n","\n","# Load Model and Tokenizer\n","model, tokenizer = load_model_and_tokenizer(base_model_slug)\n","print(f\"Model Device: {model.device}\")\n","\n","# Load and Clean Text Data\n","file_path = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","clean_text = load_and_clean_text(file_path)\n","\n","# Parse Discourse Units\n","discourse_units = parse_discourse_units(clean_text, overwrite=True)\n","\n","# Create Chunks\n","chunks = create_chunks(\n","    discourse_units,\n","    tokenizer,\n","    max_length=max_length,\n","    overlap_size=overlap_size,\n","    overwrite=True,\n",")\n","\n","# Create Tokenized Dataset\n","train_dataset, eval_dataset = create_tokenized_dataset(\n","    chunks, tokenizer, max_length)\n","\n","# Save datasets as Hugging Face `datasets`\n","train_dataset.save_to_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","eval_dataset.save_to_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')\n","\n","### To Do - Make the below as dynamic and as a functio\n","# # Uncomment following if you want to just load the data from temp directory\n","# from datasets import load_from_disk\n","\n","# train_dataset = load_from_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","# eval_dataset = load_from_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import IntervalStrategy\n","from transformers.integrations import TensorBoardCallback\n","\n","import wandb\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_dataset,\n","    eval_dataset = eval_dataset,  # Use 10% of data for evaluation\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = batchSize,\n","        gradient_accumulation_steps = ga,\n","\n","        # Set both max_steps and num_train_epochs\n","        max_steps = maxSteps,\n","        num_train_epochs = numTrainEpochs,\n","\n","        # Use a single learning rate for all parameters\n","        learning_rate = lRate,\n","\n","        # Warmup strategy from successful runs\n","        warmup_steps = warmupSteps,\n","        # warmup_ratio = 0,\n","\n","        # Explicitly set precision based on hardware support\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        \n","        logging_steps = 1,\n","        \n","        optim = optim,\n","        weight_decay = 0.01,\n","        lr_scheduler_type = lrSchedule,\n","        \n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        \n","        report_to=[\"tensorboard\", \"wandb\"],\n","        logging_dir=f\"./trel-fft-logs/{run_name}\",\n","        \n","        # Set both save and evaluation strategies to 'steps'\n","        # save_strategy = IntervalStrategy.STEPS,\n","        # eval_strategy = IntervalStrategy.STEPS,\n","        # save_steps = 1,  # Save checkpoint every 20 steps\n","        # eval_steps = 1,  # Evaluate every 20 steps (matching save_steps)\n","        \n","        # load_best_model_at_end = True,\n","        # metric_for_best_model = \"eval_loss\",\n","    ),\n","    # compute_metrics = compute_metrics,\n",")"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["\n","# instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","# ### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","# concept_name: {}\n","# detailed_explanation: {}\n","\n","# ### Response:\n","# {}\"\"\"\n","\n","# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","# outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n","\n","# tokenizer.batch_decode(outputs)\n","\n","# # Text Streaming\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n","\n","# # inputs = tokenizer(\n","# # [\n","# #     instruction_prompt.format(\n","# #         \"Hero Archetype\", # concept_name\n","# #         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","# #         \"\", # output - leave this blank for generation!\n","# #     )\n","# # ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# # from transformers import TextStreamer\n","# # text_streamer = TextStreamer(tokenizer)\n","# # _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","# #                    repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:55:41.075256Z","iopub.status.busy":"2024-11-04T08:55:41.074858Z","iopub.status.idle":"2024-11-04T10:16:26.590311Z","shell.execute_reply":"2024-11-04T10:16:26.589250Z","shell.execute_reply.started":"2024-11-04T08:55:41.075219Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 11: Start Training\n","# ----------------------------- #\n","\n","#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","trainer_stats = trainer.train()\n","\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ----------------------------- #\n","# Part 8: Configure Training Arguments\n","# ----------------------------- #\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_dataset,\n","    eval_dataset = eval_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = batchSize,\n","        gradient_accumulation_steps = ga,\n","\n","        # Use warmup_ratio and num_train_epochs for longer runs!\n","        max_steps = maxSteps,\n","        warmup_steps = warmupSteps,\n","        # warmup_ratio = 0.1,\n","        num_train_epochs = numTrainEpochs,\n","\n","        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n","        learning_rate = lRate,\n","        embedding_learning_rate = embLRate,\n","\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = optim,\n","        weight_decay = 0.01,\n","        lr_scheduler_type = lrSchedule,\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","\n","        report_to=[\"tensorboard\", \"wandb\"],\n","        logging_dir=f\"./trel-fft-logs/{run_name}\",\n","    ),\n","    compute_metrics = compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ----------------------------- #\n","# Part 11: Start Training\n","# ----------------------------- #\n","\n","#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","trainer_stats = trainer.train()\n","\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import wandb\n","from pprint import pprint\n","\n","def get_run_config(project_name, run_id):\n","    try:\n","        # Initialize the wandb API\n","        api = wandb.Api()\n","\n","        # Access the specific run\n","        run = api.run(f\"{project_name}/{run_id}\")\n","\n","        # Get the full configuration\n","        config = run.config\n","\n","        # Filter for trainer-specific configuration\n","        trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n","\n","        return trainer_config\n","\n","    except wandb.errors.CommError:\n","        print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n","        return None\n","    except Exception as e:\n","        print(f\"An error occurred: {str(e)}\")\n","        return None\n","\n","# Usage\n","project_name = \"olabs-asia-olabs-pro/OLA-quantumLeap\"\n","run_id = \"we4axhd1\"\n","\n","trainer_config = get_run_config(project_name, run_id)\n","\n","if trainer_config:\n","    print(f\"Trainer configuration for run {run_id}:\")\n","    pprint(trainer_config)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","import os\n","\n","# Create timestamp\n","timestamp = int(time.time())\n","\n","# Create directory if it doesn't exist\n","save_dir = f\"/root/quantumLeap/models/qLeap_model_v0_{timestamp}\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# Save functions with explicit paths\n","def save_model_versions(model, tokenizer, timestamp, token):\n","    \"\"\"\n","    Save model in different formats with proper error handling\n","    \"\"\"\n","    try:\n","        # Save base model locally\n","        print(\"Saving base model locally...\")\n","        # model.save_pretrained(f\"{save_dir}/base\")\n","        # tokenizer.save_pretrained(f\"{save_dir}/base\")\n","        \n","        # Save 8-bit Q8_0 version\n","        print(\"Saving 8-bit Q8_0 version...\")\n","        try:\n","            model.push_to_hub_gguf(\n","                f\"olabs-ai/qLeap_model_v0_8bit_Q8_{timestamp}\",\n","                tokenizer,\n","                token=token,\n","                quantization_method=\"q8_0\"\n","            )\n","            print(\"Successfully saved 8-bit model\")\n","        except Exception as e:\n","            print(f\"Error saving 8-bit model: {str(e)}\")\n","            \n","        # Optional: Save 16-bit version\n","        print(\"Saving 16-bit version...\")\n","        try:\n","            model.push_to_hub_gguf(\n","                f\"olabs-ai/qLeap_model_v0_16bit_GGUF_{timestamp}\",\n","                tokenizer,\n","                quantization_method=\"f16\",\n","                token=token\n","            )\n","            print(\"Successfully saved 16-bit model\")\n","        except Exception as e:\n","            print(f\"Error saving 16-bit model: {str(e)}\")\n","            \n","    except Exception as e:\n","        print(f\"Error in save process: {str(e)}\")\n","        raise\n","\n","# Call the save function\n","huggingface_token = \"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\"\n","save_model_versions(model, tokenizer, timestamp, huggingface_token)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"markdown","metadata":{},"source":["### if the loss from earlier training is too high try training arguments from unsloth colab notebook \"Llama-3.1 8b + Unsloth 2x faster finetuning.ipynb\". URL below\n","### https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=95_Nn-89DhsL"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Inference\n"," \n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Hero Archetype\", # concept_name\n","        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Hero Archetype\", # concept_name\n","        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset creation based on the book itself using AugmenToolkit"]},{"cell_type":"markdown","metadata":{},"source":["# Instruction  Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n","\n","import json\n","import ast\n","import logging\n","\n","import csv\n","\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n","    reader = csv.DictReader(f)\n","    data = list(reader)\n","    \n","type(data)\n","\n","\n","# Configure logging\n","logging.basicConfig(\n","    filename='transformation_errors.log',\n","    filemode='w',\n","    level=logging.ERROR,\n","    format='%(levelname)s:%(message)s'\n",")\n","\n","# Sample original data\n","original_data = data\n","\n","def transform_data(original_data):\n","    \"\"\"\n","    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n","\n","    Parameters:\n","        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n","\n","    Returns:\n","        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n","    \"\"\"\n","    new_data = []\n","\n","    for idx, entry in enumerate(original_data, start=1):\n","        concept_name = entry.get('concept_name', '').strip()\n","        detailed_explanation = entry.get('detailed_explanation', '').strip()\n","        example_scenario_str = entry.get('example_scenario', '').strip()\n","\n","        if not concept_name or not detailed_explanation or not example_scenario_str:\n","            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n","            continue\n","\n","        # Attempt to parse with json.loads\n","        try:\n","            example_scenarios = json.loads(example_scenario_str)\n","            if not isinstance(example_scenarios, list):\n","                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","        except json.JSONDecodeError:\n","            # Fallback to ast.literal_eval\n","            try:\n","                example_scenarios = ast.literal_eval(example_scenario_str)\n","                if not isinstance(example_scenarios, list):\n","                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","            except (ValueError, SyntaxError) as e:\n","                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n","                continue\n","\n","        # Iterate through each scenario and create a new entry\n","        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n","            if not isinstance(scenario, str):\n","                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n","                continue\n","\n","            new_entry = {\n","                'concept_name': concept_name,\n","                'detailed_explanation': detailed_explanation,\n","                'example_scenario': scenario.strip()\n","            }\n","            new_data.append(new_entry)\n","\n","    return new_data\n","\n","# Transform the data\n","transformed_data = transform_data(original_data)\n","\n","# Optional: Save the transformed data to a JSON file\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n","    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n","\n","print(f\"Transformation complete. {len(transformed_data)} entries created.\")\n","print(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n","\n","print(len(transformed_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","\n","EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","\n","def instruction_prompt_func(examples):\n","    concept_name = examples[\"concept_name\"]\n","    detailed_explanation = examples[\"detailed_explanation\"]\n","    example_scenario = examples[\"example_scenario\"]\n","    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\n","pass\n","\n","\n","# convert transformed_data to a huggingface dataset\n","instruction_dataset = Dataset.from_dict(transformed_data)\n","instruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n","\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = instruction_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 8,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 8,\n","\n","        # Use num_train_epochs and warmup_ratio for longer runs!\n","        max_steps = 120,\n","        warmup_steps = 10,\n","        # warmup_ratio = 0.1,\n","        # num_train_epochs = 1,\n","\n","        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n","        learning_rate = 5e-5,\n","        embedding_learning_rate = 1e-5,\n","\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.00,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")\n","trainer_stats = trainer.train()\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# add current timestamp to model name\n","model.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\n","tokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\n","model.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","tokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","        \n","# Save to 8bit GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_GGUF_{int(time.time())}\", tokenizer,quantization_method = \"q8_0\", token = \"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\")\n","\n","# Save to 16bit GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n","\n","# Save to q4_k_m GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")\n","\n","\n","\n","# # Merge to 16bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n","\n","# # Merge to 4bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n","\n","# # Just LoRA adapters\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
