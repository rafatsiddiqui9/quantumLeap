{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Git clone qLeap-fft repo inside `/root/` directory\n","## Ensure to have the latest branch\n","## Switch to quantumLeap directory"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","\n","# Set these environment variables before importing torch-related modules\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","\n","from pathlib import Path\n","\n","def ensure_working_directory():\n","    \"\"\"\n","    Check if we're in the correct working directory, if not switch to it.\n","    Creates the directory if it doesn't exist.\n","    \"\"\"\n","    target_dir = '/home/ubuntu/quantumLeap'\n","    current_dir = os.getcwd()\n","    \n","    # Print current directory\n","    print(f\"Current directory: {current_dir}\")\n","    \n","    # Check if we need to switch directories\n","    if current_dir != target_dir:\n","        # Create directory if it doesn't exist\n","        Path(target_dir).mkdir(parents=True, exist_ok=True)\n","        \n","        try:\n","            # Change to target directory\n","            os.chdir(target_dir)\n","            print(f\"Successfully switched to: {target_dir}\")\n","        except Exception as e:\n","            print(f\"Error switching to directory: {str(e)}\")\n","            raise\n","    else:\n","        print(\"Already in correct directory\")\n","    \n","    # Verify current directory\n","    print(f\"Working directory: {os.getcwd()}\")\n","\n","# Call the function before your main code\n","ensure_working_directory()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ----------------------------- #\n","# Part 1.1: Install and Setup Libraries - for Ola Krutrim Cloud Instance\n","# ----------------------------- #\n","\n","# # if executing below in terminal with virtual env, do not need to add --system tag\n","# pip install uv #install this in the virtual environment where you want to execute the notebook.\n","# pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121 # as on 07Nov2024, xformers is compatible with torch=2.4.0 only; uv doesnt work for installing torch\n","# uv pip install packaging ninja\n","# uv pip install flash-attn --no-build-isolation\n","# uv pip install unsloth\n","# python -m xformers.info\n","# uv pip install wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets tqdm Iprogress ipywidgets python-dotenv tensorboard -q\n","\n","# # restart once you have installed all of the above\n","# !nvidia-smi\n","# !nvcc --version\n","# import torch\n","# print(torch.__version__)          # Should reflect 2.5.0+cu124\n","# print(torch.version.cuda)         # Should output 12.4\n","# print(torch.cuda.is_available())  # Should return True"]},{"cell_type":"markdown","metadata":{},"source":["# Restart again so that all the libraries are properly initialized"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:33.365845Z","iopub.status.busy":"2024-11-04T08:49:33.365559Z","iopub.status.idle":"2024-11-04T08:49:47.786343Z","shell.execute_reply":"2024-11-04T08:49:47.785092Z","shell.execute_reply.started":"2024-11-04T08:49:33.365811Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 1.2: Import Necessary Libraries\n","# ----------------------------- #\n","\n","# General Libraries\n","import os\n","import json\n","import sys\n","import subprocess\n","import argparse\n","import logging\n","import math\n","import random\n","from datetime import datetime\n","import re\n","import gc\n","import weakref\n","import multiprocessing\n","\n","# Torch related\n","import torch\n","from torch import nn\n","import torch.distributed as dist\n","\n","# Transformers related\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForLanguageModeling,\n","    Adafactor\n",")\n","\n","# Huggingface TRL for full finetune\n","from trl import SFTTrainer, SFTConfig\n","\n","# General huggingface libraries\n","import huggingface_hub\n","from datasets import load_dataset, Dataset\n","from accelerate import Accelerator\n","\n","\n","# Unsloth specificic libraries\n","import unsloth\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n","\n","# Other Libraries\n","from peft import LoraConfig\n","import wandb\n","import nltk\n","import spacy\n","# from galore_torch import GaLoreAdamW, GaLoreAdafactor, GaLoreAdamW8bit\n","\n","# Check and import NLTK and spacy modules\n","# Ensure NLTK's punkt tokenizer is available\n","import nltk\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","    print('punkt was already available.')\n","except LookupError:\n","    nltk.download('punkt')\n","    print('punkt was not available. It has been downloaded')\n","\n","# Initialize spaCy English model\n","try:\n","    nlp = spacy.load('en_core_web_sm')\n","    print('en_core_web_sm was already available.')\n","except OSError:\n","    print(\"SpaCy English model not found. Downloading...\")\n","    os.system('python -m spacy download en_core_web_sm')\n","    nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.535557Z","iopub.status.busy":"2024-11-04T08:49:54.535142Z","iopub.status.idle":"2024-11-04T08:49:54.549789Z","shell.execute_reply":"2024-11-04T08:49:54.549048Z","shell.execute_reply.started":"2024-11-04T08:49:54.535509Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 2: Load and Clean the Text Data\n","# ----------------------------- #\n","\n","def load_and_clean_text(file_path):\n","    \"\"\"\n","    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n","    \"\"\"\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    # # Remove Project Gutenberg's license text and headers/footers\n","    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n","\n","    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n","    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n","    return text.strip()\n","\n","# Replace 'psychology_of_unconscious.txt' with your actual file path\n","file_path = '/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'\n","clean_text = load_and_clean_text(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install t"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tiktoken\n","from openai import OpenAI\n","import json\n","import time\n","from typing import List, Dict\n","import numpy as np\n","import os\n","from datetime import datetime\n","\n","class SemanticChunker:\n","    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n","        \"\"\"Initialize the semantic chunker with model configuration\"\"\"\n","        self.client = OpenAI(\n","            base_url=\"http://localhost:8000/v1\",\n","            api_key=\"dummy\"\n","        )\n","        self.model_name = model_name\n","        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n","        self.max_tokens = 3000\n","        \n","        # Set up logging directory with timestamp\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        self.log_dir = f\"/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/Psychology_Of_Unconscious_Mind/chunks_{timestamp}\"\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        \n","        # Set up logging file for processing summary\n","        self.log_file = os.path.join(self.log_dir, \"processing_log.txt\")\n","        \n","    def log_message(self, message: str):\n","        \"\"\"Write log message with timestamp\"\"\"\n","        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        with open(self.log_file, 'a', encoding='utf-8') as f:\n","            f.write(f\"[{timestamp}] {message}\\n\")\n","        print(message)\n","        \n","    def count_tokens(self, text: str) -> int:\n","        \"\"\"Count tokens in text using tiktoken\"\"\"\n","        return len(self.encoding.encode(text))\n","    \n","    def create_initial_chunks(self, text: str) -> List[str]:\n","        \"\"\"Create initial chunks of approximately max_tokens size\"\"\"\n","        paragraphs = text.split('\\n\\n')\n","        chunks = []\n","        current_chunk = []\n","        current_tokens = 0\n","        \n","        for paragraph in paragraphs:\n","            para_tokens = self.count_tokens(paragraph)\n","            \n","            if current_tokens + para_tokens > self.max_tokens:\n","                # Join current chunk and add to chunks\n","                chunks.append('\\n\\n'.join(current_chunk))\n","                current_chunk = [paragraph]\n","                current_tokens = para_tokens\n","            else:\n","                current_chunk.append(paragraph)\n","                current_tokens += para_tokens\n","        \n","        # Add the last chunk if it exists\n","        if current_chunk:\n","            chunks.append('\\n\\n'.join(current_chunk))\n","            \n","        self.log_message(f\"Created {len(chunks)} initial chunks\")\n","        return chunks\n","    \n","    def save_chunk_log(self, chunk_num: int, original_chunk: str, llm_response: str, parsed_sections: List[Dict]):\n","        \"\"\"Save intermediate chunks and responses\"\"\"\n","        log_file = os.path.join(self.log_dir, f\"chunk_{chunk_num:04d}.json\")\n","        log_data = {\n","            \"chunk_number\": chunk_num,\n","            \"original_text\": original_chunk,\n","            \"llm_raw_response\": llm_response,\n","            \"parsed_sections\": parsed_sections,\n","            \"token_count\": self.count_tokens(original_chunk)\n","        }\n","        with open(log_file, 'w', encoding='utf-8') as f:\n","            json.dump(log_data, f, indent=2, ensure_ascii=False)\n","    \n","    def get_semantic_sections(self, chunk: str) -> tuple[List[Dict], str]:\n","        \"\"\"Send chunk to LLM for semantic sectioning with structured JSON output\"\"\"\n","\n","        # Define the JSON schema\n","        schema = {\n","            \"type\": \"object\",\n","            \"properties\": {\n","                \"sections\": {\n","                    \"type\": \"array\",\n","                    \"items\": {\n","                        \"type\": \"object\",\n","                        \"properties\": {\n","                            \"topic\": {\"type\": \"string\"},\n","                            \"content\": {\"type\": \"string\"},\n","                            \"key_concepts\": {\n","                                \"type\": \"array\",\n","                                \"items\": {\"type\": \"string\"}\n","                            }\n","                        },\n","                        \"required\": [\"topic\", \"content\", \"key_concepts\"],\n","                        \"additionalProperties\": False\n","                    }\n","                }\n","            },\n","            \"required\": [\"sections\"],\n","            \"additionalProperties\": False\n","        }\n","\n","        try:\n","            response = self.client.chat.completions.create(\n","                model=self.model_name,\n","                messages=[\n","                    {\n","                        \"role\": \"system\",\n","                        \"content\": \"You are a text analysis expert. Break the given text into coherent sections by topic.\"\n","                    },\n","                    {\n","                        \"role\": \"user\",\n","                        \"content\": f\"Analyze this text and break it into coherent sections:\\n\\n{chunk}\"\n","                    }\n","                ],\n","                max_tokens=self.max_tokens,\n","                temperature=0.2,\n","                response_format={\n","                    \"type\": \"json_schema\",\n","                    \"json_schema\": {\n","                        \"name\": \"text_sections\",\n","                        \"schema\": schema,\n","                        \"strict\": True\n","                    }\n","                }\n","            )\n","            \n","            result = response.choices[0].message.content\n","            \n","            try:\n","                parsed = json.loads(result)\n","                self.log_message(f\"Successfully parsed JSON with {len(parsed['sections'])} sections\")\n","                return parsed['sections'], result\n","            except json.JSONDecodeError as e:\n","                self.log_message(f\"JSON parsing error: {str(e)}\")\n","                self.log_message(f\"Raw response: {result}\")\n","                return [], result\n","                \n","        except Exception as e:\n","            self.log_message(f\"Error in LLM request: {str(e)}\")\n","            return [], str(e)\n","    \n","    def process_text(self, text: str) -> List[Dict]:\n","        \"\"\"Process entire text into semantic sections\"\"\"\n","        self.log_message(\"Starting text processing\")\n","        \n","        # Create initial chunks\n","        initial_chunks = self.create_initial_chunks(text)\n","        \n","        # Process each chunk\n","        all_sections = []\n","        for i, chunk in enumerate(initial_chunks):\n","            self.log_message(f\"Processing chunk {i+1}/{len(initial_chunks)}\")\n","            sections, raw_response = self.get_semantic_sections(chunk)\n","            \n","            # Save intermediate results\n","            self.save_chunk_log(i+1, chunk, raw_response, sections)\n","            \n","            all_sections.extend(sections)\n","            time.sleep(1)  # Rate limiting\n","            \n","        self.log_message(f\"Processing complete. Total sections created: {len(all_sections)}\")\n","        return all_sections\n","\n","    def save_sections(self, sections: List[Dict], output_file: str):\n","        \"\"\"Save processed sections to JSON file\"\"\"\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","            json.dump({'sections': sections}, f, indent=2, ensure_ascii=False)\n","        self.log_message(f\"Saved sections to {output_file}\")\n","\n","def print_chunk_summary(log_dir: str):\n","    \"\"\"Print summary of processed chunks\"\"\"\n","    print(\"\\nChunk Processing Summary:\")\n","    print(\"-\" * 50)\n","    \n","    for file in sorted(os.listdir(log_dir)):\n","        if file.endswith(\".json\") and file != \"processing_log.txt\":\n","            with open(os.path.join(log_dir, file), 'r') as f:\n","                data = json.load(f)\n","                print(f\"\\nChunk {data['chunk_number']}:\")\n","                print(f\"Token count: {data['token_count']}\")\n","                print(f\"Sections created: {len(data['parsed_sections'])}\")\n","                for section in data['parsed_sections']:\n","                    print(f\"- {section['topic']}\")\n","\n","def main():\n","    # Initialize chunker\n","    chunker = SemanticChunker()\n","    \n","    # Read input file\n","    input_file = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    \n","    # Process text\n","    sections = chunker.process_text(text)\n","    \n","    # Save results\n","    output_file = os.path.join(chunker.log_dir, \"semantic_sections.json\")\n","    chunker.save_sections(sections, output_file)\n","    \n","    # Print chunk summary\n","    print_chunk_summary(chunker.log_dir)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[2024-11-10 03:53:05] Starting text processing\n","[2024-11-10 03:53:05] \n","Processing chunk 1\n","[2024-11-10 03:53:05] Starting get_complete_paragraphs with 845623 chars of text\n","[2024-11-10 03:53:05] Found 84 sections\n","[2024-11-10 03:53:05] Processing section 1: SectionType.CONTENT, 4787 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=4787, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:05] Processing section 2: SectionType.HEADER, 9 tokens\n","[2024-11-10 03:53:05] Completed processing: 1 sections included, 83 remaining\n","[2024-11-10 03:53:05] Processed text preview: Psychology of the Unconscious by Jung\n","\n","\n","\n","\n","       AN INTRODUCTION TO PSYCHOANALYSIS AND ANALYTIC PSYCHOLOGY\n","\n","\n","When Professor Freud of Vienna made his early discoveries in the realm\n","of the neuroses, and...\n","[2024-11-10 03:53:05] Created chunk 1 with 4786 tokens\n","[2024-11-10 03:53:05] Chunk 1 preview:\n","Psychology of the Unconscious by Jung\n","\n","\n","\n","\n","       AN INTRODUCTION TO PSYCHOANALYSIS AND ANALYTIC PSYCHOLOGY\n","\n","\n","When Professor Freud of Vienna made his early discoveries in the realm\n","of the neuroses, and...\n","[2024-11-10 03:53:05] \n","Processing chunk 2\n","[2024-11-10 03:53:05] Starting get_complete_paragraphs with 821801 chars of text\n","[2024-11-10 03:53:05] Found 83 sections\n","[2024-11-10 03:53:05] Processing section 1: SectionType.HEADER, 9 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=9, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:05] Processing section 2: SectionType.CONTENT, 8055 tokens\n","[2024-11-10 03:53:05] Completed processing: 1 sections included, 82 remaining\n","[2024-11-10 03:53:05] Processed text preview:                           THE OEDIPUS PROBLEM\n","...\n","[2024-11-10 03:53:05] Created chunk 2 with 9 tokens\n","[2024-11-10 03:53:05] Chunk 2 preview:\n","                          THE OEDIPUS PROBLEM\n","\n","[2024-11-10 03:53:05] \n","Processing chunk 3\n","[2024-11-10 03:53:05] Starting get_complete_paragraphs with 821754 chars of text\n","[2024-11-10 03:53:05] Found 82 sections\n","[2024-11-10 03:53:05] Processing section 1: SectionType.CONTENT, 8055 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=8055, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:05] Processing section 2: SectionType.HEADER, 11 tokens\n","[2024-11-10 03:53:05] Completed processing: 1 sections included, 81 remaining\n","[2024-11-10 03:53:05] Processed text preview: With further investigations into the nature of the repressed complexes a\n","very astonishing situation was revealed. The parental influence on\n","children is something so well recognized and understood that...\n","[2024-11-10 03:53:05] Created chunk 3 with 8055 tokens\n","[2024-11-10 03:53:05] Chunk 3 preview:\n","With further investigations into the nature of the repressed complexes a\n","very astonishing situation was revealed. The parental influence on\n","children is something so well recognized and understood that...\n","[2024-11-10 03:53:05] \n","Processing chunk 4\n","[2024-11-10 03:53:05] Starting get_complete_paragraphs with 782337 chars of text\n","[2024-11-10 03:53:05] Found 81 sections\n","[2024-11-10 03:53:05] Processing section 1: SectionType.HEADER, 11 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=19, max_tokens=3000, sections_processed=2\n","[2024-11-10 03:53:05] Processing section 3: SectionType.HEADER, 5 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=255, max_tokens=3000, sections_processed=4\n","[2024-11-10 03:53:05] Processing section 5: SectionType.HEADER, 8 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=267, max_tokens=3000, sections_processed=6\n","[2024-11-10 03:53:05] Processing section 7: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=271, max_tokens=3000, sections_processed=7\n","[2024-11-10 03:53:05] Processing section 8: SectionType.HEADER, 3 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=283, max_tokens=3000, sections_processed=9\n","[2024-11-10 03:53:05] Processing section 10: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=358, max_tokens=3000, sections_processed=11\n","[2024-11-10 03:53:05] Processing section 12: SectionType.HEADER, 18 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=697, max_tokens=3000, sections_processed=13\n","[2024-11-10 03:53:05] Processing section 14: SectionType.HEADER, 14 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=778, max_tokens=3000, sections_processed=15\n","[2024-11-10 03:53:05] Processing section 16: SectionType.HEADER, 13 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=1016, max_tokens=3000, sections_processed=17\n","[2024-11-10 03:53:05] Processing section 18: SectionType.HEADER, 14 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=1376, max_tokens=3000, sections_processed=19\n","[2024-11-10 03:53:05] Processing section 20: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=1380, max_tokens=3000, sections_processed=20\n","[2024-11-10 03:53:05] Processing section 21: SectionType.HEADER, 14 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=1494, max_tokens=3000, sections_processed=22\n","[2024-11-10 03:53:05] Processing section 23: SectionType.HEADER, 19 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=1705, max_tokens=3000, sections_processed=24\n","[2024-11-10 03:53:05] Processing section 25: SectionType.HEADER, 20 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=2132, max_tokens=3000, sections_processed=26\n","[2024-11-10 03:53:05] Processing section 27: SectionType.HEADER, 17 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=2526, max_tokens=3000, sections_processed=28\n","[2024-11-10 03:53:05] Processing section 29: SectionType.HEADER, 18 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=2544, max_tokens=3000, sections_processed=29\n","[2024-11-10 03:53:05] Processing section 30: SectionType.CONTENT, 504 tokens\n","[2024-11-10 03:53:05] Completed processing: 29 sections included, 52 remaining\n","[2024-11-10 03:53:05] Processed text preview:                                                      BEATRICE M. HINKLE.\n","\n","\n"," =10 Gramercy Park.=\n","\n","                             AUTHOR’S NOTE\n","\n","\n","\n","My task in this work has been to investigate an individua...\n","[2024-11-10 03:53:05] Created chunk 4 with 2543 tokens\n","[2024-11-10 03:53:05] Chunk 4 preview:\n","                                                     BEATRICE M. HINKLE.\n","\n","\n"," =10 Gramercy Park.=\n","\n","                             AUTHOR’S NOTE\n","\n","\n","\n","My task in this work has been to investigate an individua...\n","[2024-11-10 03:53:05] \n","Processing chunk 5\n","[2024-11-10 03:53:05] Starting get_complete_paragraphs with 770236 chars of text\n","[2024-11-10 03:53:05] Found 52 sections\n","[2024-11-10 03:53:05] Processing section 1: SectionType.CONTENT, 504 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=504, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:05] Processing section 2: SectionType.HEADER, 18 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=801, max_tokens=3000, sections_processed=3\n","[2024-11-10 03:53:05] Processing section 4: SectionType.HEADER, 13 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=1468, max_tokens=3000, sections_processed=5\n","[2024-11-10 03:53:05] Processing section 6: SectionType.HEADER, 11 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=2152, max_tokens=3000, sections_processed=7\n","[2024-11-10 03:53:05] Processing section 8: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=2156, max_tokens=3000, sections_processed=8\n","[2024-11-10 03:53:05] Processing section 9: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=2160, max_tokens=3000, sections_processed=9\n","[2024-11-10 03:53:05] Processing section 10: SectionType.CONTENT, 1543 tokens\n","[2024-11-10 03:53:05] Completed processing: 9 sections included, 43 remaining\n","[2024-11-10 03:53:05] Processed text preview:         The crowd as symbol of mystery—The city as symbol of the\n","        mother—The motive of continuous “union”—The typical journey\n","        of the sun-hero—Examples—A longing for rebirth through the\n","...\n","[2024-11-10 03:53:05] Created chunk 5 with 2160 tokens\n","[2024-11-10 03:53:05] Chunk 5 preview:\n","        The crowd as symbol of mystery—The city as symbol of the\n","        mother—The motive of continuous “union”—The typical journey\n","        of the sun-hero—Examples—A longing for rebirth through the\n","...\n","[2024-11-10 03:53:05] \n","Processing chunk 6\n","[2024-11-10 03:53:05] Starting get_complete_paragraphs with 760384 chars of text\n","[2024-11-10 03:53:05] Found 43 sections\n","[2024-11-10 03:53:05] Processing section 1: SectionType.CONTENT, 1543 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=1543, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:05] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=1547, max_tokens=3000, sections_processed=2\n","[2024-11-10 03:53:05] Processing section 3: SectionType.HEADER, 13 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=1560, max_tokens=3000, sections_processed=3\n","[2024-11-10 03:53:05] Processing section 4: SectionType.CONTENT, 12263 tokens\n","[2024-11-10 03:53:05] Completed processing: 3 sections included, 40 remaining\n","[2024-11-10 03:53:05] Processed text preview: \n","Any one who can read Freud’s “Interpretation of the Dream” without\n","scientific rebellion at the newness and apparently unjustified daring of\n","its analytical presentation, and without moral indignation ...\n","[2024-11-10 03:53:05] Created chunk 6 with 1560 tokens\n","[2024-11-10 03:53:05] Chunk 6 preview:\n","\n","Any one who can read Freud’s “Interpretation of the Dream” without\n","scientific rebellion at the newness and apparently unjustified daring of\n","its analytical presentation, and without moral indignation ...\n","[2024-11-10 03:53:05] \n","Processing chunk 7\n","[2024-11-10 03:53:05] Starting get_complete_paragraphs with 753187 chars of text\n","[2024-11-10 03:53:05] Found 40 sections\n","[2024-11-10 03:53:05] Processing section 1: SectionType.CONTENT, 12263 tokens\n","[2024-11-10 03:53:05] After processing: current_tokens=12263, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:05] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:05] Completed processing: 1 sections included, 39 remaining\n","[2024-11-10 03:53:05] Processed text preview: \n","It is a well-known fact that one of the principles of analytic\n","psychology is that the dream images are to be understood symbolically;\n","that is to say, that they are not to be taken literally just as t...\n","[2024-11-10 03:53:05] Created chunk 7 with 12263 tokens\n","[2024-11-10 03:53:05] Chunk 7 preview:\n","\n","It is a well-known fact that one of the principles of analytic\n","psychology is that the dream images are to be understood symbolically;\n","that is to say, that they are not to be taken literally just as t...\n","[2024-11-10 03:53:05] \n","Processing chunk 8\n","[2024-11-10 03:53:05] Starting get_complete_paragraphs with 698132 chars of text\n","[2024-11-10 03:53:06] Found 39 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 9 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=2413, max_tokens=3000, sections_processed=3\n","[2024-11-10 03:53:06] Processing section 4: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=2417, max_tokens=3000, sections_processed=4\n","[2024-11-10 03:53:06] Processing section 5: SectionType.HEADER, 8 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=2425, max_tokens=3000, sections_processed=5\n","[2024-11-10 03:53:06] Processing section 6: SectionType.CONTENT, 14130 tokens\n","[2024-11-10 03:53:06] Completed processing: 5 sections included, 34 remaining\n","[2024-11-10 03:53:06] Processed text preview:                                CHAPTER II\n","\n","\n","                         THE MILLER PHANTASIES\n","\n","\n","\n","We know, from much psychoanalytic experience, that whenever one recounts\n","his phantasies or his dreams, he ...\n","[2024-11-10 03:53:06] Created chunk 8 with 2424 tokens\n","[2024-11-10 03:53:06] Chunk 8 preview:\n","                               CHAPTER II\n","\n","\n","                         THE MILLER PHANTASIES\n","\n","\n","\n","We know, from much psychoanalytic experience, that whenever one recounts\n","his phantasies or his dreams, he ...\n","[2024-11-10 03:53:06] \n","Processing chunk 9\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 687716 chars of text\n","[2024-11-10 03:53:06] Found 34 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.CONTENT, 14130 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=14130, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] Completed processing: 1 sections included, 33 remaining\n","[2024-11-10 03:53:06] Processed text preview: \n","The second chapter in Miss Miller’s work is entitled, “Gloire à Dieu.\n","Poème onirique.”\n","\n","When twenty years of age, Miss Miller took a long journey through\n","Europe. We leave the description of it to her...\n","[2024-11-10 03:53:06] Created chunk 9 with 14130 tokens\n","[2024-11-10 03:53:06] Chunk 9 preview:\n","\n","The second chapter in Miss Miller’s work is entitled, “Gloire à Dieu.\n","Poème onirique.”\n","\n","When twenty years of age, Miss Miller took a long journey through\n","Europe. We leave the description of it to her...\n","[2024-11-10 03:53:06] \n","Processing chunk 10\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 626318 chars of text\n","[2024-11-10 03:53:06] Found 33 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 9 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=13, max_tokens=3000, sections_processed=2\n","[2024-11-10 03:53:06] Processing section 3: SectionType.CONTENT, 16219 tokens\n","[2024-11-10 03:53:06] Completed processing: 2 sections included, 31 remaining\n","[2024-11-10 03:53:06] Processed text preview:                                CHAPTER IV\n","\n","\n","                          THE SONG OF THE MOTH\n","...\n","[2024-11-10 03:53:06] Created chunk 10 with 13 tokens\n","[2024-11-10 03:53:06] Chunk 10 preview:\n","                               CHAPTER IV\n","\n","\n","                          THE SONG OF THE MOTH\n","\n","[2024-11-10 03:53:06] \n","Processing chunk 11\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 626227 chars of text\n","[2024-11-10 03:53:06] Found 31 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.CONTENT, 16219 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=16219, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] Completed processing: 1 sections included, 30 remaining\n","[2024-11-10 03:53:06] Processed text preview: \n","A little later Miss Miller travelled from Geneva to Paris. She says:\n","\n","  “My weariness on the railway was so great that I could hardly sleep an\n","  hour. It was terrifically hot in the ladies’ carriage....\n","[2024-11-10 03:53:06] Created chunk 11 with 16219 tokens\n","[2024-11-10 03:53:06] Chunk 11 preview:\n","\n","A little later Miss Miller travelled from Geneva to Paris. She says:\n","\n","  “My weariness on the railway was so great that I could hardly sleep an\n","  hour. It was terrifically hot in the ladies’ carriage....\n","[2024-11-10 03:53:06] \n","Processing chunk 12\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 565882 chars of text\n","[2024-11-10 03:53:06] Found 30 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=8, max_tokens=3000, sections_processed=2\n","[2024-11-10 03:53:06] Processing section 3: SectionType.HEADER, 9 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=17, max_tokens=3000, sections_processed=3\n","[2024-11-10 03:53:06] Processing section 4: SectionType.CONTENT, 4430 tokens\n","[2024-11-10 03:53:06] Completed processing: 3 sections included, 27 remaining\n","[2024-11-10 03:53:06] Processed text preview:                                 PART II\n","\n","\n","                               CHAPTER I\n","\n","\n","                         ASPECTS OF THE LIBIDO\n","...\n","[2024-11-10 03:53:06] Created chunk 12 with 17 tokens\n","[2024-11-10 03:53:06] Chunk 12 preview:\n","                                PART II\n","\n","\n","                               CHAPTER I\n","\n","\n","                         ASPECTS OF THE LIBIDO\n","\n","[2024-11-10 03:53:06] \n","Processing chunk 13\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 565751 chars of text\n","[2024-11-10 03:53:06] Found 27 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.CONTENT, 4430 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4430, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] Completed processing: 1 sections included, 26 remaining\n","[2024-11-10 03:53:06] Processed text preview: \n","Before I enter upon the contents of this second part, it seems necessary\n","to cast a backward glance over the singular train of thought which the\n","analysis of the poem “The Moth to the Sun” has produced...\n","[2024-11-10 03:53:06] Created chunk 13 with 4430 tokens\n","[2024-11-10 03:53:06] Chunk 13 preview:\n","\n","Before I enter upon the contents of this second part, it seems necessary\n","to cast a backward glance over the singular train of thought which the\n","analysis of the poem “The Moth to the Sun” has produced...\n","[2024-11-10 03:53:06] \n","Processing chunk 14\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 548560 chars of text\n","[2024-11-10 03:53:06] Found 26 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 14 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=18, max_tokens=3000, sections_processed=2\n","[2024-11-10 03:53:06] Processing section 3: SectionType.CONTENT, 6624 tokens\n","[2024-11-10 03:53:06] Completed processing: 2 sections included, 24 remaining\n","[2024-11-10 03:53:06] Processed text preview:                                CHAPTER II\n","\n","\n","            THE CONCEPTION AND THE GENETIC THEORY OF LIBIDO\n","...\n","[2024-11-10 03:53:06] Created chunk 14 with 18 tokens\n","[2024-11-10 03:53:06] Chunk 14 preview:\n","                               CHAPTER II\n","\n","\n","            THE CONCEPTION AND THE GENETIC THEORY OF LIBIDO\n","\n","[2024-11-10 03:53:06] \n","Processing chunk 15\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 548456 chars of text\n","[2024-11-10 03:53:06] Found 24 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.CONTENT, 6624 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=6624, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] Completed processing: 1 sections included, 23 remaining\n","[2024-11-10 03:53:06] Processed text preview: \n","The chief source of the history of the analytic conception of libido is\n","Freud’s “Three Contributions to the Sexual Theory.” There the term\n","libido is conceived by him in the original narrow sense of s...\n","[2024-11-10 03:53:06] Created chunk 15 with 6624 tokens\n","[2024-11-10 03:53:06] Chunk 15 preview:\n","\n","The chief source of the history of the analytic conception of libido is\n","Freud’s “Three Contributions to the Sexual Theory.” There the term\n","libido is conceived by him in the original narrow sense of s...\n","[2024-11-10 03:53:06] \n","Processing chunk 16\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 518390 chars of text\n","[2024-11-10 03:53:06] Found 23 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=22, max_tokens=3000, sections_processed=2\n","[2024-11-10 03:53:06] Processing section 3: SectionType.HEADER, 5 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=27, max_tokens=3000, sections_processed=3\n","[2024-11-10 03:53:06] Processing section 4: SectionType.CONTENT, 12988 tokens\n","[2024-11-10 03:53:06] Completed processing: 3 sections included, 20 remaining\n","[2024-11-10 03:53:06] Processed text preview:                               CHAPTER III\n","\n","\n"," THE TRANSFORMATION OF THE LIBIDO. A POSSIBLE SOURCE OF PRIMITIVE HUMAN\n","\n","                              DISCOVERIES\n","...\n","[2024-11-10 03:53:06] Created chunk 16 with 27 tokens\n","[2024-11-10 03:53:06] Chunk 16 preview:\n","                              CHAPTER III\n","\n","\n"," THE TRANSFORMATION OF THE LIBIDO. A POSSIBLE SOURCE OF PRIMITIVE HUMAN\n","\n","                              DISCOVERIES\n","\n","[2024-11-10 03:53:06] \n","Processing chunk 17\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 518231 chars of text\n","[2024-11-10 03:53:06] Found 20 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.CONTENT, 12988 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=12988, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] Completed processing: 1 sections included, 19 remaining\n","[2024-11-10 03:53:06] Processed text preview: \n","In the following pages I will endeavor to picture a concrete example of\n","the transition of the libido. I once treated a patient who suffered from\n","a depressive catatonic condition. The case was one of ...\n","[2024-11-10 03:53:06] Created chunk 17 with 12988 tokens\n","[2024-11-10 03:53:06] Chunk 17 preview:\n","\n","In the following pages I will endeavor to picture a concrete example of\n","the transition of the libido. I once treated a patient who suffered from\n","a depressive catatonic condition. The case was one of ...\n","[2024-11-10 03:53:06] \n","Processing chunk 18\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 464267 chars of text\n","[2024-11-10 03:53:06] Found 19 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 12 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=16, max_tokens=3000, sections_processed=2\n","[2024-11-10 03:53:06] Processing section 3: SectionType.CONTENT, 16122 tokens\n","[2024-11-10 03:53:06] Completed processing: 2 sections included, 17 remaining\n","[2024-11-10 03:53:06] Processed text preview:                                CHAPTER IV\n","\n","\n","                   THE UNCONSCIOUS ORIGIN OF THE HERO\n","...\n","[2024-11-10 03:53:06] Created chunk 18 with 16 tokens\n","[2024-11-10 03:53:06] Chunk 18 preview:\n","                               CHAPTER IV\n","\n","\n","                   THE UNCONSCIOUS ORIGIN OF THE HERO\n","\n","[2024-11-10 03:53:06] \n","Processing chunk 19\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 464169 chars of text\n","[2024-11-10 03:53:06] Found 17 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.CONTENT, 16122 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=16122, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] Completed processing: 1 sections included, 16 remaining\n","[2024-11-10 03:53:06] Processed text preview: \n","Prepared by the previous chapters, we approach the personification of\n","the libido in the form of a conqueror, a hero or a demon. With this,\n","symbolism leaves the impersonal and neuter realm, which char...\n","[2024-11-10 03:53:06] Created chunk 19 with 16122 tokens\n","[2024-11-10 03:53:06] Chunk 19 preview:\n","\n","Prepared by the previous chapters, we approach the personification of\n","the libido in the form of a conqueror, a hero or a demon. With this,\n","symbolism leaves the impersonal and neuter realm, which char...\n","[2024-11-10 03:53:06] \n","Processing chunk 20\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 396010 chars of text\n","[2024-11-10 03:53:06] Found 16 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 13 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=2976, max_tokens=3000, sections_processed=3\n","[2024-11-10 03:53:06] Processing section 4: SectionType.HEADER, 3 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=2979, max_tokens=3000, sections_processed=4\n","[2024-11-10 03:53:06] Processing section 5: SectionType.CONTENT, 26046 tokens\n","[2024-11-10 03:53:06] Completed processing: 4 sections included, 12 remaining\n","[2024-11-10 03:53:06] Processed text preview:                                CHAPTER V\n","\n","\n","                 SYMBOLISM OF THE MOTHER AND OF REBIRTH\n","\n","\n","\n","The vision following the creation of the hero is described by Miss\n","Miller as a “throng of people.”...\n","[2024-11-10 03:53:06] Created chunk 20 with 2978 tokens\n","[2024-11-10 03:53:06] Chunk 20 preview:\n","                               CHAPTER V\n","\n","\n","                 SYMBOLISM OF THE MOTHER AND OF REBIRTH\n","\n","\n","\n","The vision following the creation of the hero is described by Miss\n","Miller as a “throng of people.”...\n","[2024-11-10 03:53:06] \n","Processing chunk 21\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 383840 chars of text\n","[2024-11-10 03:53:06] Found 12 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.CONTENT, 26046 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=26046, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] Completed processing: 1 sections included, 11 remaining\n","[2024-11-10 03:53:06] Processed text preview:   (1) “Stand fast therefore in the liberty wherewith Christ has made us\n","  free.”\n","\n","The Christians are the children of the City Above, a symbol of the\n","mother, not sons of the earthly city-mother, who is...\n","[2024-11-10 03:53:06] Created chunk 21 with 26046 tokens\n","[2024-11-10 03:53:06] Chunk 21 preview:\n","  (1) “Stand fast therefore in the liberty wherewith Christ has made us\n","  free.”\n","\n","The Christians are the children of the City Above, a symbol of the\n","mother, not sons of the earthly city-mother, who is...\n","[2024-11-10 03:53:06] \n","Processing chunk 22\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 278077 chars of text\n","[2024-11-10 03:53:06] Found 11 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 13 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=400, max_tokens=3000, sections_processed=3\n","[2024-11-10 03:53:06] Processing section 4: SectionType.HEADER, 16 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=416, max_tokens=3000, sections_processed=4\n","[2024-11-10 03:53:06] Processing section 5: SectionType.CONTENT, 11971 tokens\n","[2024-11-10 03:53:06] Completed processing: 4 sections included, 7 remaining\n","[2024-11-10 03:53:06] Processed text preview:                                CHAPTER VI\n","\n","\n","               THE BATTLE FOR DELIVERANCE FROM THE MOTHER\n","\n","\n","\n","There now comes a pause in the production of visions by Miss Miller;\n","then the activity of the u...\n","[2024-11-10 03:53:06] Created chunk 22 with 415 tokens\n","[2024-11-10 03:53:06] Chunk 22 preview:\n","                               CHAPTER VI\n","\n","\n","               THE BATTLE FOR DELIVERANCE FROM THE MOTHER\n","\n","\n","\n","There now comes a pause in the production of visions by Miss Miller;\n","then the activity of the u...\n","[2024-11-10 03:53:06] \n","Processing chunk 23\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 276209 chars of text\n","[2024-11-10 03:53:06] Found 7 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.CONTENT, 11971 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=11971, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] Completed processing: 1 sections included, 6 remaining\n","[2024-11-10 03:53:06] Processed text preview: death as the hero, and is even called “faithful brother” by the latter.\n","These allusions point to a remarkable similarity between horse and\n","rider. There seems to exist an intimate connection between th...\n","[2024-11-10 03:53:06] Created chunk 23 with 11971 tokens\n","[2024-11-10 03:53:06] Chunk 23 preview:\n","death as the hero, and is even called “faithful brother” by the latter.\n","These allusions point to a remarkable similarity between horse and\n","rider. There seems to exist an intimate connection between th...\n","[2024-11-10 03:53:06] \n","Processing chunk 24\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 225464 chars of text\n","[2024-11-10 03:53:06] Found 6 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 11 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=15, max_tokens=3000, sections_processed=2\n","[2024-11-10 03:53:06] Processing section 3: SectionType.CONTENT, 33161 tokens\n","[2024-11-10 03:53:06] Completed processing: 2 sections included, 4 remaining\n","[2024-11-10 03:53:06] Processed text preview:                               CHAPTER VII\n","\n","\n","                          THE DUAL MOTHER RÔLE\n","...\n","[2024-11-10 03:53:06] Created chunk 24 with 15 tokens\n","[2024-11-10 03:53:06] Chunk 24 preview:\n","                              CHAPTER VII\n","\n","\n","                          THE DUAL MOTHER RÔLE\n","\n","[2024-11-10 03:53:06] \n","Processing chunk 25\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 225373 chars of text\n","[2024-11-10 03:53:06] Found 4 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.CONTENT, 33161 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=33161, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] Completed processing: 1 sections included, 3 remaining\n","[2024-11-10 03:53:06] Processed text preview: \n","After the disappearance of the assailant, Chiwantopel begins the\n","following monologue:\n","\n","  “From the extreme ends of these continents, from the farthest\n","  lowlands, after having forsaken the palace of ...\n","[2024-11-10 03:53:06] Created chunk 25 with 33161 tokens\n","[2024-11-10 03:53:06] Chunk 25 preview:\n","\n","After the disappearance of the assailant, Chiwantopel begins the\n","following monologue:\n","\n","  “From the extreme ends of these continents, from the farthest\n","  lowlands, after having forsaken the palace of ...\n","[2024-11-10 03:53:06] \n","Processing chunk 26\n","[2024-11-10 03:53:06] Starting get_complete_paragraphs with 88787 chars of text\n","[2024-11-10 03:53:06] Found 3 sections\n","[2024-11-10 03:53:06] Processing section 1: SectionType.HEADER, 4 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=4, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:06] Processing section 2: SectionType.HEADER, 7 tokens\n","[2024-11-10 03:53:06] After processing: current_tokens=11, max_tokens=3000, sections_processed=2\n","[2024-11-10 03:53:07] Processing section 3: SectionType.CONTENT, 20667 tokens\n","[2024-11-10 03:53:07] Completed processing: 2 sections included, 1 remaining\n","[2024-11-10 03:53:07] Processed text preview:                               CHAPTER VIII\n","\n","\n","                             THE SACRIFICE\n","...\n","[2024-11-10 03:53:07] Created chunk 26 with 11 tokens\n","[2024-11-10 03:53:07] Chunk 26 preview:\n","                              CHAPTER VIII\n","\n","\n","                             THE SACRIFICE\n","\n","[2024-11-10 03:53:07] \n","Processing chunk 27\n","[2024-11-10 03:53:07] Starting get_complete_paragraphs with 88699 chars of text\n","[2024-11-10 03:53:07] Found 1 sections\n","[2024-11-10 03:53:07] Processing section 1: SectionType.CONTENT, 20667 tokens\n","[2024-11-10 03:53:07] After processing: current_tokens=20667, max_tokens=3000, sections_processed=1\n","[2024-11-10 03:53:07] Completed processing: 1 sections included, 0 remaining\n","[2024-11-10 03:53:07] Processed text preview: \n","After this long digression, let us return to Miss Miller’s vision. We\n","can now answer the question as to the significance of Siegfried’s\n","longing for Brunhilde. It is the striving of the libido away _f...\n","[2024-11-10 03:53:07] Created chunk 27 with 20667 tokens\n","[2024-11-10 03:53:07] Chunk 27 preview:\n","\n","After this long digression, let us return to Miss Miller’s vision. We\n","can now answer the question as to the significance of Siegfried’s\n","longing for Brunhilde. It is the striving of the libido away _f...\n","[2024-11-10 03:53:07] Created 27 initial chunks\n","[2024-11-10 03:53:07] Processing limited to first 3 chunks\n","[2024-11-10 03:53:07] Processing chunk 1/3\n","[2024-11-10 03:53:07] Sending request to LLM (input tokens: 4786)\n","[2024-11-10 03:53:08] Error in LLM request: Connection error.\n","\n","====================================================================================================\n","INPUT CHUNK\n","====================================================================================================\n","Chunk 1 (Tokens: 4786)\n","Content preview:\n","Psychology of the Unconscious by Jung\n","\n","\n","\n","\n","       AN INTRODUCTION TO PSYCHOANALYSIS AND ANALYTIC PSYCHOLOGY\n","\n","\n","When Professor Freud of Vienna made his early discoveries in the realm\n","of the neuroses, and announced that the basis and origin of the various\n","symptoms grouped under the terms hysteria and neuroses lay in\n","unfulfilled desires and wishes, unexpressed and unknown to the patient\n","for the most part, and concerned chiefly with the sexual instinct, it\n","was not realized what far-reaching influence this unpopular and bitterly\n","attacked theory would exert on the understanding of human life in\n","general.\n","\n","For this theory has so widened in its scope that its application has now\n","extended beyond a particular group of pathologic states. It has in fact\n","led to a new evaluation of the whole conduct of human life; a new\n","comprehension has developed which explains those things which formerly\n","were unexplained, and there is offered an understanding not only of the\n","symptoms of a neurosis and the phenomena o...\n","\n","====================================================================================================\n","SEMANTIC SECTIONS\n","====================================================================================================\n","\n","====================================================================================================\n","METRICS\n","====================================================================================================\n","{}\n","[2024-11-10 03:53:09] Processing chunk 2/3\n","[2024-11-10 03:53:09] Sending request to LLM (input tokens: 9)\n","[2024-11-10 03:53:10] Error in LLM request: Connection error.\n","\n","====================================================================================================\n","INPUT CHUNK\n","====================================================================================================\n","Chunk 2 (Tokens: 9)\n","Content preview:\n","                          THE OEDIPUS PROBLEM\n","\n","\n","====================================================================================================\n","SEMANTIC SECTIONS\n","====================================================================================================\n","\n","====================================================================================================\n","METRICS\n","====================================================================================================\n","{}\n","[2024-11-10 03:53:11] Processing chunk 3/3\n","[2024-11-10 03:53:11] Sending request to LLM (input tokens: 8055)\n","[2024-11-10 03:53:12] Error in LLM request: Connection error.\n","\n","====================================================================================================\n","INPUT CHUNK\n","====================================================================================================\n","Chunk 3 (Tokens: 8055)\n","Content preview:\n","With further investigations into the nature of the repressed complexes a\n","very astonishing situation was revealed. The parental influence on\n","children is something so well recognized and understood that to call\n","attention to it sounds much like a banality. However, here an\n","extraordinary discovery was made, for in tracing out the feelings and\n","emotions of adults it became evident that this influence was paramount\n","not only for children but for adults as well; that the entire direction\n","of lives was largely determined quite unconsciously by the parental\n","associations, and that, although adults, the emotional side of their\n","nature was still infantile in type and demanded unconsciously the\n","infantile or childish relations.\n","\n","Freud traces out the commencement of the infantile attachment for the\n","parents in this wise.\n","\n","In the beginning the child derives its first satisfaction and pleasure\n","from the mother in the form of nutrition and care for its wants. In this\n","first act of suckling Freud sees already a...\n","\n","====================================================================================================\n","SEMANTIC SECTIONS\n","====================================================================================================\n","\n","====================================================================================================\n","METRICS\n","====================================================================================================\n","{}\n","[2024-11-10 03:53:13] Processing complete. Total semantic chunks created: 0\n","[2024-11-10 03:53:13] Saved 0 semantic chunks to /home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/Psychology_Of_Unconscious_Mind/chunks_20241110_035305/semantic_chunks\n"]}],"source":["import tiktoken\n","from openai import OpenAI\n","import json\n","import time\n","from typing import List, Dict, Tuple, Optional\n","import numpy as np\n","import os\n","from datetime import datetime\n","from pprint import pprint\n","import re\n","from dataclasses import dataclass\n","from enum import Enum\n","\n","# Add these new data structures after imports\n","class SectionType(Enum):\n","    HEADER = \"header\"\n","    CONTENT = \"content\"\n","    QUOTE = \"quote\"\n","    ATTRIBUTION = \"attribution\"\n","    LIST = \"list\"\n","    FRONT_MATTER = \"front_matter\"\n","    TABLE_OF_CONTENTS = \"table_of_contents\"\n","    \n","@dataclass\n","class Section:\n","    text: str\n","    type: SectionType\n","    level: int = 0\n","    metadata: Dict = None\n","    \n","\n","class SemanticChunker:\n","    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n","        \"\"\"Initialize the semantic chunker with model configuration\"\"\"\n","        self.client = OpenAI(\n","            base_url=\"http://localhost:8000/v1\",\n","            api_key=\"dummy\"\n","        )\n","        self.model_name = model_name\n","        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n","        self.max_tokens = 3000\n","        \n","        # Set up logging directory with timestamp\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        self.log_dir = f\"/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/Psychology_Of_Unconscious_Mind/chunks_{timestamp}\"\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        \n","        # Set up logging file for processing summary\n","        self.log_file = os.path.join(self.log_dir, \"processing_log.txt\")\n","        \n","        # Initialize state variables\n","        self.missed_text = \"\"  # Store text not included in LLM output\n","        \n","    def log_message(self, message: str):\n","        \"\"\"Write log message with timestamp and print to console\"\"\"\n","        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        log_entry = f\"[{timestamp}] {message}\"\n","        with open(self.log_file, 'a', encoding='utf-8') as f:\n","            f.write(log_entry + \"\\n\")\n","        print(log_entry)\n","    \n","    def print_separator(self, message: str = \"\"):\n","        \"\"\"Print a separator line with optional message\"\"\"\n","        print(f\"\\n{'='*100}\")\n","        if message:\n","            print(f\"{message}\")\n","            print('='*100)\n","    \n","    def count_tokens(self, text: str) -> int:\n","        \"\"\"Count tokens in text using tiktoken\"\"\"\n","        return len(self.encoding.encode(text))\n","    \n","    def find_chapter_breaks(self, text: str) -> List[int]:\n","        \"\"\"Find indices where chapters begin (centered headings)\"\"\"\n","        lines = text.split('\\n')\n","        chapter_breaks = []\n","        \n","        for i, line in enumerate(lines):\n","            if self.is_chapter_heading(line):\n","                chapter_breaks.append(i)\n","        \n","        return chapter_breaks\n","    \n","    def is_chapter_heading(self, text: str) -> Tuple[bool, int]:\n","        \"\"\"\n","        Enhanced chapter heading detection with level identification.\n","        Returns (is_heading, level).\n","        \"\"\"\n","        text = text.strip()\n","        if not text:\n","            return False, 0\n","            \n","        # Chapter patterns\n","        chapter_patterns = [\n","            (r'^CHAPTER\\s+[IVXL]+', 1),  # Main chapter headers\n","            (r'^[IVX]+\\.\\s*—\\s*', 2),    # Sub-chapter headers\n","            (r'^\\d+\\.\\s*—\\s*', 2),       # Numbered sections\n","        ]\n","        \n","        for pattern, level in chapter_patterns:\n","            if re.match(pattern, text, re.I):\n","                return True, level\n","        \n","        # Check for centered text formatting\n","        line_length = len(text)\n","        leading_spaces = len(text) - len(text.lstrip())\n","        trailing_spaces = len(text) - len(text.rstrip())\n","        \n","        is_centered = abs(leading_spaces - trailing_spaces) <= 2 and leading_spaces > 5\n","        is_caps = text.isupper()\n","        reasonable_length = 10 < len(text.strip()) < 100\n","        \n","        if is_centered:\n","            if is_caps and reasonable_length:\n","                return True, 1  # Main header\n","            elif reasonable_length:\n","                return True, 2  # Sub header\n","                \n","        return False, 0\n","    \n","    def analyze_text_structure(self, text: str) -> List[Section]:\n","        \"\"\"\n","        Enhanced text structure analysis with better header and spacing detection.\n","        \"\"\"\n","        sections = []\n","        lines = text.split('\\n')\n","        current_section = []\n","        current_type = None\n","        current_level = 0\n","        \n","        def flush_section():\n","            nonlocal current_section, current_type\n","            if current_section:\n","                # Skip empty sections\n","                content = '\\n'.join(current_section).strip()\n","                if content:  # Only create section if there's actual content\n","                    sections.append(Section(\n","                        text='\\n'.join(current_section),\n","                        type=current_type or SectionType.CONTENT,\n","                        level=current_level\n","                    ))\n","                current_section = []\n","                current_type = None\n","        \n","        in_toc = False\n","        in_front_matter = False\n","        \n","        i = 0\n","        while i < len(lines):\n","            line = lines[i]\n","            next_line = lines[i + 1] if i + 1 < len(lines) else \"\"\n","            \n","            # Detect centered headers\n","            if line.strip() and line.strip().isupper():\n","                leading_spaces = len(line) - len(line.lstrip())\n","                if leading_spaces > 10:  # Likely centered\n","                    flush_section()\n","                    current_type = SectionType.HEADER\n","                    current_level = 1\n","                    current_section = [line]\n","                    if not next_line.strip():  # Include following blank line\n","                        current_section.append(next_line)\n","                        i += 1\n","                    flush_section()\n","                    i += 1\n","                    continue\n","            \n","            # Detect Table of Contents\n","            if re.match(r'^\\s*CONTENTS\\s*$', line, re.I):\n","                flush_section()\n","                in_toc = True\n","                current_type = SectionType.TABLE_OF_CONTENTS\n","                current_section = [line]\n","                if not next_line.strip():  # Include following blank line\n","                    current_section.append(next_line)\n","                    i += 1\n","                i += 1\n","                continue\n","            \n","            # Detect Author's Note\n","            if re.match(r'^\\s*AUTHOR\\'S\\s+NOTE\\s*$', line, re.I):\n","                flush_section()\n","                in_front_matter = True\n","                current_type = SectionType.FRONT_MATTER\n","                current_section = [line]\n","                if not next_line.strip():  # Include following blank line\n","                    current_section.append(next_line)\n","                    i += 1\n","                i += 1\n","                continue\n","            \n","            # Detect chapter headings\n","            is_heading, level = self.is_chapter_heading(line)\n","            if is_heading:\n","                flush_section()\n","                current_type = SectionType.HEADER\n","                current_level = level\n","                current_section = [line]\n","                if not next_line.strip():  # Include following blank line\n","                    current_section.append(next_line)\n","                    i += 1\n","                flush_section()\n","                i += 1\n","                continue\n","            \n","            # Handle section content\n","            if in_toc:\n","                if not line.strip() and not next_line.strip():\n","                    in_toc = False\n","                    flush_section()\n","                else:\n","                    current_section.append(line)\n","            elif in_front_matter:\n","                if not line.strip() and not next_line.strip():\n","                    in_front_matter = False\n","                    flush_section()\n","                else:\n","                    current_section.append(line)\n","            else:\n","                current_section.append(line)\n","            \n","            i += 1\n","        \n","        flush_section()  # Flush any remaining content\n","        \n","        # Filter out empty sections and preserve correct spacing\n","        filtered_sections = []\n","        for section in sections:\n","            if section.text.strip():\n","                filtered_sections.append(section)\n","        \n","        return filtered_sections\n","    \n","    def verify_output_completeness(self, input_text: str, output_sections: List[str]) -> str:\n","        \"\"\"Verify all input text is present in output sections and return missing text\"\"\"\n","        # Normalize texts for comparison\n","        input_normalized = ' '.join(input_text.split())\n","        output_normalized = ' '.join(' '.join(output_sections).split())\n","        \n","        # Find missing content\n","        words = input_normalized.split()\n","        window_size = 5  # Look for sequences of 5 words\n","        \n","        missing_sequences = []\n","        i = 0\n","        while i < len(words) - window_size:\n","            sequence = ' '.join(words[i:i+window_size])\n","            if sequence not in output_normalized:\n","                # Find complete missing phrase\n","                start = i\n","                while start > 0 and ' '.join(words[start-1:i+window_size]) not in output_normalized:\n","                    start -= 1\n","                end = i + window_size\n","                while end < len(words) and ' '.join(words[i:end+1]) not in output_normalized:\n","                    end += 1\n","                missing_sequences.append(' '.join(words[start:end]))\n","                i = end\n","            else:\n","                i += 1\n","        \n","        return '\\n'.join(missing_sequences) if missing_sequences else \"\"\n","    \n","    def create_initial_chunks(self, text: str) -> List[str]:\n","        \"\"\"\n","        Create initial chunks with enhanced logging.\n","        \"\"\"\n","        chunks = []\n","        remaining_text = text\n","        chunk_number = 0\n","        \n","        while remaining_text.strip():\n","            chunk_number += 1\n","            self.log_message(f\"\\nProcessing chunk {chunk_number}\")\n","            \n","            # Add any missed text from previous chunk\n","            if self.missed_text:\n","                self.log_message(\"Adding missed text from previous chunk\")\n","                remaining_text = self.missed_text + '\\n\\n' + remaining_text\n","                self.missed_text = \"\"\n","            \n","            # Get complete paragraphs up to token limit\n","            chunk_text, remaining_text = self.get_complete_paragraphs(remaining_text, self.max_tokens)\n","            \n","            if chunk_text.strip():\n","                self.log_message(f\"Created chunk {chunk_number} with {self.count_tokens(chunk_text)} tokens\")\n","                chunks.append(chunk_text)\n","                \n","                # Debug output\n","                preview = chunk_text[:200] + \"...\" if len(chunk_text) > 200 else chunk_text\n","                self.log_message(f\"Chunk {chunk_number} preview:\\n{preview}\")\n","            else:\n","                self.log_message(\"Warning: Empty chunk produced\")\n","                if not remaining_text.strip():\n","                    break\n","            \n","            if len(chunks) >= 100:  # Safety limit\n","                self.log_message(\"Warning: Maximum chunk limit reached\")\n","                break\n","        \n","        self.log_message(f\"Created {len(chunks)} initial chunks\")\n","        \n","        # Save the chunks\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        for i, chunk in enumerate(chunks):\n","            with open(os.path.join(self.log_dir, f\"chunk_{i+1:04d}.txt\"), 'w', encoding='utf-8') as f:\n","                f.write(chunk)\n","                \n","        return chunks\n","    \n","    def get_semantic_sections(self, chunk: str) -> Tuple[List[str], Dict]:\n","        \"\"\"Update the system prompt for better structural preservation.\"\"\"\n","        try:\n","            self.log_message(f\"Sending request to LLM (input tokens: {self.count_tokens(chunk)})\")\n","            \n","            # Add timeout to the request\n","            response = self.client.chat.completions.create(\n","                model=self.model_name,\n","                messages=[\n","                    {\n","                        \"role\": \"system\",\n","                        \"content\": \"\"\"You are a text analysis expert. Your task is to:\n","                        1. Maintain the original document structure (headers, lists, quotes)\n","                        2. Split the input text into coherent semantic sections\n","                        3. Each section must respect structural boundaries\n","                        4. Use <START_SECTION> and <END_SECTION> to mark sections\n","                        5. Include ALL text from the input - do not skip any content\n","                        6. Preserve ALL formatting, indentation, and special characters\n","                        7. If there's a header, keep it with its content\n","                        8. Keep lists and quotes intact within their sections\n","                        9. If a section would be incomplete, mark it with <INCOMPLETE> tags\"\"\"\n","                    },\n","                    {\n","                        \"role\": \"user\",\n","                        \"content\": f\"Split this text into coherent sections, preserving ALL content and structure:\\n\\n{chunk}\"\n","                    }\n","                ],\n","                max_tokens=self.max_tokens,\n","                temperature=0.2,\n","                timeout=30  # Add 30 second timeout\n","            )\n","            \n","            result = response.choices[0].message.content\n","            \n","            # Extract sections\n","            sections = []\n","            section_pattern = r'<START_SECTION>(.*?)<END_SECTION>'\n","            for match in re.finditer(section_pattern, result, re.DOTALL):\n","                section_text = match.group(1).strip()\n","                if section_text and len(section_text) > 50:  # Ignore empty or very short sections\n","                    sections.append(section_text)\n","            \n","            # Check for incomplete section\n","            incomplete_pattern = r'<INCOMPLETE>(.*?)</INCOMPLETE>'\n","            incomplete_match = re.search(incomplete_pattern, result, re.DOTALL)\n","            if incomplete_match:\n","                incomplete_text = incomplete_match.group(1).strip()\n","                if incomplete_text:\n","                    self.missed_text = incomplete_text\n","                    self.log_message(f\"Found incomplete section ({self.count_tokens(incomplete_text)} tokens)\")\n","            \n","            # Verify all content is included\n","            if not incomplete_match:  # Only check if no explicit incomplete section\n","                missed_text = self.verify_output_completeness(chunk, sections)\n","                if missed_text:\n","                    self.missed_text = missed_text\n","                    self.log_message(f\"Found missed text ({self.count_tokens(missed_text)} tokens)\")\n","            \n","            metrics = {\n","                \"completion_tokens\": response.usage.completion_tokens,\n","                \"prompt_tokens\": response.usage.prompt_tokens,\n","                \"total_tokens\": response.usage.total_tokens,\n","                \"finish_reason\": response.choices[0].finish_reason,\n","                \"sections_created\": len(sections),\n","                \"has_missed_text\": bool(self.missed_text)\n","            }\n","            \n","            return sections, metrics\n","                \n","        except Exception as e:\n","            self.log_message(f\"Error in LLM request: {str(e)}\")\n","            return [], {}\n","\n","    def get_complete_paragraphs(self, text: str, max_tokens: int) -> Tuple[str, str]:\n","        \"\"\"\n","        Enhanced version with corrected content processing logic.\n","        \"\"\"\n","        self.log_message(f\"Starting get_complete_paragraphs with {len(text)} chars of text\")\n","        \n","        sections = self.analyze_text_structure(text)\n","        self.log_message(f\"Found {len(sections)} sections\")\n","        \n","        current_sections = []\n","        current_tokens = 0\n","        section_index = 0\n","        \n","        try:\n","            while section_index < len(sections):\n","                section = sections[section_index]\n","                section_tokens = self.count_tokens(section.text)\n","                \n","                self.log_message(f\"Processing section {section_index + 1}: {section.type}, {section_tokens} tokens\")\n","                \n","                # If this section would exceed our token limit\n","                if current_tokens + section_tokens > max_tokens:\n","                    if current_sections:  # Only break if we have content\n","                        break\n","                \n","                # Always include header with its following content\n","                if section.type == SectionType.HEADER:\n","                    # Add the header\n","                    current_sections.append(section)\n","                    current_tokens += section_tokens\n","                    \n","                    # Look ahead for content\n","                    next_index = section_index + 1\n","                    if next_index < len(sections) and sections[next_index].type == SectionType.CONTENT:\n","                        next_section = sections[next_index]\n","                        next_tokens = self.count_tokens(next_section.text)\n","                        if current_tokens + next_tokens <= max_tokens:\n","                            current_sections.append(next_section)\n","                            current_tokens += next_tokens\n","                            section_index += 1  # Skip the content section in next iteration\n","                    \n","                # Handle content sections not attached to headers\n","                elif section.type == SectionType.CONTENT:\n","                    current_sections.append(section)\n","                    current_tokens += section_tokens\n","                \n","                # Handle other section types (TABLE_OF_CONTENTS, etc.)\n","                else:\n","                    current_sections.append(section)\n","                    current_tokens += section_tokens\n","                \n","                section_index += 1\n","                self.log_message(f\"After processing: current_tokens={current_tokens}, max_tokens={max_tokens}, sections_processed={len(current_sections)}\")\n","            \n","            # Combine sections with proper spacing\n","            processed_sections = []\n","            for i, section in enumerate(current_sections):\n","                # Add extra newline before sections (except the first one)\n","                if i > 0:\n","                    processed_sections.append(\"\")\n","                \n","                # Add the section text\n","                processed_sections.append(section.text.rstrip())\n","                \n","                # Add extra newline after headers\n","                if section.type == SectionType.HEADER:\n","                    processed_sections.append(\"\")\n","            \n","            processed_text = \"\\n\".join(processed_sections)\n","            \n","            # Prepare remaining sections\n","            remaining_sections = []\n","            if section_index < len(sections):\n","                for section in sections[section_index:]:\n","                    if remaining_sections:\n","                        remaining_sections.append(\"\")\n","                    remaining_sections.append(section.text.rstrip())\n","            \n","            remaining_text = \"\\n\".join(remaining_sections) if remaining_sections else \"\"\n","            \n","            self.log_message(f\"Completed processing: {len(current_sections)} sections included, {len(sections) - section_index} remaining\")\n","            self.log_message(f\"Processed text preview: {processed_text[:200]}...\")\n","            \n","            return processed_text, remaining_text\n","            \n","        except Exception as e:\n","            self.log_message(f\"Error in get_complete_paragraphs: {str(e)}\")\n","            if current_sections:\n","                return \"\\n\".join([s.text for s in current_sections]), text\n","            return \"\", text\n","\n","    def process_text(self, text: str, max_chunks: int = None) -> List[str]:\n","        \"\"\"Process entire text into semantic sections with enhanced logging\"\"\"\n","        self.log_message(\"Starting text processing\")\n","        \n","        # Create initial chunks\n","        initial_chunks = self.create_initial_chunks(text)\n","        \n","        if max_chunks:\n","            initial_chunks = initial_chunks[:max_chunks]\n","            self.log_message(f\"Processing limited to first {max_chunks} chunks\")\n","        \n","        # Process each chunk\n","        semantic_chunks = []\n","        for i, chunk in enumerate(initial_chunks):\n","            self.log_message(f\"Processing chunk {i+1}/{len(initial_chunks)}\")\n","            \n","            # Get semantic sections\n","            sections, metrics = self.get_semantic_sections(chunk)\n","            \n","            # Print processing details\n","            self.print_separator(\"INPUT CHUNK\")\n","            print(f\"Chunk {i+1} (Tokens: {self.count_tokens(chunk)})\")\n","            print(\"Content preview:\")\n","            print(chunk[:1000] + \"...\" if len(chunk) > 1000 else chunk)\n","            \n","            self.print_separator(\"SEMANTIC SECTIONS\")\n","            for j, section in enumerate(sections):\n","                print(f\"\\nSection {j+1} (Tokens: {self.count_tokens(section)})\")\n","                print(\"Content preview:\")\n","                print(section[:500] + \"...\" if len(section) > 500 else section)\n","            \n","            self.print_separator(\"METRICS\")\n","            pprint(metrics)\n","            \n","            if self.missed_text:\n","                self.print_separator(\"MISSED TEXT\")\n","                print(self.missed_text)\n","            \n","            semantic_chunks.extend(sections)\n","            \n","            # Save intermediate results\n","            self.save_chunk_log(i+1, chunk, sections, metrics)\n","            \n","            time.sleep(1)  # Rate limiting\n","            \n","        self.log_message(f\"Processing complete. Total semantic chunks created: {len(semantic_chunks)}\")\n","        return semantic_chunks\n","\n","    def save_chunk_log(self, chunk_num: int, original_chunk: str, sections: List[str], metrics: Dict):\n","        \"\"\"Save intermediate processing results\"\"\"\n","        log_file = os.path.join(self.log_dir, f\"chunk_{chunk_num:04d}.json\")\n","        log_data = {\n","            \"chunk_number\": chunk_num,\n","            \"original_text\": original_chunk,\n","            \"semantic_sections\": sections,\n","            \"missed_text\": self.missed_text,\n","            \"metrics\": metrics,\n","            \"token_counts\": {\n","                \"input\": self.count_tokens(original_chunk),\n","                \"sections\": [self.count_tokens(s) for s in sections],\n","                \"missed\": self.count_tokens(self.missed_text) if self.missed_text else 0\n","            }\n","        }\n","        \n","        with open(log_file, 'w', encoding='utf-8') as f:\n","            json.dump(log_data, f, indent=2, ensure_ascii=False)\n","            \n","    def validate_chunk(self, chunk: str, original_sections: List[Section]) -> bool:\n","        \"\"\"Validate that chunk contains all expected content\"\"\"\n","        # Normalize texts for comparison\n","        chunk_text = ' '.join(chunk.split())\n","        original_text = ' '.join(' '.join(s.text for s in original_sections).split())\n","        \n","        # Check if all content is present\n","        missing_content = []\n","        words = original_text.split()\n","        window_size = 5\n","        \n","        i = 0\n","        while i < len(words) - window_size:\n","            sequence = ' '.join(words[i:i+window_size])\n","            if sequence not in chunk_text:\n","                missing_content.append(sequence)\n","            i += 1\n","        \n","        if missing_content:\n","            self.log_message(\"Missing content detected:\")\n","            for mc in missing_content:\n","                self.log_message(f\"  - {mc}\")\n","            return False\n","        \n","        return True\n","\n","def main():\n","    # Initialize chunker\n","    chunker = SemanticChunker()\n","    \n","    # Read input file\n","    input_file = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    \n","    # Process text (limit to first 5 chunks for testing)\n","    semantic_chunks = chunker.process_text(text, max_chunks=3)\n","    \n","    # Save final chunks\n","    output_dir = os.path.join(chunker.log_dir, \"semantic_chunks\")\n","    os.makedirs(output_dir, exist_ok=True)\n","    \n","    for i, chunk in enumerate(semantic_chunks):\n","        output_file = os.path.join(output_dir, f\"semantic_chunk_{i+1:04d}.txt\")\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","            f.write(chunk)\n","    \n","    chunker.log_message(f\"Saved {len(semantic_chunks)} semantic chunks to {output_dir}\")\n","\n","def test_structure_analysis():\n","    chunker = SemanticChunker()\n","    \n","    test_text = \"\"\"\n","                             AUTHOR'S NOTE\n","\n","My task in this work has been to investigate an individual phantasy\n","system, and in the doing of it problems of such magnitude have been,,,,,,,,,,,,,,\n","uncovered, that my endeavor to grasp them in their entirety has\n","necessarily meant only a superficial orientation toward those paths, the\n","opening and exploration of which may possibly crown the work of future\n","investigators with success.\n","\n","                                CONTENTS\n","\n","        INTRODUCTION                                                     3\n","        \n","        Relation of the Incest Phantasy to the Oedipus Legend—Moral\n","        revulsion over such a discovery\n","\n"," I.—    CONCERNING THE TWO KINDS OF THINKING                             8\n","\"\"\"\n","    \n","    try:\n","        print(\"Testing structural analysis...\")\n","        sections = chunker.analyze_text_structure(test_text)\n","        \n","        print(\"\\nIdentified sections:\")\n","        for i, section in enumerate(sections, 1):\n","            print(f\"\\nSection {i}:\")\n","            print(f\"Type: {section.type}\")\n","            print(f\"Level: {section.level}\")\n","            print(f\"Content preview: {section.text[:100]}...\")\n","        \n","        print(\"\\nTesting chunking with structure preservation...\")\n","        chunks = chunker.create_initial_chunks(test_text)\n","        \n","        print(\"\\nResulting chunks:\")\n","        for i, chunk in enumerate(chunks, 1):\n","            print(f\"\\nChunk {i}:\")\n","            print(chunk[:200])\n","            print(\"...\")\n","            \n","    except Exception as e:\n","        print(f\"Error during testing: {str(e)}\")\n","        \n","      # Add validation\n","    print(\"\\nValidating chunk content...\")\n","    for i, chunk in enumerate(chunks, 1):\n","        print(f\"\\nValidating chunk {i}:\")\n","        is_valid = chunker.validate_chunk(chunk, sections)\n","        print(f\"Chunk {i} validation: {'PASSED' if is_valid else 'FAILED'}\")\n","        \n","if __name__ == \"__main__\":\n","    # test_structure_analysis()\n","    # Comment out main() for testing\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import statements and data structures\n","import tiktoken\n","from openai import OpenAI\n","import json\n","import time\n","from typing import List, Dict, Tuple, Optional\n","import numpy as np\n","import os\n","from datetime import datetime\n","from pprint import pprint\n","import re\n","from dataclasses import dataclass\n","from enum import Enum\n","\n","# Core data structures\n","class SectionType(Enum):\n","    HEADER = \"header\"\n","    CONTENT = \"content\"\n","    QUOTE = \"quote\"\n","    ATTRIBUTION = \"attribution\"\n","    LIST = \"list\"\n","    FRONT_MATTER = \"front_matter\"\n","    TABLE_OF_CONTENTS = \"table_of_contents\"\n","    \n","@dataclass\n","class Section:\n","    text: str\n","    type: SectionType\n","    level: int = 0\n","    metadata: Dict = None\n","\n","# Main class initialization\n","class SemanticChunker:\n","    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n","        \"\"\"Initialize the semantic chunker with model configuration\"\"\"\n","        self.client = OpenAI(\n","            base_url=\"http://localhost:8000/v1\",\n","            api_key=\"dummy\"\n","        )\n","        self.model_name = model_name\n","        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n","        self.max_tokens = 3000\n","        \n","        # Set up logging directory with timestamp\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        self.log_dir = f\"/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/Psychology_Of_Unconscious_Mind/chunks_{timestamp}\"\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        \n","        # Set up logging file for processing summary\n","        self.log_file = os.path.join(self.log_dir, \"processing_log.txt\")\n","        \n","        # Initialize state variables\n","        self.missed_text = \"\"  # Store text not included in LLM output\n","\n","    def log_message(self, message: str):\n","        \"\"\"Write log message with timestamp and print to console\"\"\"\n","        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        log_entry = f\"[{timestamp}] {message}\"\n","        with open(self.log_file, 'a', encoding='utf-8') as f:\n","            f.write(log_entry + \"\\n\")\n","        print(log_entry)\n","    \n","    def print_separator(self, message: str = \"\"):\n","        \"\"\"Print a separator line with optional message\"\"\"\n","        print(f\"\\n{'='*100}\")\n","        if message:\n","            print(f\"{message}\")\n","            print('='*100)\n","    \n","    def count_tokens(self, text: str) -> int:\n","        \"\"\"Count tokens in text using tiktoken\"\"\"\n","        return len(self.encoding.encode(text))\n","    \n","# Continuing the SemanticChunker class...\n","\n","    def is_chapter_heading(self, text: str) -> Tuple[bool, int]:\n","        \"\"\"\n","        Enhanced chapter heading detection with level identification.\n","        Returns (is_heading, level).\n","        \"\"\"\n","        text = text.strip()\n","        if not text:\n","            return False, 0\n","            \n","        # Chapter patterns\n","        chapter_patterns = [\n","            (r'^CHAPTER\\s+[IVXL]+', 1),  # Main chapter headers\n","            (r'^[IVX]+\\.\\s*—\\s*', 2),    # Sub-chapter headers\n","            (r'^\\d+\\.\\s*—\\s*', 2),       # Numbered sections\n","            (r'^\\s*[A-Z][A-Z\\s]+$', 1),  # ALL CAPS lines\n","            (r'^\\s*[IVX]+\\.\\s+[A-Z]', 2) # Roman numeral sections\n","        ]\n","        \n","        for pattern, level in chapter_patterns:\n","            if re.match(pattern, text, re.I):\n","                return True, level\n","        \n","        # Check for centered text formatting\n","        line_length = len(text)\n","        leading_spaces = len(text) - len(text.lstrip())\n","        trailing_spaces = len(text) - len(text.rstrip())\n","        \n","        # Improved centered text detection\n","        is_centered = (abs(leading_spaces - trailing_spaces) <= 2 and \n","                      leading_spaces > 5 and\n","                      text.strip())  # Must have content\n","        is_caps = text.isupper()\n","        reasonable_length = 10 < len(text.strip()) < 100\n","        \n","        if is_centered:\n","            if is_caps and reasonable_length:\n","                return True, 1  # Main header\n","            elif reasonable_length:\n","                return True, 2  # Sub header\n","                \n","        return False, 0\n","    \n","    def analyze_text_structure(self, text: str) -> List[Section]:\n","        \"\"\"\n","        Enhanced text structure analysis with improved section detection.\n","        \"\"\"\n","        sections = []\n","        lines = text.split('\\n')\n","        current_section = []\n","        current_type = None\n","        current_level = 0\n","        \n","        def flush_section():\n","            nonlocal current_section, current_type, current_level\n","            if current_section:\n","                # Skip empty sections but preserve intentional spacing\n","                content = '\\n'.join(current_section).strip()\n","                if content or current_type in {SectionType.HEADER, SectionType.FRONT_MATTER}:\n","                    sections.append(Section(\n","                        text='\\n'.join(current_section),\n","                        type=current_type or SectionType.CONTENT,\n","                        level=current_level\n","                    ))\n","                current_section = []\n","                current_type = None\n","                current_level = 0\n","        \n","        i = 0\n","        while i < len(lines):\n","            line = lines[i]\n","            next_line = lines[i + 1] if i + 1 < len(lines) else \"\"\n","            \n","            # Detect centered headers\n","            if line.strip() and line.strip().isupper():\n","                leading_spaces = len(line) - len(line.lstrip())\n","                if leading_spaces > 10:  # Likely centered\n","                    flush_section()\n","                    current_type = SectionType.HEADER\n","                    current_level = 1\n","                    current_section = [line]\n","                    # Include following blank lines\n","                    while i + 1 < len(lines) and not lines[i + 1].strip():\n","                        current_section.append(lines[i + 1])\n","                        i += 1\n","                    flush_section()\n","                    i += 1\n","                    continue\n","            \n","            # Detect Table of Contents\n","            if re.match(r'^\\s*CONTENTS\\s*$', line, re.I):\n","                flush_section()\n","                current_type = SectionType.TABLE_OF_CONTENTS\n","                current_section = [line]\n","                # Include following blank lines\n","                while i + 1 < len(lines) and not lines[i + 1].strip():\n","                    current_section.append(lines[i + 1])\n","                    i += 1\n","                i += 1\n","                continue\n","            \n","            # Detect Front Matter\n","            if re.match(r'^\\s*(?:AUTHOR\\'S\\s+NOTE|PREFACE|INTRODUCTION)\\s*$', line, re.I):\n","                flush_section()\n","                current_type = SectionType.FRONT_MATTER\n","                current_section = [line]\n","                # Include following blank lines\n","                while i + 1 < len(lines) and not lines[i + 1].strip():\n","                    current_section.append(lines[i + 1])\n","                    i += 1\n","                i += 1\n","                continue\n","            \n","            # Detect chapter headings\n","            is_heading, level = self.is_chapter_heading(line)\n","            if is_heading:\n","                flush_section()\n","                current_type = SectionType.HEADER\n","                current_level = level\n","                current_section = [line]\n","                # Include following blank lines\n","                while i + 1 < len(lines) and not lines[i + 1].strip():\n","                    current_section.append(lines[i + 1])\n","                    i += 1\n","                flush_section()\n","                i += 1\n","                continue\n","            \n","            # Detect quotes\n","            if ((line.startswith('\"') and len(line) > 50) or \n","                (line.startswith('_') and line.endswith('_'))):\n","                if current_type != SectionType.QUOTE:\n","                    flush_section()\n","                    current_type = SectionType.QUOTE\n","                \n","            # Detect attributions\n","            if re.match(r'^\\s*(?:—|--)\\s*[A-Z]', line):\n","                flush_section()\n","                current_type = SectionType.ATTRIBUTION\n","                current_section = [line]\n","                i += 1\n","                continue\n","                \n","            # Detect lists\n","            if re.match(r'^\\s{4,}(?:[\\w\\-]+\\.|\\-|\\*)\\s', line):\n","                if current_type != SectionType.LIST:\n","                    flush_section()\n","                    current_type = SectionType.LIST\n","            \n","            current_section.append(line)\n","            i += 1\n","            \n","            # Handle section transitions\n","            if i < len(lines):\n","                next_line = lines[i]\n","                # Detect section breaks by multiple blank lines\n","                if (not line.strip() and not next_line.strip() and \n","                    current_type not in {SectionType.HEADER, SectionType.FRONT_MATTER}):\n","                    flush_section()\n","        \n","        flush_section()  # Flush any remaining content\n","        \n","        # Filter and clean sections\n","        filtered_sections = []\n","        for section in sections:\n","            if section.text.strip() or section.type in {SectionType.HEADER, SectionType.FRONT_MATTER}:\n","                filtered_sections.append(section)\n","        \n","        return filtered_sections\n","    \n","# Continuing the SemanticChunker class...\n","\n","    def get_complete_paragraphs(self, text: str, max_tokens: int) -> Tuple[str, str]:\n","        \"\"\"\n","        Enhanced version with improved content handling and structure preservation.\n","        \"\"\"\n","        self.log_message(f\"Starting get_complete_paragraphs with {len(text)} chars of text\")\n","        \n","        # First, analyze the structure\n","        sections = self.analyze_text_structure(text)\n","        self.log_message(f\"Found {len(sections)} sections\")\n","        \n","        current_sections = []\n","        current_tokens = 0\n","        section_index = 0\n","        \n","        try:\n","            while section_index < len(sections):\n","                section = sections[section_index]\n","                section_tokens = self.count_tokens(section.text)\n","                \n","                self.log_message(f\"Processing section {section_index + 1}: {section.type}, {section_tokens} tokens\")\n","                \n","                # Handle oversized sections\n","                if section_tokens > max_tokens:\n","                    if not current_sections:  # If this is our first section\n","                        self.log_message(f\"Warning: Section {section_index + 1} exceeds token limit\")\n","                        # Try to split at paragraph boundary\n","                        paragraphs = section.text.split('\\n\\n')\n","                        current_text = \"\"\n","                        for para in paragraphs:\n","                            if self.count_tokens(current_text + para) > max_tokens:\n","                                break\n","                            current_text += para + '\\n\\n'\n","                        if current_text:\n","                            current_sections.append(Section(current_text.rstrip(), section.type, section.level))\n","                        remaining_text = section.text[len(current_text):]\n","                        if remaining_text:\n","                            self.missed_text = remaining_text\n","                        section_index += 1\n","                        continue\n","                    else:\n","                        break\n","                \n","                # If adding this section would exceed the limit\n","                if current_tokens + section_tokens > max_tokens:\n","                    if current_sections:  # Only break if we have content\n","                        break\n","                \n","                # Handle headers and their content together\n","                if section.type == SectionType.HEADER:\n","                    header_and_content = [section]\n","                    total_tokens = section_tokens\n","                    \n","                    # Look ahead for associated content\n","                    next_idx = section_index + 1\n","                    while (next_idx < len(sections) and \n","                           sections[next_idx].type == SectionType.CONTENT and \n","                           total_tokens + self.count_tokens(sections[next_idx].text) <= max_tokens):\n","                        header_and_content.append(sections[next_idx])\n","                        total_tokens += self.count_tokens(sections[next_idx].text)\n","                        next_idx += 1\n","                    \n","                    # Add header and its content\n","                    current_sections.extend(header_and_content)\n","                    current_tokens = total_tokens\n","                    section_index = next_idx\n","                    continue\n","                \n","                # Handle other section types\n","                current_sections.append(section)\n","                current_tokens += section_tokens\n","                section_index += 1\n","                \n","                self.log_message(f\"After processing: tokens={current_tokens}/{max_tokens}, sections={len(current_sections)}\")\n","            \n","            # Combine sections with proper spacing\n","            processed_sections = []\n","            for i, section in enumerate(current_sections):\n","                if i > 0:  # Add spacing before sections\n","                    if section.type == SectionType.HEADER or current_sections[i-1].type == SectionType.HEADER:\n","                        processed_sections.append(\"\")  # Extra line before/after headers\n","                    processed_sections.append(\"\")  # Standard section spacing\n","                \n","                processed_sections.append(section.text.rstrip())\n","                \n","                # Add extra spacing after headers\n","                if section.type == SectionType.HEADER:\n","                    processed_sections.append(\"\")\n","            \n","            processed_text = \"\\n\".join(processed_sections)\n","            \n","            # Prepare remaining text\n","            remaining_sections = sections[section_index:]\n","            remaining_text = \"\"\n","            if remaining_sections:\n","                remaining_parts = []\n","                for section in remaining_sections:\n","                    if remaining_parts:  # Add spacing between sections\n","                        remaining_parts.append(\"\")\n","                    remaining_parts.append(section.text.rstrip())\n","                remaining_text = \"\\n\".join(remaining_parts)\n","            \n","            self.log_message(f\"Processed {len(current_sections)} sections, {len(remaining_sections)} remaining\")\n","            return processed_text, remaining_text\n","            \n","        except Exception as e:\n","            self.log_message(f\"Error in get_complete_paragraphs: {str(e)}\")\n","            if current_sections:\n","                return \"\\n\".join(s.text for s in current_sections), text\n","            return \"\", text\n","    \n","    def create_initial_chunks(self, text: str) -> List[str]:\n","        \"\"\"\n","        Create initial chunks while preserving document structure.\n","        \"\"\"\n","        chunks = []\n","        remaining_text = text\n","        chunk_number = 0\n","        \n","        while remaining_text.strip():\n","            chunk_number += 1\n","            self.log_message(f\"\\nProcessing chunk {chunk_number}\")\n","            \n","            # Handle missed text from previous chunk\n","            if self.missed_text:\n","                self.log_message(\"Adding missed text from previous chunk\")\n","                remaining_text = self.missed_text + '\\n\\n' + remaining_text\n","                self.missed_text = \"\"\n","            \n","            # Get complete paragraphs up to token limit\n","            chunk_text, remaining_text = self.get_complete_paragraphs(remaining_text, self.max_tokens)\n","            \n","            if chunk_text.strip():\n","                token_count = self.count_tokens(chunk_text)\n","                self.log_message(f\"Created chunk {chunk_number} ({token_count} tokens)\")\n","                chunks.append(chunk_text)\n","                \n","                # Log chunk preview\n","                preview_length = min(len(chunk_text), 500)\n","                preview = chunk_text[:preview_length] + (\"...\" if preview_length < len(chunk_text) else \"\")\n","                self.log_message(f\"Chunk preview:\\n{preview}\")\n","            else:\n","                self.log_message(\"Warning: Empty chunk produced\")\n","                if not remaining_text.strip():\n","                    break\n","            \n","            # Safety limit\n","            if len(chunks) >= 100:\n","                self.log_message(\"Warning: Maximum chunk limit reached\")\n","                break\n","        \n","        # Save chunks to files\n","        self.log_message(f\"Created {len(chunks)} initial chunks\")\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        for i, chunk in enumerate(chunks):\n","            chunk_file = os.path.join(self.log_dir, f\"chunk_{i+1:04d}.txt\")\n","            with open(chunk_file, 'w', encoding='utf-8') as f:\n","                f.write(chunk)\n","        \n","        return chunks\n","    \n","# Continuing the SemanticChunker class...\n","\n","    def get_semantic_sections(self, chunk: str) -> Tuple[List[str], Dict]:\n","        \"\"\"\n","        Process chunks through LLM for semantic analysis with improved handling.\n","        \"\"\"\n","        try:\n","            self.log_message(f\"Sending request to LLM (input tokens: {self.count_tokens(chunk)})\")\n","            \n","            # Enhanced prompt for better structure preservation\n","            system_prompt = \"\"\"You are a text analysis expert. Your task is to:\n","            1. Maintain the original document structure exactly as provided\n","            2. Split the input into semantically coherent sections\n","            3. Preserve all formatting, spacing, and special characters\n","            4. Keep headers with their associated content\n","            5. Keep lists and quotes intact within their sections\n","            6. Mark sections using <START_SECTION> and <END_SECTION> tags\n","            7. Mark incomplete sections with <INCOMPLETE> tags\n","            8. Handle front matter, tables of contents, and chapter headings appropriately\n","            9. Preserve all original line breaks and paragraph spacing\n","\n","            Critical Rules:\n","            - Do not modify any text content\n","            - Preserve all original formatting\n","            - Keep structural elements together (headers with content)\n","            - Maintain document hierarchy\n","            - Include ALL text - do not skip anything\n","            \"\"\"\n","            \n","            response = self.client.chat.completions.create(\n","                model=self.model_name,\n","                messages=[\n","                    {\"role\": \"system\", \"content\": system_prompt},\n","                    {\"role\": \"user\", \"content\": f\"Split this text into sections, preserving ALL content and structure:\\n\\n{chunk}\"}\n","                ],\n","                max_tokens=self.max_tokens,\n","                temperature=0.2,\n","                timeout=60  # 1-minute timeout\n","            )\n","            \n","            result = response.choices[0].message.content\n","            \n","            # Extract sections with improved parsing\n","            sections = []\n","            section_pattern = r'<START_SECTION>(.*?)<END_SECTION>'\n","            for match in re.finditer(section_pattern, result, re.DOTALL):\n","                section_text = match.group(1).strip()\n","                if section_text:  # Keep even short sections if they're structural\n","                    if len(section_text) > 50 or any(marker in section_text.upper() \n","                        for marker in ['CHAPTER', 'CONTENTS', 'NOTE', 'INTRODUCTION']):\n","                        sections.append(section_text)\n","            \n","            # Handle incomplete sections\n","            incomplete_pattern = r'<INCOMPLETE>(.*?)</INCOMPLETE>'\n","            incomplete_match = re.search(incomplete_pattern, result, re.DOTALL)\n","            if incomplete_match:\n","                incomplete_text = incomplete_match.group(1).strip()\n","                if incomplete_text:\n","                    self.missed_text = incomplete_text\n","                    self.log_message(f\"Found incomplete section ({self.count_tokens(incomplete_text)} tokens)\")\n","            \n","            # Verify content preservation\n","            if not sections:\n","                self.log_message(\"Warning: No sections created by LLM\")\n","                self.missed_text = chunk\n","            elif not incomplete_match:\n","                missed_text = self.verify_output_completeness(chunk, sections)\n","                if missed_text:\n","                    self.missed_text = missed_text\n","                    self.log_message(f\"Found missed text ({self.count_tokens(missed_text)} tokens)\")\n","            \n","            # Collect metrics\n","            metrics = {\n","                \"completion_tokens\": response.usage.completion_tokens,\n","                \"prompt_tokens\": response.usage.prompt_tokens,\n","                \"total_tokens\": response.usage.total_tokens,\n","                \"finish_reason\": response.choices[0].finish_reason,\n","                \"sections_created\": len(sections),\n","                \"has_missed_text\": bool(self.missed_text),\n","                \"avg_section_length\": sum(len(s) for s in sections) / len(sections) if sections else 0,\n","                \"timestamp\": datetime.now().isoformat()\n","            }\n","            \n","            return sections, metrics\n","                \n","        except Exception as e:\n","            self.log_message(f\"Error in LLM request: {str(e)}\")\n","            return [], {}\n","\n","    def verify_output_completeness(self, input_text: str, output_sections: List[str]) -> str:\n","        \"\"\"\n","        Enhanced verification of content preservation with improved detection.\n","        \"\"\"\n","        # Normalize texts for comparison\n","        input_normalized = ' '.join(input_text.split())\n","        output_normalized = ' '.join(' '.join(output_sections).split())\n","        \n","        # Quick full-text comparison\n","        if input_normalized == output_normalized:\n","            return \"\"\n","        \n","        # Find missing content using sliding window\n","        words = input_normalized.split()\n","        missing_sequences = set()  # Use set to avoid duplicates\n","        \n","        # Use multiple window sizes for better detection\n","        for window_size in [5, 10, 15]:  # Try different window sizes\n","            i = 0\n","            while i < len(words) - window_size:\n","                sequence = ' '.join(words[i:i+window_size])\n","                if sequence not in output_normalized:\n","                    # Find complete missing phrase\n","                    start = i\n","                    while start > 0 and ' '.join(words[start-1:i+window_size]) not in output_normalized:\n","                        start -= 1\n","                    end = i + window_size\n","                    while end < len(words) and ' '.join(words[i:end+1]) not in output_normalized:\n","                        end += 1\n","                    missing_sequences.add(' '.join(words[start:end]))\n","                    i = end\n","                else:\n","                    i += 1\n","        \n","        return '\\n'.join(sorted(missing_sequences)) if missing_sequences else \"\"\n","\n","    def validate_chunk(self, chunk: str, original_sections: List[Section]) -> bool:\n","        \"\"\"\n","        Comprehensive chunk validation with detailed reporting.\n","        \"\"\"\n","        # Normalize texts for comparison\n","        chunk_text = ' '.join(chunk.split())\n","        \n","        # Track missing content by section type\n","        missing_by_type = {}\n","        \n","        for section in original_sections:\n","            section_text = ' '.join(section.text.split())\n","            \n","            # For headers and front matter, require exact matches\n","            if section.type in [SectionType.HEADER, SectionType.FRONT_MATTER]:\n","                if section_text not in chunk_text:\n","                    missing_by_type.setdefault(section.type, []).append(section.text)\n","                continue\n","            \n","            # For other content, use sliding window detection\n","            words = section_text.split()\n","            window_size = 5\n","            missing_chunks = set()\n","            \n","            i = 0\n","            while i < len(words) - window_size:\n","                sequence = ' '.join(words[i:i+window_size])\n","                if sequence not in chunk_text:\n","                    # Find complete phrase\n","                    start = i\n","                    while start > 0 and ' '.join(words[start-1:i+window_size]) not in chunk_text:\n","                        start -= 1\n","                    end = i + window_size\n","                    while end < len(words) and ' '.join(words[i:end+1]) not in chunk_text:\n","                        end += 1\n","                    missing_chunks.add(' '.join(words[start:end]))\n","                    i = end\n","                else:\n","                    i += 1\n","            \n","            if missing_chunks:\n","                missing_by_type.setdefault(section.type, []).extend(missing_chunks)\n","        \n","        # Report missing content by type\n","        if missing_by_type:\n","            self.log_message(\"Missing content detected:\")\n","            for section_type, missing_content in missing_by_type.items():\n","                self.log_message(f\"\\n{section_type.value}:\")\n","                for content in missing_content:\n","                    self.log_message(f\"  - {content[:100]}...\")\n","            return False\n","        \n","        return True\n","\n","    def save_chunk_log(self, chunk_num: int, original_chunk: str, sections: List[str], metrics: Dict):\n","        \"\"\"\n","        Enhanced logging with more detailed analytics.\n","        \"\"\"\n","        log_file = os.path.join(self.log_dir, f\"chunk_{chunk_num:04d}.json\")\n","        \n","        # Add detailed token analysis\n","        section_analytics = [{\n","            \"length\": len(section),\n","            \"tokens\": self.count_tokens(section),\n","            \"lines\": len(section.split('\\n')),\n","            \"paragraphs\": len(section.split('\\n\\n')),\n","            \"preview\": section[:200]\n","        } for section in sections]\n","        \n","        log_data = {\n","            \"chunk_number\": chunk_num,\n","            \"timestamp\": datetime.now().isoformat(),\n","            \"original_text\": {\n","                \"content\": original_chunk,\n","                \"length\": len(original_chunk),\n","                \"tokens\": self.count_tokens(original_chunk)\n","            },\n","            \"sections\": {\n","                \"count\": len(sections),\n","                \"analytics\": section_analytics,\n","                \"content\": sections\n","            },\n","            \"missed_text\": {\n","                \"content\": self.missed_text,\n","                \"length\": len(self.missed_text) if self.missed_text else 0,\n","                \"tokens\": self.count_tokens(self.missed_text) if self.missed_text else 0\n","            },\n","            \"metrics\": metrics,\n","            \"validation\": {\n","                \"all_content_preserved\": not bool(self.missed_text),\n","                \"total_output_tokens\": sum(self.count_tokens(s) for s in sections)\n","            }\n","        }\n","        \n","        with open(log_file, 'w', encoding='utf-8') as f:\n","            json.dump(log_data, f, indent=2, ensure_ascii=False)\n","            \n","    def process_text(self, text: str, max_chunks: int = None) -> List[str]:\n","            \"\"\"\n","            Main text processing pipeline with enhanced error handling and logging.\n","            \"\"\"\n","            self.log_message(\"Starting text processing\")\n","            \n","            try:\n","                # Create initial chunks\n","                initial_chunks = self.create_initial_chunks(text)\n","                \n","                if max_chunks:\n","                    initial_chunks = initial_chunks[:max_chunks]\n","                    self.log_message(f\"Processing limited to first {max_chunks} chunks\")\n","                \n","                # Process each chunk\n","                semantic_chunks = []\n","                for i, chunk in enumerate(initial_chunks):\n","                    self.log_message(f\"\\nProcessing chunk {i+1}/{len(initial_chunks)}\")\n","                    \n","                    # Detailed chunk analysis\n","                    chunk_tokens = self.count_tokens(chunk)\n","                    self.log_message(f\"Chunk size: {len(chunk)} chars, {chunk_tokens} tokens\")\n","                    \n","                    # Print input preview\n","                    self.print_separator(\"INPUT CHUNK\")\n","                    print(f\"Chunk {i+1}:\")\n","                    print(\"=\"*80)\n","                    print(chunk[:1000] + \"...\" if len(chunk) > 1000 else chunk)\n","                    print(\"=\"*80)\n","                    \n","                    # Get semantic sections\n","                    sections, metrics = self.get_semantic_sections(chunk)\n","                    \n","                    # Process and validate sections\n","                    self.print_separator(\"SEMANTIC SECTIONS\")\n","                    for j, section in enumerate(sections):\n","                        section_tokens = self.count_tokens(section)\n","                        print(f\"\\nSection {j+1} ({section_tokens} tokens):\")\n","                        print(\"-\"*40)\n","                        print(section[:500] + \"...\" if len(section) > 500 else section)\n","                        print(\"-\"*40)\n","                    \n","                    # Print metrics\n","                    self.print_separator(\"PROCESSING METRICS\")\n","                    pprint(metrics)\n","                    \n","                    if self.missed_text:\n","                        self.print_separator(\"MISSED CONTENT\")\n","                        print(self.missed_text)\n","                    \n","                    semantic_chunks.extend(sections)\n","                    \n","                    # Save detailed processing log\n","                    self.save_chunk_log(i+1, chunk, sections, metrics)\n","                    \n","                    time.sleep(1)  # Rate limiting\n","                \n","                self.log_message(f\"Processing complete. Created {len(semantic_chunks)} semantic chunks\")\n","                return semantic_chunks\n","                \n","            except Exception as e:\n","                self.log_message(f\"Error in text processing: {str(e)}\")\n","                raise\n","\n","def main():\n","    \"\"\"\n","    Main execution function with enhanced error handling and reporting.\n","    \"\"\"\n","    try:\n","        # Initialize chunker\n","        chunker = SemanticChunker()\n","        \n","        # Read input file\n","        input_file = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","        \n","        if not os.path.exists(input_file):\n","            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n","        \n","        with open(input_file, 'r', encoding='utf-8') as f:\n","            text = f.read()\n","        \n","        chunker.log_message(f\"Starting processing of {input_file}\")\n","        chunker.log_message(f\"Input text: {len(text)} chars, {chunker.count_tokens(text)} tokens\")\n","        \n","        # Process text with limit for testing\n","        semantic_chunks = chunker.process_text(text, max_chunks=3)\n","        \n","        # Save final chunks\n","        output_dir = os.path.join(chunker.log_dir, \"semantic_chunks\")\n","        os.makedirs(output_dir, exist_ok=True)\n","        \n","        for i, chunk in enumerate(semantic_chunks):\n","            output_file = os.path.join(output_dir, f\"semantic_chunk_{i+1:04d}.txt\")\n","            with open(output_file, 'w', encoding='utf-8') as f:\n","                f.write(chunk)\n","        \n","        chunker.log_message(f\"Saved {len(semantic_chunks)} semantic chunks to {output_dir}\")\n","        \n","        # Generate processing summary\n","        summary_file = os.path.join(chunker.log_dir, \"processing_summary.json\")\n","        summary = {\n","            \"timestamp\": datetime.now().isoformat(),\n","            \"input_file\": input_file,\n","            \"input_stats\": {\n","                \"chars\": len(text),\n","                \"tokens\": chunker.count_tokens(text)\n","            },\n","            \"output_stats\": {\n","                \"total_chunks\": len(semantic_chunks),\n","                \"total_tokens\": sum(chunker.count_tokens(c) for c in semantic_chunks),\n","                \"avg_chunk_size\": sum(len(c) for c in semantic_chunks) / len(semantic_chunks)\n","            },\n","            \"output_directory\": output_dir\n","        }\n","        \n","        with open(summary_file, 'w', encoding='utf-8') as f:\n","            json.dump(summary, f, indent=2)\n","        \n","    except Exception as e:\n","        print(f\"Error in main execution: {str(e)}\")\n","        raise\n","\n","def test_structure_analysis():\n","    \"\"\"\n","    Enhanced test function with detailed validation and reporting.\n","    \"\"\"\n","    chunker = SemanticChunker()\n","    \n","    test_text = \"\"\"\n","                             AUTHOR'S NOTE\n","\n","My task in this work has been to investigate an individual phantasy\n","system, and in the doing of it problems of such magnitude have been\n","uncovered, that my endeavor to grasp them in their entirety has\n","necessarily meant only a superficial orientation toward those paths, the\n","opening and exploration of which may possibly crown the work of future\n","investigators with success.\n","\n","                                CONTENTS\n","\n","        INTRODUCTION                                                     3\n","        \n","        Relation of the Incest Phantasy to the Oedipus Legend—Moral\n","        revulsion over such a discovery\n","\n"," I.—    CONCERNING THE TWO KINDS OF THINKING                             8\n","\"\"\"\n","    \n","    try:\n","        print(\"\\nTesting structural analysis...\")\n","        sections = chunker.analyze_text_structure(test_text)\n","        \n","        print(\"\\nIdentified sections:\")\n","        for i, section in enumerate(sections, 1):\n","            print(f\"\\nSection {i}:\")\n","            print(\"=\"*80)\n","            print(f\"Type: {section.type}\")\n","            print(f\"Level: {section.level}\")\n","            print(f\"Length: {len(section.text)} chars, {chunker.count_tokens(section.text)} tokens\")\n","            print(\"-\"*40)\n","            print(section.text)\n","            print(\"=\"*80)\n","        \n","        print(\"\\nTesting chunking with structure preservation...\")\n","        chunks = chunker.create_initial_chunks(test_text)\n","        \n","        print(\"\\nResulting chunks:\")\n","        for i, chunk in enumerate(chunks, 1):\n","            print(f\"\\nChunk {i}:\")\n","            print(\"=\"*80)\n","            print(chunk)\n","            print(\"=\"*80)\n","            \n","        print(\"\\nValidating chunk content...\")\n","        for i, chunk in enumerate(chunks, 1):\n","            print(f\"\\nValidating chunk {i}:\")\n","            is_valid = chunker.validate_chunk(chunk, sections)\n","            print(f\"Chunk {i} validation: {'PASSED' if is_valid else 'FAILED'}\")\n","            \n","        # Generate test summary\n","        test_summary = {\n","            \"timestamp\": datetime.now().isoformat(),\n","            \"sections_identified\": len(sections),\n","            \"chunks_created\": len(chunks),\n","            \"section_types\": {str(s.type): sum(1 for sec in sections if sec.type == s.type) for s in sections},\n","            \"validation_results\": [chunker.validate_chunk(c, sections) for c in chunks]\n","        }\n","        \n","        print(\"\\nTest Summary:\")\n","        pprint(test_summary)\n","            \n","    except Exception as e:\n","        print(f\"Error during testing: {str(e)}\")\n","        raise\n","\n","if __name__ == \"__main__\":\n","    if os.environ.get(\"SEMANTIC_CHUNKER_TEST\"):\n","        test_structure_analysis()\n","    else:\n","        main()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.825136Z","iopub.status.busy":"2024-11-04T08:49:54.824806Z","iopub.status.idle":"2024-11-04T08:49:54.833600Z","shell.execute_reply":"2024-11-04T08:49:54.832541Z","shell.execute_reply.started":"2024-11-04T08:49:54.825103Z"},"trusted":true},"outputs":[],"source":["# # ----------------------------- #\n","# # Part 3: Parse Text into Discourse Units\n","# # ----------------------------- #\n","\n","def parse_discourse_units(text, overwrite=False):\n","    \"\"\"\n","    Parses text into discourse units using spaCy.\n","    Currently splits text into sentences.\n","    \"\"\"\n","    paragraphs = text.split('\\n\\n')\n","    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n","\n","    discourse_units = []\n","    for para in paragraphs:\n","        doc = nlp(para)\n","        sentences = [sent.text for sent in doc.sents]\n","        discourse_units.extend(sentences)\n","\n","    output_path = '/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_discourse_units.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Discourse Units: {len(discourse_units)}\")\n","    return discourse_units"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:54.987822Z","iopub.status.busy":"2024-11-04T08:49:54.987546Z","iopub.status.idle":"2024-11-04T08:49:54.997464Z","shell.execute_reply":"2024-11-04T08:49:54.996587Z","shell.execute_reply.started":"2024-11-04T08:49:54.987793Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 4: Create Chunks Using Hybrid Strategy\n","# ----------------------------- #\n","\n","def create_chunks(discourse_units, tokenizer, max_length=4096, overlap_size=1, overwrite=False):\n","    \"\"\"\n","    Creates chunks from discourse units using a sliding window with overlapping chunks.\n","    Optimized to work directly with token IDs and utilize efficient list operations.\n","    \"\"\"\n","    chunks = []\n","    current_chunk_tokens = []\n","    current_length = 0\n","\n","    for unit in discourse_units:\n","        unit_tokens = tokenizer.encode(unit, add_special_tokens=True)\n","        unit_length = len(unit_tokens)\n","\n","        if current_length + unit_length <= max_length:\n","            current_chunk_tokens.extend(unit_tokens)\n","            current_length += unit_length\n","        else:\n","            # Decode and append the current chunk\n","            chunk_text = tokenizer.decode(\n","                current_chunk_tokens, skip_special_tokens=True)\n","            chunks.append(chunk_text)\n","\n","            # Prepare overlap tokens\n","            overlap_tokens = current_chunk_tokens[-overlap_size:]\n","            current_chunk_tokens = overlap_tokens + unit_tokens\n","            current_length = len(current_chunk_tokens)\n","\n","    # Append any remaining tokens as the last chunk\n","    if current_chunk_tokens:\n","        chunk_text = tokenizer.decode(\n","            current_chunk_tokens, skip_special_tokens=True)\n","        chunks.append(chunk_text)\n","\n","    # Write or read chunks as before\n","    output_path = '/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_chunks.json'\n","    if not os.path.exists(output_path) or overwrite:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        with open(output_path, 'w') as f:\n","            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n","    else:\n","        with open(output_path, 'r') as f:\n","            discourse_units = json.load(f)\n","\n","    print(f\"Total Chunks Created: {len(chunks)}\")\n","    return chunks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:49:56.853343Z","iopub.status.busy":"2024-11-04T08:49:56.852589Z","iopub.status.idle":"2024-11-04T08:49:56.862517Z","shell.execute_reply":"2024-11-04T08:49:56.861550Z","shell.execute_reply.started":"2024-11-04T08:49:56.853301Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 5: Create and Tokenize Dataset\n","# ----------------------------- #\n","\n","# To Do - make book titles and prompt generic so\n","def create_tokenized_dataset(chunks, tokenizer, max_length=1024, num_proc=2):\n","\n","    # Create a Dataset object from chunks\n","\n","    book_title = 'Psychology of the Unconscious by C. G. Jung'\n","    wikipedia_prompt = \"\"\"\n","    Psychology Book\n","\n","    ### Title: {}\n","\n","    ### Article: {}\n","    \"\"\"\n","\n","    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n","\n","    def formatting_prompts_func(examples):\n","        titles = book_title\n","        texts = examples[\"text\"]\n","        outputs = []\n","        for title, text in zip([book_title]*len(chunks), texts):\n","            text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n","            outputs.append(text)\n","        return {\"text\": outputs, }\n","    pass\n","\n","    # convert chunks variable to huggingface dataset\n","\n","    from datasets import Dataset\n","\n","    dataset = Dataset.from_dict({\"text\": chunks})\n","\n","    dataset = dataset.map(formatting_prompts_func,\n","                          batched=True, num_proc=num_proc)\n","    # Split the dataset into training and validation sets\n","    split = dataset.train_test_split(test_size=0.1, seed=42)\n","    train_dataset = split['train']\n","    eval_dataset = split['test']\n","\n","    print(len(dataset))\n","    # Find the maximum length of the text field in the entire dataset\n","    max_length = max(len(text) for text in dataset['text'])\n","    print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","#     print(f\"First 5 rows of training dataset: {train_dataset[:5]}\")\n","    print(f\"Validation Dataset Size: {len(eval_dataset)}\")\n","#     print(f\"First 5 rows of validation dataset: {eval_dataset[:5]}\")\n","    return train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:50:01.810305Z","iopub.status.busy":"2024-11-04T08:50:01.809400Z","iopub.status.idle":"2024-11-04T08:50:01.844994Z","shell.execute_reply":"2024-11-04T08:50:01.844131Z","shell.execute_reply.started":"2024-11-04T08:50:01.810262Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 6: Set up environment and other important utilities\n","# ----------------------------- #\n","\n","def setup_environment():\n","    \"\"\"\n","    Initializes the Accelerator for distributed training.\n","    \"\"\"\n","    return Accelerator()\n","\n","\n","def get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n","    \"\"\"\n","    Defines a custom learning rate scheduler with warmup, constant, and linear annealing phases.\n","    \"\"\"\n","    def lr_lambda(current_step):\n","        if current_step < num_warmup_steps:\n","            return current_step / num_warmup_steps  # Linear warmup\n","        elif current_step < initial_phase_steps:\n","            return 1.0  # Constant learning rate for initial phase\n","        else:\n","            # Linear annealing for the remaining steps\n","            return 1.0 - ((current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps))\n","\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","\n","def setup_training_steps(initial_rows, annealing_rows, batch_size, gradient_accumulation_steps, num_epochs):\n","    \"\"\"\n","    Calculates total and initial training steps based on dataset size and training parameters.\n","    \"\"\"\n","    total_rows = initial_rows + annealing_rows\n","    total_steps = (total_rows * num_epochs) // (batch_size *\n","                                                gradient_accumulation_steps)\n","    initial_steps = (initial_rows * num_epochs) // (batch_size *\n","                                                    gradient_accumulation_steps)\n","    return max(1, total_steps), max(1, initial_steps)\n","\n","\n","def print_memory_usage(step_desc):\n","    \"\"\"\n","    Prints the CUDA memory summary if CUDA is available.\n","    \"\"\"\n","    if torch.cuda.is_available():\n","        print(f\"Memory Usage at {step_desc}:\")\n","        print(torch.cuda.memory_summary())\n","        print(\"\\n\")\n","    else:\n","        print(f\"No CUDA available at {step_desc}.\\n\")\n","\n","\n","def inference(model, tokenizer):\n","    \"\"\"\n","    Runs inference using the trained model.\n","    \"\"\"\n","    # Define sample prompts\n","    prompts = [\n","        \"Explain what is medical anthropology and its importance in elevating the quality of life?\",\n","        \"Explain what are the types of Jungian archetypes and how they manifest in the human psyche?\"\n","    ]\n","\n","    for prompt in prompts:\n","        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, max_length=256)\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n","        \n","def compute_metrics(eval_pred):\n","    \"\"\"\n","    Computes perplexity based on model predictions and labels.\n","    \"\"\"\n","    logits, labels = eval_pred\n","    # Convert to torch tensors\n","    logits = torch.tensor(logits)\n","    labels = torch.tensor(labels)\n","    \n","    # Ensure shapes match\n","    if logits.shape[:2] != labels.shape:\n","        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n","    \n","    # Shift logits and labels\n","    shift_logits = logits[:, :-1, :].contiguous()\n","    shift_labels = labels[:, 1:].contiguous()\n","\n","    # Check label values\n","    if shift_labels.max() >= model.config.vocab_size:\n","        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n","    \n","    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n","    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","    perplexity = torch.exp(loss).item()\n","    return {\"perplexity\": perplexity}\n","\n","#  Login to Huggingface\n","from huggingface_hub import login\n","import os\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","def setup_huggingface_access():\n","    \"\"\"Setup Hugging Face access with token from environment or manual input\"\"\"\n","    # First try to get token from environment variable\n","    token = os.getenv('HUGGINGFACE_TOKEN')\n","    \n","    if not token:\n","        # If not in environment, prompt for token\n","        token = input(\"Enter your Hugging Face token: \")\n","        \n","    if token:\n","        try:\n","            login(token, add_to_git_credential=True)\n","            print(\"Successfully logged in to Hugging Face!\")\n","        except Exception as e:\n","            print(f\"Failed to log in to Hugging Face: {str(e)}\")\n","            return False\n","    else:\n","        print(\"No Hugging Face token provided\")\n","        return False\n","    \n","    return True"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:51:23.895754Z","iopub.status.busy":"2024-11-04T08:51:23.894928Z","iopub.status.idle":"2024-11-04T08:51:23.904037Z","shell.execute_reply":"2024-11-04T08:51:23.903118Z","shell.execute_reply.started":"2024-11-04T08:51:23.895714Z"},"trusted":true},"outputs":[],"source":["def load_model_and_tokenizer(base_model_slug, max_seq_length=4096, dtype=None, load_in_4bit=True):\n","    \"\"\"\n","    Load and configure the model and tokenizer with specified parameters on a single GPU.\n","    \"\"\"\n","    import torch\n","    import os\n","\n","    # Force CUDA if available\n","    if torch.cuda.is_available():\n","        print(\"CUDA is available.\")\n","        print(f\"Using GPU: {torch.cuda.get_device_properties(0).name}\")\n","        device = torch.device(\"cuda:0\")\n","        device_map = {\"\": 0}  # Force everything to GPU 0\n","    else:\n","        print(\"WARNING: CUDA is not available. Using CPU.\")\n","        device = torch.device(\"cpu\")\n","        device_map = \"cpu\"\n","\n","    # Print initial GPU memory\n","    if torch.cuda.is_available():\n","        print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n","\n","    try:\n","        # Load base model and tokenizer\n","        model, tokenizer = FastLanguageModel.from_pretrained(\n","            model_name=base_model_slug,\n","            max_seq_length=max_seq_length,\n","            dtype=dtype,\n","            load_in_4bit=load_in_4bit,\n","            device_map=device_map,\n","            token=os.getenv('HUGGINGFACE_TOKEN'),\n","        )\n","        \n","        print(f\"Model device after loading: {next(model.parameters()).device}\")\n","        \n","        # Configure PEFT model\n","        model = FastLanguageModel.get_peft_model(\n","            model,\n","            r=128,\n","            target_modules=[\n","                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                \"gate_proj\", \"up_proj\", \"down_proj\",\n","                \"embed_tokens\", \"lm_head\",\n","            ],\n","            lora_alpha=32,\n","            lora_dropout=0,\n","            bias=\"none\",\n","            use_gradient_checkpointing=\"unsloth\",\n","            random_state=3407,\n","            use_rslora=True,\n","            loftq_config=None,\n","        )\n","        \n","        # Ensure model is on GPU after PEFT configuration\n","        if torch.cuda.is_available():\n","            model = model.to(device)\n","            \n","        # Verify final device placement\n","        print(f\"Final model device: {next(model.parameters()).device}\")\n","        \n","        # Print GPU memory usage\n","        if torch.cuda.is_available():\n","            print(f\"\\nGPU Memory After Complete Setup: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n","            \n","    except Exception as e:\n","        print(f\"Error in model loading/configuration: {str(e)}\")\n","        raise\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:51:27.038467Z","iopub.status.busy":"2024-11-04T08:51:27.038068Z","iopub.status.idle":"2024-11-04T08:51:27.044121Z","shell.execute_reply":"2024-11-04T08:51:27.043250Z","shell.execute_reply.started":"2024-11-04T08:51:27.038425Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","# Set the environment variable\n","os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n","os.environ[\"WANDB_API_KEY\"] = \"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\"\n","# Verify it's set correctly\n","print(os.getenv(\"HUGGINGFACE_TOKEN\"))\n","print(os.getenv(\"WANDB_API_KEY\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:53:40.380684Z","iopub.status.busy":"2024-11-04T08:53:40.380279Z","iopub.status.idle":"2024-11-04T08:54:30.243475Z","shell.execute_reply":"2024-11-04T08:54:30.242381Z","shell.execute_reply.started":"2024-11-04T08:53:40.380645Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 2: Configure Environment Variables & Create Main Variables\n","# ----------------------------- #\n","\n","# Unsloth modell initialization variables\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","max_length = max_seq_length\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","# device_map = \"auto\"\n","base_model_slug = \"Qwen/Qwen2.5-7B-Instruct\"\n","base_model_name = \"lora_model_pum\"\n","chunks_max_length = max_seq_length\n","overlap_size = 1\n","# Define your parameters\n","batchSize = 2\n","ga = 8\n","maxSteps = 10\n","warmupSteps = 10\n","numTrainEpochs = 1\n","lRate = 5e-5\n","embLRate = 1e-5\n","optim = \"adamw_8bit\"\n","lrSchedule = \"linear\"\n","dataset_slug = \"psychology_of_unconscious\"\n","\n","from datetime import datetime\n","import pytz\n","import wandb\n","# Get the current date and time in Indian Standard Time (IST)\n","ist = pytz.timezone('Asia/Kolkata')\n","current_datetime = datetime.now(ist)\n","\n","# Format the datetime string\n","# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n","formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Define Run Name\n","run_name = f\"\"\"Kaggle-quantumLeap-{formatted_datetime}-{base_model_slug}-{dataset_slug}-{max_seq_length}_maxSeqLength-{max_length}_maxLength-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{numTrainEpochs}_numTrainEpochs-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n","\n","# Initialize Weights & Biases\n","# It's recommended to set your W&B API key as an environment variable for security.\n","wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n","wandb.init(project=\"OLA-quantumLeap\", name=run_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:55:17.806518Z","iopub.status.busy":"2024-11-04T08:55:17.806143Z","iopub.status.idle":"2024-11-04T08:55:20.860165Z","shell.execute_reply":"2024-11-04T08:55:20.859337Z","shell.execute_reply.started":"2024-11-04T08:55:17.806483Z"},"trusted":true},"outputs":[],"source":["\n","# ----------------------------- #\n","# Part 9: Data Processing\n","# ----------------------------- #\n","\n","# # Perform Inference Before Training\n","# inference(model, tokenizer)\n","\n","# Set number of processes to use for data loading\n","num_cpus = multiprocessing.cpu_count()\n","num_proc = max(num_cpus-2, 2)  # Adjust based on prior recommendations\n","print(f\"Number of CPU cores: {num_cpus}\")\n","print(f\"Number of processes: {num_proc}\")\n","\n","# Login to Hugging Face\n","if not setup_huggingface_access():\n","    raise Exception(\"Failed to setup Hugging Face access. Please check your token.\")\n","\n","# Load Model and Tokenizer\n","model, tokenizer = load_model_and_tokenizer(base_model_slug)\n","print(f\"Model Device: {model.device}\")\n","\n","# Load and Clean Text Data\n","file_path = \"/home/ubuntu/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\n","clean_text = load_and_clean_text(file_path)\n","\n","# Parse Discourse Units\n","discourse_units = parse_discourse_units(clean_text, overwrite=True)\n","\n","# Create Chunks\n","chunks = create_chunks(\n","    discourse_units,\n","    tokenizer,\n","    max_length=max_length,\n","    overlap_size=overlap_size,\n","    overwrite=True,\n",")\n","\n","# Create Tokenized Dataset\n","train_dataset, eval_dataset = create_tokenized_dataset(\n","    chunks, tokenizer, max_length)\n","\n","# Save datasets as Hugging Face `datasets`\n","train_dataset.save_to_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","eval_dataset.save_to_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')\n","\n","### To Do - Make the below as dynamic and as a functio\n","# # Uncomment following if you want to just load the data from temp directory\n","# from datasets import load_from_disk\n","\n","# train_dataset = load_from_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n","# eval_dataset = load_from_disk('/home/ubuntu/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import IntervalStrategy\n","from transformers.integrations import TensorBoardCallback\n","\n","import wandb\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_dataset,\n","    eval_dataset = eval_dataset,  # Use 10% of data for evaluation\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = batchSize,\n","        gradient_accumulation_steps = ga,\n","\n","        # Set both max_steps and num_train_epochs\n","        max_steps = maxSteps,\n","        num_train_epochs = numTrainEpochs,\n","\n","        # Use a single learning rate for all parameters\n","        learning_rate = lRate,\n","\n","        # Warmup strategy from successful runs\n","        warmup_steps = warmupSteps,\n","        # warmup_ratio = 0,\n","\n","        # Explicitly set precision based on hardware support\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        \n","        logging_steps = 1,\n","        \n","        optim = optim,\n","        weight_decay = 0.01,\n","        lr_scheduler_type = lrSchedule,\n","        \n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        \n","        report_to=[\"tensorboard\", \"wandb\"],\n","        logging_dir=f\"./trel-fft-logs/{run_name}\",\n","        \n","        # Set both save and evaluation strategies to 'steps'\n","        # save_strategy = IntervalStrategy.STEPS,\n","        # eval_strategy = IntervalStrategy.STEPS,\n","        # save_steps = 1,  # Save checkpoint every 20 steps\n","        # eval_steps = 1,  # Evaluate every 20 steps (matching save_steps)\n","        \n","        # load_best_model_at_end = True,\n","        # metric_for_best_model = \"eval_loss\",\n","    ),\n","    # compute_metrics = compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %%time\n","\n","# instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","# ### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","# concept_name: {}\n","# detailed_explanation: {}\n","\n","# ### Response:\n","# {}\"\"\"\n","\n","# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","# outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n","# tokenizer.batch_decode(outputs)\n","\n","\n","# %%time\n","# # Text Streaming goes into a loop and doesnt adher to EOS\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)\n","\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T08:55:41.075256Z","iopub.status.busy":"2024-11-04T08:55:41.074858Z","iopub.status.idle":"2024-11-04T10:16:26.590311Z","shell.execute_reply":"2024-11-04T10:16:26.589250Z","shell.execute_reply.started":"2024-11-04T08:55:41.075219Z"},"trusted":true},"outputs":[],"source":["# ----------------------------- #\n","# Part 11: Start Training\n","# ----------------------------- #\n","\n","#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","trainer_stats = trainer.train()\n","\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import wandb\n","from pprint import pprint\n","\n","def get_run_config(project_name, run_id):\n","    try:\n","        # Initialize the wandb API\n","        api = wandb.Api()\n","\n","        # Access the specific run\n","        run = api.run(f\"{project_name}/{run_id}\")\n","\n","        # Get the full configuration\n","        config = run.config\n","\n","        # Filter for trainer-specific configuration\n","        trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n","\n","        return trainer_config\n","\n","    except wandb.errors.CommError:\n","        print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n","        return None\n","    except Exception as e:\n","        print(f\"An error occurred: {str(e)}\")\n","        return None\n","\n","# Usage\n","project_name = \"olabs-asia-olabs-pro/OLA-quantumLeap\"\n","run_id = \"we4axhd1\"\n","\n","trainer_config = get_run_config(project_name, run_id)\n","\n","if trainer_config:\n","    print(f\"Trainer configuration for run {run_id}:\")\n","    pprint(trainer_config)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %%time\n","\n","# instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","# ### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","# concept_name: {}\n","# detailed_explanation: {}\n","\n","# ### Response:\n","# {}\"\"\"\n","\n","# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","# outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n","# tokenizer.batch_decode(outputs)\n","\n","\n","# %%time\n","# # Text Streaming goes into a loop and doesnt adher to EOS\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)\n","\n","\n","# inputs = tokenizer(\n","# [\n","#     instruction_prompt.format(\n","#         \"Hero Archetype\", # concept_name\n","#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n","#         \"\", # output - leave this blank for generation!\n","#     )\n","# ], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","\n","# from transformers import TextStreamer\n","# text_streamer = TextStreamer(tokenizer)\n","# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n","#                    repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","import os\n","\n","# Create timestamp\n","timestamp = int(time.time())\n","\n","# Create directory if it doesn't exist\n","save_dir = f\"/root/quantumLeap/models/qLeap_model_v0_{timestamp}\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# Save functions with explicit paths\n","def save_model_versions(model, tokenizer, timestamp, token):\n","    \"\"\"\n","    Save model in different formats with proper error handling\n","    \"\"\"\n","    try:\n","        # Save base model locally\n","        print(\"Saving base model locally...\")\n","        # model.save_pretrained(f\"{save_dir}/base\")\n","        # tokenizer.save_pretrained(f\"{save_dir}/base\")\n","        \n","        # Save 8-bit Q8_0 version\n","        print(\"Saving 8-bit Q8_0 version...\")\n","        try:\n","            model.push_to_hub_gguf(\n","                f\"olabs-ai/qLeap_model_v0_8bit_Q8_{timestamp}\",\n","                tokenizer,\n","                token=token,\n","                quantization_method=\"q8_0\"\n","            )\n","            print(\"Successfully saved 8-bit model\")\n","        except Exception as e:\n","            print(f\"Error saving 8-bit model: {str(e)}\")\n","            \n","        # Optional: Save 16-bit version\n","        print(\"Saving 16-bit version...\")\n","        try:\n","            model.push_to_hub_gguf(\n","                f\"olabs-ai/qLeap_model_v0_16bit_GGUF_{timestamp}\",\n","                tokenizer,\n","                quantization_method=\"f16\",\n","                token=token\n","            )\n","            print(\"Successfully saved 16-bit model\")\n","        except Exception as e:\n","            print(f\"Error saving 16-bit model: {str(e)}\")\n","            \n","    except Exception as e:\n","        print(f\"Error in save process: {str(e)}\")\n","        raise\n","\n","# Call the save function\n","huggingface_token = \"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\"\n","save_model_versions(model, tokenizer, timestamp, huggingface_token)"]},{"cell_type":"markdown","metadata":{},"source":["### if the loss from earlier training is too high try training arguments from unsloth colab notebook \"Llama-3.1 8b + Unsloth 2x faster finetuning.ipynb\". URL below\n","### https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=95_Nn-89DhsL"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset creation based on the book itself using AugmenToolkit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Instruction  Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n","\n","import json\n","import ast\n","import logging\n","\n","import csv\n","\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n","    reader = csv.DictReader(f)\n","    data = list(reader)\n","    \n","type(data)\n","\n","\n","# Configure logging\n","logging.basicConfig(\n","    filename='transformation_errors.log',\n","    filemode='w',\n","    level=logging.ERROR,\n","    format='%(levelname)s:%(message)s'\n",")\n","\n","# Sample original data\n","original_data = data\n","\n","def transform_data(original_data):\n","    \"\"\"\n","    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n","\n","    Parameters:\n","        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n","\n","    Returns:\n","        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n","    \"\"\"\n","    new_data = []\n","\n","    for idx, entry in enumerate(original_data, start=1):\n","        concept_name = entry.get('concept_name', '').strip()\n","        detailed_explanation = entry.get('detailed_explanation', '').strip()\n","        example_scenario_str = entry.get('example_scenario', '').strip()\n","\n","        if not concept_name or not detailed_explanation or not example_scenario_str:\n","            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n","            continue\n","\n","        # Attempt to parse with json.loads\n","        try:\n","            example_scenarios = json.loads(example_scenario_str)\n","            if not isinstance(example_scenarios, list):\n","                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","        except json.JSONDecodeError:\n","            # Fallback to ast.literal_eval\n","            try:\n","                example_scenarios = ast.literal_eval(example_scenario_str)\n","                if not isinstance(example_scenarios, list):\n","                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n","            except (ValueError, SyntaxError) as e:\n","                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n","                continue\n","\n","        # Iterate through each scenario and create a new entry\n","        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n","            if not isinstance(scenario, str):\n","                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n","                continue\n","\n","            new_entry = {\n","                'concept_name': concept_name,\n","                'detailed_explanation': detailed_explanation,\n","                'example_scenario': scenario.strip()\n","            }\n","            new_data.append(new_entry)\n","\n","    return new_data\n","\n","# Transform the data\n","transformed_data = transform_data(original_data)\n","\n","# Optional: Save the transformed data to a JSON file\n","with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n","    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n","\n","print(f\"Transformation complete. {len(transformed_data)} entries created.\")\n","print(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n","\n","print(len(transformed_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","\n","EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","\n","def instruction_prompt_func(examples):\n","    concept_name = examples[\"concept_name\"]\n","    detailed_explanation = examples[\"detailed_explanation\"]\n","    example_scenario = examples[\"example_scenario\"]\n","    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\n","pass\n","\n","\n","# convert transformed_data to a huggingface dataset\n","instruction_dataset = Dataset.from_dict(transformed_data)\n","instruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n","\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = instruction_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 8,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 8,\n","\n","        # Use num_train_epochs and warmup_ratio for longer runs!\n","        max_steps = 120,\n","        warmup_steps = 10,\n","        # warmup_ratio = 0.1,\n","        # num_train_epochs = 1,\n","\n","        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n","        learning_rate = 5e-5,\n","        embedding_learning_rate = 1e-5,\n","\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.00,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")\n","trainer_stats = trainer.train()\n","#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# add current timestamp to model name\n","model.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\n","tokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\n","model.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","tokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n","        \n","# Save to 8bit GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_GGUF_{int(time.time())}\", tokenizer,quantization_method = \"q8_0\", token = \"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\")\n","\n","# Save to 16bit GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n","\n","# Save to q4_k_m GGUF\n","if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n","if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")\n","\n","\n","\n","# # Merge to 16bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n","\n","# # Merge to 4bit\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n","\n","# # Just LoRA adapters\n","# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n","# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n","\n","### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n","concept_name: {}\n","detailed_explanation: {}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","\n","# Text Streaming\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n","\n","inputs = tokenizer(\n","[\n","    instruction_prompt.format(\n","        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n","        \"\", # output - leave this blank for generation!\n","    ),\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   repetition_penalty = 0.1)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
