{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !nvidia-smi\n# !nvcc --version\n# import torch\n# import torchvision\n# import torchaudio\n# print(f\"PyTorch version: {torch.__version__}\")\n# print(f\"PyTorch version: {torchvision.__version__}\")\n# print(f\"PyTorch version: {torchaudio.__version__}\")\n# print(f\"CUDA available: {torch.cuda.is_available()}\")\n# print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # ----------------------------- #\n# # # Part 1.1: Install and Setup Libraries\n# # # ----------------------------- #\n# !pip uninstall -y torch torchvision torchaudio\n# !pip install uv\n# !uv pip install torch=='2.4.1+cu121' torchvision=='0.19.1+cu121' torchaudio=='2.4.1+cu121'  --index-url https://download.pytorch.org/whl/cu121 --system -q\n# !uv pip install unsloth -q --system\n# !uv pip install flash_attn --no-build-isolation -q --system\n# !pip show flash_attn\n# !uv pip install -q wandb bitsandbytes tensorboard ipywidgets nltk spacy huggingface_hub datasets --system\n# !python -m xformers.info # # !python -m xformers.info\n# !uv pip install  -q  --upgrade triton cut-cross-entropy--no-deps --system\n# !uv pip install -U ipywidgets nbformat ipywidgets IProgress tqdm --no-cache-dir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !nvidia-smi\n# !nvcc --version\n# import torch\n# import torchvision\n# import torchaudio\n# print(f\"PyTorch version: {torch.__version__}\")\n# print(f\"PyTorch version: {torchvision.__version__}\")\n# print(f\"PyTorch version: {torchaudio.__version__}\")\n# print(f\"CUDA available: {torch.cuda.is_available()}\")\n# print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !git clone -b 10Nov2024v3  https://githomein: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@github.com/githomein/quantumLeap.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# # Set these environment variables before importing torch-related modules\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\nfrom pathlib import Path\n\ndef ensure_working_directory():\n    \"\"\"\n    Check if we're in the correct working directory, if not switch to it.\n    Creates the directory if it doesn't exist.\n    \"\"\"\n    target_dir = '/kaggle/working/quantumLeap'\n    current_dir = os.getcwd()\n    \n    # Print current directory\n    print(f\"Current directory: {current_dir}\")\n    \n    # Check if we need to switch directories\n    if current_dir != target_dir:\n        # Create directory if it doesn't exist\n        Path(target_dir).mkdir(parents=True, exist_ok=True)\n        \n        try:\n            # Change to target directory\n            os.chdir(target_dir)\n            print(f\"Successfully switched to: {target_dir}\")\n        except Exception as e:\n            print(f\"Error switching to directory: {str(e)}\")\n            raise\n    else:\n        print(\"Already in correct directory\")\n    \n    # Verify current directory\n    print(f\"Working directory: {os.getcwd()}\")\n\n# Call the function before your main code\nensure_working_directory()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 1.2: Import Necessary Libraries\n# ----------------------------- #\n\n# General Libraries\nimport os\nimport json\nimport sys\nimport subprocess\nimport argparse\nimport logging\nimport math\nimport random\nfrom datetime import datetime\nimport re\nimport gc\nimport weakref\nimport multiprocessing\n\n# Torch related\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\n\n# Transformers related\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    Adafactor\n)\n\n# Huggingface TRL for full finetune\nfrom trl import SFTTrainer, SFTConfig\n\n# General huggingface libraries\nimport huggingface_hub\nfrom datasets import load_dataset, Dataset\nfrom accelerate import Accelerator\n\n\n# Unsloth specificic libraries\nimport unsloth\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n\n# Other Libraries\nfrom peft import LoraConfig\nimport wandb\nimport nltk\nimport spacy\n# from galore_torch import GaLoreAdamW, GaLoreAdafactor, GaLoreAdamW8bit\n\n# Check and import NLTK and spacy modules\n# Ensure NLTK's punkt tokenizer is available\nimport nltk\ntry:\n    nltk.data.find('tokenizers/punkt')\n    print('punkt was already available.')\nexcept LookupError:\n    nltk.download('punkt')\n    print('punkt was not available. It has been downloaded')\n\n# Initialize spaCy English model\ntry:\n    nlp = spacy.load('en_core_web_sm')\n    print('en_core_web_sm was already available.')\nexcept OSError:\n    print(\"SpaCy English model not found. Downloading...\")\n    os.system('python -m spacy download en_core_web_sm')\n    nlp = spacy.load('en_core_web_sm')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Load and Clean the Text Data\n# ----------------------------- #\n\ndef load_and_clean_text(file_path):\n    \"\"\"\n    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        text = f.read()\n    # # Remove Project Gutenberg's license text and headers/footers\n    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n\n    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n    return text.strip()\n\n# Replace 'psychology_of_unconscious.txt' with your actual file pathrm -rfcd \nfile_path = '/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'\nclean_text = load_and_clean_text(file_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ----------------------------- #\n# # Part 3: Parse Text into Discourse Units\n# # ----------------------------- #\n\ndef parse_discourse_units(text, overwrite=False):\n    \"\"\"\n    Parses text into discourse units using spaCy.\n    Currently splits text into sentences.\n    \"\"\"\n    paragraphs = text.split('\\n\\n')\n    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n\n    discourse_units = []\n    for para in paragraphs:\n        doc = nlp(para)\n        sentences = [sent.text for sent in doc.sents]\n        discourse_units.extend(sentences)\n\n    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_discourse_units.json'\n    if not os.path.exists(output_path) or overwrite:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        with open(output_path, 'w') as f:\n            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n    else:\n        with open(output_path, 'r') as f:\n            discourse_units = json.load(f)\n\n    print(f\"Total Discourse Units: {len(discourse_units)}\")\n    return discourse_units","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 4: Create Chunks Using Hybrid Strategy\n# ----------------------------- #\n\ndef create_chunks(discourse_units, tokenizer, max_length=4096, overlap_size=1, overwrite=False):\n    \"\"\"\n    Creates chunks from discourse units using a sliding window with overlapping chunks.\n    Optimized to work directly with token IDs and utilize efficient list operations.\n    \"\"\"\n    chunks = []\n    current_chunk_tokens = []\n    current_length = 0\n\n    for unit in discourse_units:\n        unit_tokens = tokenizer.encode(unit, add_special_tokens=True)\n        unit_length = len(unit_tokens)\n\n        if current_length + unit_length <= max_length:\n            current_chunk_tokens.extend(unit_tokens)\n            current_length += unit_length\n        else:\n            # Decode and append the current chunk\n            chunk_text = tokenizer.decode(\n                current_chunk_tokens, skip_special_tokens=True)\n            chunks.append(chunk_text)\n\n            # Prepare overlap tokens\n            overlap_tokens = current_chunk_tokens[-overlap_size:]\n            current_chunk_tokens = overlap_tokens + unit_tokens\n            current_length = len(current_chunk_tokens)\n\n    # Append any remaining tokens as the last chunk\n    if current_chunk_tokens:\n        chunk_text = tokenizer.decode(\n            current_chunk_tokens, skip_special_tokens=True)\n        chunks.append(chunk_text)\n\n    # Write or read chunks as before\n    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_chunks.json'\n    if not os.path.exists(output_path) or overwrite:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        with open(output_path, 'w') as f:\n            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n    else:\n        with open(output_path, 'r') as f:\n            discourse_units = json.load(f)\n\n    print(f\"Total Chunks Created: {len(chunks)}\")\n    return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Create and Tokenize Dataset\n# ----------------------------- #\n\n# To Do - make book titles and prompt generic so\ndef create_tokenized_dataset(chunks, tokenizer, max_length=1024, num_proc=2):\n\n    # Create a Dataset object from chunks\n\n    book_title = 'Psychology of the Unconscious by C. G. Jung'\n    wikipedia_prompt = \"\"\"\n    Psychology Book\n\n    ### Title: {}\n\n    ### Article: {}\n    \"\"\"\n\n    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\n    def formatting_prompts_func(examples):\n        titles = book_title\n        texts = examples[\"text\"]\n        outputs = []\n        for title, text in zip([book_title]*len(chunks), texts):\n            text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n            outputs.append(text)\n        return {\"text\": outputs, }\n    pass\n\n    # convert chunks variable to huggingface dataset\n\n    from datasets import Dataset\n\n    dataset = Dataset.from_dict({\"text\": chunks})\n\n    dataset = dataset.map(formatting_prompts_func,\n                          batched=True, num_proc=num_proc)\n    # Split the dataset into training and validation sets\n    split = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = split['train']\n    eval_dataset = split['test']\n\n    print(len(dataset))\n    # Find the maximum length of the text field in the entire dataset\n    max_length = max(len(text) for text in dataset['text'])\n    print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n    print(f\"Training Dataset Size: {len(train_dataset)}\")\n#     print(f\"First 5 rows of training dataset: {train_dataset[:5]}\")\n    print(f\"Validation Dataset Size: {len(eval_dataset)}\")\n#     print(f\"First 5 rows of validation dataset: {eval_dataset[:5]}\")\n    return train_dataset, eval_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 6: Set up environment and other important utilities\n# ----------------------------- #\n\ndef setup_environment():\n    \"\"\"\n    Initializes the Accelerator for distributed training.\n    \"\"\"\n    return Accelerator()\n\n\ndef get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n    \"\"\"\n    Defines a custom learning rate scheduler with warmup, constant, and linear annealing phases.\n    \"\"\"\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return current_step / num_warmup_steps  # Linear warmup\n        elif current_step < initial_phase_steps:\n            return 1.0  # Constant learning rate for initial phase\n        else:\n            # Linear annealing for the remaining steps\n            return 1.0 - ((current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps))\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\ndef setup_training_steps(initial_rows, annealing_rows, batch_size, gradient_accumulation_steps, num_epochs):\n    \"\"\"\n    Calculates total and initial training steps based on dataset size and training parameters.\n    \"\"\"\n    total_rows = initial_rows + annealing_rows\n    total_steps = (total_rows * num_epochs) // (batch_size *\n                                                gradient_accumulation_steps)\n    initial_steps = (initial_rows * num_epochs) // (batch_size *\n                                                    gradient_accumulation_steps)\n    return max(1, total_steps), max(1, initial_steps)\n\n\ndef print_memory_usage(step_desc):\n    \"\"\"\n    Prints the CUDA memory summary if CUDA is available.\n    \"\"\"\n    if torch.cuda.is_available():\n        print(f\"Memory Usage at {step_desc}:\")\n        print(torch.cuda.memory_summary())\n        print(\"\\n\")\n    else:\n        print(f\"No CUDA available at {step_desc}.\\n\")\n\n\ndef inference(model, tokenizer):\n    \"\"\"\n    Runs inference using the trained model.\n    \"\"\"\n    # Define sample prompts\n    prompts = [\n        \"Explain what is medical anthropology and its importance in elevating the quality of life?\",\n        \"Explain what are the types of Jungian archetypes and how they manifest in the human psyche?\"\n    ]\n\n    for prompt in prompts:\n        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_length=256)\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n        \n#  Login to Huggingface\nfrom huggingface_hub import login\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef setup_huggingface_access():\n    \"\"\"Setup Hugging Face access with token from environment or manual input\"\"\"\n    # First try to get token from environment variable\n    token = os.getenv('HUGGINGFACE_TOKEN')\n    \n    if not token:\n        # If not in environment, prompt for token\n        token = input(\"Enter your Hugging Face token: \")\n        \n    if token:\n        try:\n            login(token, add_to_git_credential=True)\n            print(\"Successfully logged in to Hugging Face!\")\n        except Exception as e:\n            print(f\"Failed to log in to Hugging Face: {str(e)}\")\n            return False\n    else:\n        print(\"No Hugging Face token provided\")\n        return False\n    \n    return True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Load the Tokenizer and Model\n# ----------------------------- #\n\ndef load_model_and_tokenizer(base_model_slug, max_seq_length=4096, dtype=None, load_in_4bit=True, device_map = \"auto\"):\n    \"\"\"\n    Load and configure the model and tokenizer with specified parameters.\n    \n    Args:\n        base_model_slug (str): The model identifier from HuggingFace\n        max_seq_length (int): Maximum sequence length for the model\n        dtype: Data type for model parameters\n        load_in_4bit (bool): Whether to use 4-bit quantization\n        \n    Returns:\n        tuple: (model, tokenizer)\n    \"\"\"\n    # Check CUDA is available\n    import torch\n    if not torch.cuda.is_available():\n        print(\"WARNING: CUDA is not available. This might affect performance.\")\n    else:\n        print(\"CUDA available\")\n        \n    # Check available GPUs\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of available GPUs: {num_gpus}\")\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_properties(i).name}\")\n\n    # Determine optimal device map based on available GPUs\n    device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using single device: {device_map}\")\n\n    # Model paths\n    model_name = base_model_slug\n    models_dir = os.path.join(os.path.dirname(\"~/\"), \"models\")\n    model_path = os.path.join(models_dir, model_name)\n\n    # Create models directory if it doesn't exist\n    if not os.path.exists(models_dir):\n        os.makedirs(models_dir)\n\n    # Load or download model\n    try:\n        if os.path.exists(model_path):\n            print(f\"Loading model from local path: {model_path}\")\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name=base_model_slug,\n                max_seq_length=max_seq_length,\n                dtype=dtype,\n                load_in_4bit=load_in_4bit,\n                token=os.getenv('HUGGINGFACE_TOKEN'),\n            )\n        else:\n            print(f\"Downloading model from HuggingFace: {model_name}\")\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name=base_model_slug,\n                max_seq_length=max_seq_length,\n                dtype=dtype,\n                load_in_4bit=load_in_4bit,\n                token=os.getenv('HUGGINGFACE_TOKEN'),\n            )\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        raise\n\n    # Configure PEFT model\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=128,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n        target_modules=[\n            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n            \"gate_proj\", \"up_proj\", \"down_proj\",\n            \"embed_tokens\", \"lm_head\",  # Add for continual pretraining\n        ],\n        lora_alpha=32,\n        lora_dropout=0,  # Supports any, but = 0 is optimized\n        bias=\"none\",    # Supports any, but = \"none\" is optimized\n        use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n        random_state=3407,\n        use_rslora=True,   # We support rank stabilized LoRA\n        loftq_config=None,  # And LoftQ\n    )\n\n    print(\"Model and tokenizer loaded successfully!\")\n    return model, tokenizer\n\n    # Print model device distribution\n    if hasattr(model, 'hf_device_map'):\n        print(\"\\nModel Device Distribution:\")\n        for name, device in model.hf_device_map.items():\n            print(f\"{name}: {device}\")\n\n    # Print memory usage per GPU\n    print(\"\\nGPU Memory Usage after model loading:\")\n    for i in range(num_gpus):\n        memory_used = torch.cuda.memory_allocated(i) / (1024**3)\n        memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n        print(f\"GPU {i}: {memory_used:.2f}GB / {memory_total:.2f}GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Set the environment variable\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\nos.environ[\"WANDB_API_KEY\"] = \"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\"\n# Verify it's set correctly\nprint(os.getenv(\"HUGGINGFACE_TOKEN\"))\nprint(os.getenv(\"WANDB_API_KEY\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Configure Environment Variables & Create Main Variables\n# ----------------------------- #\n\n# Unsloth modell initialization variables\nmax_seq_length = 256 # Choose any! We auto support RoPE Scaling internally!\nmax_length = max_seq_length\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nbase_model_slug = \"meta-llama/Llama-3.2-1B-Instruct\"\nbase_model_name = \"lora_model_pum\"\nchunks_max_length = max_seq_length\noverlap_size = 1\n# Define your parameters\nbatchSize = 2\nga = 16\nmaxSteps = 10\n# warmupSteps = 10\nnumTrainEpochs = 1\nlRate = 5e-5\nembLRate = 1e-5\noptim = \"adamw_8bit\"\nlrSchedule = \"linear\"\ndataset_slug = \"psychology_of_unconscious\"\n\nfrom datetime import datetime\nimport pytz\nimport wandb\n# Get the current date and time in Indian Standard Time (IST)\nist = pytz.timezone('Asia/Kolkata')\ncurrent_datetime = datetime.now(ist)\n\n# Format the datetime string\n# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\nformatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n\n# Define Run Name\nrun_name = f\"\"\"Kaggle-quantumLeap-{formatted_datetime}-{base_model_slug}-{dataset_slug}-{max_seq_length}_maxSeqLength-{max_length}_maxLength-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{numTrainEpochs}_numTrainEpochs-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n\n# Initialize Weights & Biases\n# It's recommended to set your W&B API key as an environment variable for security.\nwandb.login(key=os.getenv(\"WANDB_API_KEY\"))\nwandb.init(project=\"KAGGLE-quantumLeap\", name=run_name)\n\n# Set environment variables before importing torch-related modules\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ----------------------------- #\n# Part 9: Data Processing\n# ----------------------------- #\n\n# # Perform Inference Before Training\n# inference(model, tokenizer)\n\n# Set number of processes to use for data loading\nnum_cpus = multiprocessing.cpu_count()\nnum_proc = max(num_cpus-2, 2)  # Adjust based on prior recommendations\nprint(f\"Number of CPU cores: {num_cpus}\")\nprint(f\"Number of processes: {num_proc}\")\n\n# Login to Hugging Face\nif not setup_huggingface_access():\n    raise Exception(\"Failed to setup Hugging Face access. Please check your token.\")\n\n# Load Model and Tokenizer\nmodel, tokenizer = load_model_and_tokenizer(base_model_slug)\nprint(f\"Model Device: {model.device}\")\n\n# Load and Clean Text Data\nfile_path = \"/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\nclean_text = load_and_clean_text(file_path)\n\n# Parse Discourse Units\ndiscourse_units = parse_discourse_units(clean_text, overwrite=True)\n\n# Create Chunks\nchunks = create_chunks(\n    discourse_units,\n    tokenizer,\n    max_length=max_length,\n    overlap_size=overlap_size,\n    overwrite=True,\n)\n\n# Create Tokenized Dataset\ntrain_dataset, eval_dataset = create_tokenized_dataset(\n    chunks, tokenizer, max_length)\n\n# Save datasets as Hugging Face `datasets`\ntrain_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\neval_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')\n\n### To Do - Make the below as dynamic and as a functio\n# # Uncomment following if you want to just load the data from temp directory\n# from datasets import load_from_disk\n\n# train_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n# eval_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 8: Configure Training Arguments\n# ----------------------------- #\n\nimport torch\n\n# Ensure device is set to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transfer model to GPU\nmodel = model.to(device)\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset.to(device),  # Ensure dataset tensors are on GPU\n    eval_dataset = eval_dataset.to(device),\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = batchSize,\n        gradient_accumulation_steps = ga,\n\n        # Use warmup_ratio and num_train_epochs for longer runs!\n        max_steps = maxSteps,\n#         warmup_steps = warmupSteps,\n        # warmup_ratio = 0.1,\n        num_train_epochs = numTrainEpochs,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = lRate,\n        embedding_learning_rate = embLRate,\n\n        # Ensure fp16 precision is used\n        fp16 = True,\n        bf16 = False,\n\n        logging_steps = 1,\n        optim = optim,\n        weight_decay = 0.01,\n        lr_scheduler_type = lrSchedule,\n        seed = 3407,\n        output_dir = \"outputs\",\n\n        report_to=[\"tensorboard\", \"wandb\"],\n        logging_dir=f\"./trel-fft-logs/{run_name}\",\n    ),\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 11: Start Training\n# ----------------------------- #\n\n#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\ntrainer_stats = trainer.train()\n\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.device","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}