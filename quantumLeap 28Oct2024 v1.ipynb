{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /root/miniconda/envs/olabs\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m74 packages\u001b[0m \u001b[2min 1.42s\u001b[0m\u001b[0m                                        \u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)                                                  \n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)------\u001b[0m\u001b[0m     0 B/1.24 MiB                   \u001b[1A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/158.33 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)------\u001b[0m\u001b[0m     0 B/1.24 MiB                   \u001b[2A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.83 KiB/158.33 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)------\u001b[0m\u001b[0m     0 B/1.24 MiB                   \u001b[2A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/160.85 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)------\u001b[0m\u001b[0m     0 B/1.24 MiB                   \u001b[3A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/160.85 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)------\u001b[0m\u001b[0m     0 B/1.24 MiB                   \u001b[3A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)------\u001b[0m\u001b[0m     0 B/1.24 MiB                   \u001b[3A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)------\u001b[0m\u001b[0m     0 B/1.24 MiB                   \u001b[4A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)-----\u001b[0m\u001b[0m     0 B/9.03 MiB                    \u001b[5A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/9.03 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m     0 B/15.94 MiB                     \u001b[6A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/9.03 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m     0 B/15.94 MiB                     \u001b[7A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/9.03 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m     0 B/15.94 MiB                     \u001b[8A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/15.94 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m     0 B/38.19 MiB                     \u001b[9A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/15.94 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m     0 B/38.19 MiB                     \u001b[9A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/15.94 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m     0 B/38.19 MiB                     \u001b[9A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/15.94 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m     0 B/38.19 MiB                     \u001b[9A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/15.94 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 16.00 KiB/38.19 MiB                   \u001b[9A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.24 MiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/15.94 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 16.00 KiB/38.19 MiB                   \u001b[9A\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 39.07 KiB/39.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 62.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 56.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 1.07 MiB/1.24 MiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 94.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 32.00 KiB/38.19 MiB                   \u001b[9A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 62.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 56.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 31.89 KiB/310.91 KiB\n",
            "\u001b[2msentencepiece\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 1.24 MiB/1.24 MiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 110.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 64.00 KiB/38.19 MiB                   \u001b[8A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 78.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 72.74 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 31.89 KiB/310.91 KiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 126.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 144.00 KiB/38.19 MiB                  \u001b[7A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 142.83 KiB/158.33 KiB\n",
            "\u001b[2munsloth   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 160.85 KiB/160.85 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 64.00 KiB/310.91 KiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 1.78 MiB/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 206.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 852.81 KiB/38.19 MiB                  \u001b[7A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 158.33 KiB/158.33 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 80.00 KiB/310.91 KiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 2.66 MiB/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 222.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 1.54 MiB/38.19 MiB                    \u001b[6A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 158.33 KiB/158.33 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 80.00 KiB/310.91 KiB\n",
            "\u001b[2mtokenizers\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 3.43 MiB/3.44 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 238.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 2.18 MiB/38.19 MiB                    \u001b[6A\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 158.33 KiB/158.33 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 96.00 KiB/310.91 KiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 349.92 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 2.97 MiB/38.19 MiB                    \u001b[5A\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 112.00 KiB/310.91 KiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 365.92 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/10)---\u001b[0m\u001b[0m 3.97 MiB/38.19 MiB                    \u001b[4A\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 112.00 KiB/310.91 KiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 365.92 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/10)---\u001b[0m\u001b[0m 5.25 MiB/38.19 MiB                    \u001b[4A\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 128.00 KiB/310.91 KiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 398.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/10)---\u001b[0m\u001b[0m 8.68 MiB/38.19 MiB                    \u001b[4A\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 192.00 KiB/310.91 KiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 542.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/10)---\u001b[0m\u001b[0m 11.64 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 287.89 KiB/310.91 KiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 638.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/10)---\u001b[0m\u001b[0m 14.75 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 304.00 KiB/310.91 KiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 654.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/10)---\u001b[0m\u001b[0m 18.16 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.91 KiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 670.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/10)---\u001b[0m\u001b[0m 18.38 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 979.56 KiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 686.26 KiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/10)---\u001b[0m\u001b[0m 20.39 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 979.56 KiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.38 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/10)---\u001b[0m\u001b[0m 21.27 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 979.56 KiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.60 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/10)---\u001b[0m\u001b[0m 23.94 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 979.56 KiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 2.69 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)---\u001b[0m\u001b[0m 23.94 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 1.99 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 2.83 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)---\u001b[0m\u001b[0m 26.11 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 1.99 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 2.86 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)---\u001b[0m\u001b[0m 29.42 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 1.99 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 2.90 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)---\u001b[0m\u001b[0m 32.28 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 1.99 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 2.95 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 945.48 KiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)m--\u001b[0m\u001b[0m 35.17 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 1.99 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 3.52 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.31 MiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)2m-\u001b[0m\u001b[0m 36.39 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 2.47 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 3.56 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 4.22 MiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)2m-\u001b[0m\u001b[0m 36.39 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 2.95 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 3.57 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 6.97 MiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)2m-\u001b[0m\u001b[0m 36.40 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 2.95 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 3.59 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 10.66 MiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)2m-\u001b[0m\u001b[0m 36.40 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 2.95 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 3.62 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 14.25 MiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)2m-\u001b[0m\u001b[0m 36.40 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 2.95 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 4.16 MiB/9.03 MiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 15.78 MiB/15.94 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)2m-\u001b[0m\u001b[0m 36.67 MiB/38.19 MiB                   \u001b[4A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 2.95 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 4.44 MiB/9.03 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)2m-\u001b[0m\u001b[0m 36.83 MiB/38.19 MiB                   \u001b[3A\n",
            "\u001b[2mhf-transfer\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 2.95 MiB/3.40 MiB\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 5.17 MiB/9.03 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (6/10)[2m\u001b[0m\u001b[0m 37.28 MiB/38.19 MiB                   \u001b[3A\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 5.35 MiB/9.03 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/10)[2m\u001b[0m\u001b[0m 37.39 MiB/38.19 MiB                   \u001b[2A\n",
            "\u001b[2mtransformers\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 5.95 MiB/9.03 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/10)[2m\u001b[0m\u001b[0m 37.97 MiB/38.19 MiB                   \u001b[2A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/10)-----\u001b[0m\u001b[0m 6.17 MiB/9.03 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/10)-----\u001b[0m\u001b[0m 7.06 MiB/9.03 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m10 packages\u001b[0m \u001b[2min 1.33s\u001b[0m\u001b[0m                                                \u001b[1A\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m55 packages\u001b[0m \u001b[2min 262ms\u001b[0m\u001b[0m                              \u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.4.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.10.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==24.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.44.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.16\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.5.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhf-transfer\u001b[0m\u001b[2m==0.1.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.13.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==3.20.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==18.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2024.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.9.11\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.9.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.4.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mshtab\u001b[0m\u001b[2m==1.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.19.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.44.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.11.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyro\u001b[0m\u001b[2m==0.8.14\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2024.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1munsloth\u001b[0m\u001b[2m==2024.10.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1munsloth-zoo\u001b[0m\u001b[2m==2024.10.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.28.post2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /root/miniconda/envs/olabs\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m27 packages\u001b[0m \u001b[2min 2.76s\u001b[0m\u001b[0m                                        \u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)                                                   \n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m     0 B/4.29 MiB                      \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 8.00 KiB/4.29 MiB                     \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 16.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 24.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 32.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 40.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 48.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 56.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 64.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 72.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 80.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 88.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 96.00 KiB/4.29 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 104.00 KiB/4.29 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 134.87 KiB/4.29 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 198.87 KiB/4.29 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)----\u001b[0m\u001b[0m 270.87 KiB/4.29 MiB                   \u001b[1A\n",
            "\u001b[2mtorchaudio\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 8.00 KiB/3.26 MiB\n",
            "\u001b[2mpillow    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 918.87 KiB/4.29 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-----\u001b[0m\u001b[0m 1.05 MiB/6.95 MiB                    \u001b[3A\n",
            "\u001b[2mtorchaudio\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/3.26 MiB\n",
            "\u001b[2mpillow    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 2.28 MiB/4.29 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-----\u001b[0m\u001b[0m 2.23 MiB/6.95 MiB                    \u001b[3A\n",
            "\u001b[2mtorchaudio\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 1006.84 KiB/3.26 MiB\n",
            "\u001b[2mpillow    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 3.81 MiB/4.29 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-----\u001b[0m\u001b[0m 2.31 MiB/6.95 MiB                    \u001b[3A\n",
            "\u001b[2mtorchaudio\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 2.34 MiB/3.26 MiB\n",
            "\u001b[2mpillow    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 4.26 MiB/4.29 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-----\u001b[0m\u001b[0m 2.59 MiB/6.95 MiB                    \u001b[3A\n",
            "\u001b[2mtorchaudio\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 2.51 MiB/3.26 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-----\u001b[0m\u001b[0m 2.59 MiB/6.95 MiB                    \u001b[2A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-----\u001b[0m\u001b[0m 2.71 MiB/6.95 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-----\u001b[0m\u001b[0m 3.73 MiB/6.95 MiB                    \u001b[1A\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 302ms\u001b[0m\u001b[0m                                                 \u001b[1A\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0mcu124                            \u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.5.0+cu124\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.20.0+cu124\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /root/miniconda/envs/olabs\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m112 packages\u001b[0m \u001b[2min 1.06s\u001b[0m\u001b[0m                                       \u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)                                                  \n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m     0 B/294.74 KiB                    \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[1A\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/34.62 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[2A\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[2A\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/46.18 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[3A\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[3A\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/21.82 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[4A\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[4A\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[5A\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.83 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[5A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/9.53 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.83 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[6A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/9.53 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.83 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[7A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.53 KiB/9.53 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.83 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[7A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.53 KiB/9.53 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 14.92 KiB/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.83 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[7A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.53 KiB/9.53 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 14.92 KiB/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.83 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/51.32 KiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[8A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.53 KiB/9.53 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 14.92 KiB/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.83 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 14.91 KiB/51.32 KiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[8A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.53 KiB/9.53 KiB\n",
            "\u001b[2mcatalogue \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/16.92 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 14.92 KiB/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.83 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.91 KiB/34.62 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 14.91 KiB/51.32 KiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)---\u001b[0m\u001b[0m 14.88 KiB/294.74 KiB                  \u001b[9A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.53 KiB/9.53 KiB\n",
            "\u001b[2mcatalogue \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 14.93 KiB/16.92 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 14.92 KiB/27.23 KiB\n",
            "\u001b[2mmurmurhash\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.93 KiB/28.45 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 14.83 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 30.91 KiB/34.62 KiB\n",
            "\u001b[2mcymem     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 45.56 KiB/45.56 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/46.18 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 32.73 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 16.00 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 62.91 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.87 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 30.88 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 506.67 KiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 524.03 KiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/3.54 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)--\u001b[0m\u001b[0m 16.00 KiB/8.82 MiB\u001b[23A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.53 KiB/9.53 KiB\n",
            "\u001b[2mcatalogue \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 16.92 KiB/16.92 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 14.92 KiB/27.23 KiB\n",
            "\u001b[2mmurmurhash\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 28.45 KiB/28.45 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mconfection\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 34.62 KiB/34.62 KiB\n",
            "\u001b[2mcymem     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 45.56 KiB/45.56 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 30.92 KiB/46.18 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 32.73 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 16.00 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 174.91 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.87 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 46.88 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 1.01 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 78.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 956.03 KiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/3.54 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)-----\u001b[0m\u001b[0m 38.84 KiB/5.14 MiB\u001b[23A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.53 KiB/9.53 KiB\n",
            "\u001b[2mcatalogue \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 16.92 KiB/16.92 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mmurmurhash\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 28.45 KiB/28.45 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mcymem     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 45.56 KiB/45.56 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 30.92 KiB/46.18 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 32.73 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 30.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 63.70 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 174.91 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.68 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 46.88 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 1.14 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 94.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 1.06 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 534.84 KiB/5.14 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)--\u001b[0m\u001b[0m 1.08 MiB/8.82 MiB\u001b[23A\n",
            "\u001b[2mshellingham\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.53 KiB/9.53 KiB\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mmurmurhash\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 28.45 KiB/28.45 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mcymem     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 45.56 KiB/45.56 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 30.92 KiB/46.18 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 32.73 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 30.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 63.70 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 174.91 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.68 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 62.88 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 1.14 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 94.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 1.06 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 534.84 KiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.08 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)--\u001b[0m\u001b[0m 16.00 KiB/30.34 MiB                   \u001b[23A\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mmurmurhash\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 28.45 KiB/28.45 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mcymem     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 45.56 KiB/45.56 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 43.57 KiB/46.18 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 32.73 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 30.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 63.70 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 174.91 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 92.75 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 62.88 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 30.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 1.18 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 110.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 1.10 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 577.87 KiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.12 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[22A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)--\u001b[0m\u001b[0m 16.00 KiB/30.34 MiB                   \u001b[22A\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 14.93 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mmurmurhash\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 28.45 KiB/28.45 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mcymem     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 45.56 KiB/45.56 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 43.57 KiB/46.18 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 32.73 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 30.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 63.70 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 174.91 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 124.75 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 78.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 30.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 1.26 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 110.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 1.10 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 662.84 KiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.14 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[22A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)--\u001b[0m\u001b[0m 1010.85 KiB/30.34 MiB                 \u001b[22A\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 21.82 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mmurmurhash\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 28.45 KiB/28.45 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 43.57 KiB/46.18 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 32.73 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 30.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 63.70 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 174.91 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 124.75 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 78.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 30.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 1.26 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 110.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 1.12 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 662.84 KiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.24 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[21A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)--\u001b[0m\u001b[0m 1010.85 KiB/30.34 MiB                 \u001b[21A\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 21.82 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mtyper     \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 43.57 KiB/46.18 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.01 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 153.13 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 178.12 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 162.57 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 110.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 30.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 1.32 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 142.81 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 1.34 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 982.84 KiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.51 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[20A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)--\u001b[0m\u001b[0m 1.03 MiB/30.34 MiB                    \u001b[20A\n",
            "\u001b[2mspacy-loggers\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 21.82 KiB/21.82 KiB\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.01 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 46.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 153.13 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 178.12 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 178.57 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 110.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 30.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 1.33 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 142.81 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 1.67 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 1.13 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.69 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[19A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)--\u001b[0m\u001b[0m 1.03 MiB/30.34 MiB                    \u001b[19A\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.01 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 46.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 153.13 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 178.12 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 178.57 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 110.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 46.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 1.33 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 158.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 1.67 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 1.13 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.69 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[18A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/25)--\u001b[0m\u001b[0m 1.03 MiB/30.34 MiB                    \u001b[18A\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.01 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 46.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 153.13 KiB/153.13 KiB\n",
            "\u001b[2mlangcodes \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 178.12 KiB/178.12 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 194.57 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 110.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 46.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 1.33 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 158.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 1.67 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 1.15 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.69 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[18A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.03 MiB/30.34 MiB                    \u001b[18A\n",
            "\u001b[2mwasabi    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.23 KiB/27.23 KiB\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.01 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 46.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 153.13 KiB/153.13 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 194.57 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 110.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 46.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 1.33 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 158.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 1.75 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 1.19 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.75 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[17A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.20 MiB/30.34 MiB                    \u001b[17A\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.01 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 46.06 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 153.13 KiB/153.13 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 205.68 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 110.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 46.93 KiB/480.21 KiB\n",
            "\u001b[2mmarisa-trie\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 1.33 MiB/1.33 MiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 158.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 1.75 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 1.19 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.75 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[16A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.20 MiB/30.34 MiB                    \u001b[16A\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.01 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 59.95 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/136.49 KiB\n",
            "\u001b[2mpreshed   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 153.13 KiB/153.13 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 205.68 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 126.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 46.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 158.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 1.89 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 64.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 1.26 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 1.87 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[15A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.22 MiB/30.34 MiB                    \u001b[15A\n",
            "\u001b[2mspacy-legacy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 29.27 KiB/29.27 KiB\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 30.92 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.01 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 59.95 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/136.49 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 205.68 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 126.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 46.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 174.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 1.89 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 64.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 1.29 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 1.89 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[14A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.22 MiB/30.34 MiB                    \u001b[14A\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 46.82 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.01 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 59.95 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/136.49 KiB\n",
            "\u001b[2mjupyterlab-widgets\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 205.68 KiB/209.37 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 142.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 62.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 190.92 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 2.06 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 313.09 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 1.29 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.04 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[13A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.22 MiB/30.34 MiB                    \u001b[13A\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 46.82 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 51.32 KiB/51.32 KiB\n",
            "\u001b[2msmart-open\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 59.95 KiB/59.95 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 62.88 KiB/136.49 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 142.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 62.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 204.61 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 2.13 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 313.09 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.38 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.12 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[12A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.22 MiB/30.34 MiB                    \u001b[12A\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 46.82 KiB/49.09 KiB\n",
            "\u001b[2mcloudpathlib\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 51.32 KiB/51.32 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 62.88 KiB/136.49 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 142.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 62.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 204.61 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 2.13 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 313.09 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.38 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.12 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.25 MiB/30.34 MiB                    \u001b[11A\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 46.82 KiB/49.09 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 62.88 KiB/136.49 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 142.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 62.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 209.38 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 2.18 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 383.89 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.40 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.17 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.25 MiB/30.34 MiB                    \u001b[10A\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 46.82 KiB/49.09 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 62.88 KiB/136.49 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 142.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 73.51 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 225.38 KiB/1.44 MiB\n",
            "\u001b[2mwidgetsnbextension\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 2.18 MiB/2.23 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 608.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.40 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.17 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)--\u001b[0m\u001b[0m 1.25 MiB/30.34 MiB                    \u001b[10A\n",
            "\u001b[2mweasel    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 46.82 KiB/49.09 KiB\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 78.88 KiB/136.49 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 158.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 94.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 257.38 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 758.89 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.46 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 2.81 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)---\u001b[0m\u001b[0m 2.47 MiB/30.34 MiB                    \u001b[9A\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 78.88 KiB/136.49 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 174.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 110.82 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 257.38 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 784.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.47 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 2.90 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)---\u001b[0m\u001b[0m 2.50 MiB/30.34 MiB                    \u001b[8A\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 78.88 KiB/136.49 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 174.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 110.82 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 257.38 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 784.00 KiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.47 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 2.91 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)---\u001b[0m\u001b[0m 2.69 MiB/30.34 MiB                    \u001b[8A\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 94.88 KiB/136.49 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 190.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 190.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 284.61 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 2.41 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.49 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 3.48 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/25)---\u001b[0m\u001b[0m 3.17 MiB/30.34 MiB                    \u001b[8A\n",
            "\u001b[2mipywidgets\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 136.49 KiB/136.49 KiB\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 240.36 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 318.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 332.61 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 2.44 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.52 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 4.73 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (17/25)--\u001b[0m\u001b[0m 4.37 MiB/30.34 MiB                    \u001b[8A\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 240.36 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 318.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 332.61 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 2.44 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 1.52 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 5.01 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (17/25)--\u001b[0m\u001b[0m 4.37 MiB/30.34 MiB                    \u001b[7A\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 256.36 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 366.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 364.61 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 2.46 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 1.58 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 6.59 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (17/25)--\u001b[0m\u001b[0m 4.88 MiB/30.34 MiB                    \u001b[7A\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 256.36 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 382.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 395.98 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 2.47 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 1.58 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 7.70 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (17/25)--\u001b[0m\u001b[0m 6.15 MiB/30.34 MiB                    \u001b[7A\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 270.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 414.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 476.61 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 2.48 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 1.65 MiB/5.14 MiB\n",
            "\u001b[2mblis      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 8.81 MiB/8.82 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (17/25)--\u001b[0m\u001b[0m 7.14 MiB/30.34 MiB                    \u001b[7A\n",
            "\u001b[2mjoblib    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 286.16 KiB/294.74 KiB\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 414.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 524.61 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 2.50 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 1.71 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (17/25)--\u001b[0m\u001b[0m 7.16 MiB/30.34 MiB                    \u001b[6A\n",
            "\u001b[2msrsly     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 430.93 KiB/480.21 KiB\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 588.50 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 2.52 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 1.73 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (17/25)--\u001b[0m\u001b[0m 7.19 MiB/30.34 MiB                    \u001b[5A\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 604.61 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 2.52 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 1.76 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (17/25)--\u001b[0m\u001b[0m 7.20 MiB/30.34 MiB                    \u001b[4A\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 732.61 KiB/1.44 MiB\n",
            "\u001b[2mthinc     \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 3.00 MiB/3.54 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 1.83 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/25)--\u001b[0m\u001b[0m 7.27 MiB/30.34 MiB                    \u001b[4A\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 812.61 KiB/1.44 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 1.88 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/25)--\u001b[0m\u001b[0m 7.30 MiB/30.34 MiB                    \u001b[3A\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 988.61 KiB/1.44 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 2.10 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/25)--\u001b[0m\u001b[0m 7.38 MiB/30.34 MiB                    \u001b[3A\n",
            "\u001b[2mnltk      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 1.20 MiB/1.44 MiB\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 2.46 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/25)--\u001b[0m\u001b[0m 7.42 MiB/30.34 MiB                    \u001b[3A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 2.65 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/25)--\u001b[0m\u001b[0m 7.47 MiB/30.34 MiB                    \u001b[2A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 2.76 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/25)--\u001b[0m\u001b[0m 7.53 MiB/30.34 MiB                    \u001b[2A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 2.83 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/25)--\u001b[0m\u001b[0m 9.44 MiB/30.34 MiB                    \u001b[2A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 2.88 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/25)--\u001b[0m\u001b[0m 12.34 MiB/30.34 MiB                   \u001b[2A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 2.91 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/25)--\u001b[0m\u001b[0m 14.66 MiB/30.34 MiB                   \u001b[2A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 2.99 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/25)--\u001b[0m\u001b[0m 17.00 MiB/30.34 MiB                   \u001b[2A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 3.16 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/25)--\u001b[0m\u001b[0m 17.45 MiB/30.34 MiB                   \u001b[2A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 3.34 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/25)--\u001b[0m\u001b[0m 17.59 MiB/30.34 MiB                   \u001b[2A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 3.57 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/25)--\u001b[0m\u001b[0m 17.70 MiB/30.34 MiB                   \u001b[2A\n",
            "\u001b[2mlanguage-data\u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 4.74 MiB/5.14 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/25)--\u001b[0m\u001b[0m 17.76 MiB/30.34 MiB                   \u001b[2A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/25)--\u001b[0m\u001b[0m 17.84 MiB/30.34 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/25)--\u001b[0m\u001b[0m 17.85 MiB/30.34 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/25)--\u001b[0m\u001b[0m 20.20 MiB/30.34 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/25)--\u001b[0m\u001b[0m 22.56 MiB/30.34 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/25)--\u001b[0m\u001b[0m 24.61 MiB/30.34 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/25)--\u001b[0m\u001b[0m 26.82 MiB/30.34 MiB                   \u001b[1A\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m25 packages\u001b[0m \u001b[2min 1.49s\u001b[0m\u001b[0m                                                \u001b[1A\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 18ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m34 packages\u001b[0m \u001b[2min 49ms\u001b[0m\u001b[0m                               \u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mblis\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcatalogue\u001b[0m\u001b[2m==2.0.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcloudpathlib\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mconfection\u001b[0m\u001b[2m==0.1.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcymem\u001b[0m\u001b[2m==2.0.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdocker-pycreds\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.11\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.43\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipywidgets\u001b[0m\u001b[2m==8.1.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyterlab-widgets\u001b[0m\u001b[2m==3.0.13\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangcodes\u001b[0m\u001b[2m==3.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlanguage-data\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarisa-trie\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmurmurhash\u001b[0m\u001b[2m==1.0.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnltk\u001b[0m\u001b[2m==3.9.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpreshed\u001b[0m\u001b[2m==3.0.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msentry-sdk\u001b[0m\u001b[2m==2.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetproctitle\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msmart-open\u001b[0m\u001b[2m==7.0.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mspacy\u001b[0m\u001b[2m==3.8.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mspacy-legacy\u001b[0m\u001b[2m==3.0.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mspacy-loggers\u001b[0m\u001b[2m==1.0.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msrsly\u001b[0m\u001b[2m==2.4.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mthinc\u001b[0m\u001b[2m==8.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.12.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.18.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwasabi\u001b[0m\u001b[2m==1.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mweasel\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwidgetsnbextension\u001b[0m\u001b[2m==4.0.13\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwrapt\u001b[0m\u001b[2m==1.16.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# # ----------------------------- #\n",
        "# # Part 1.1: Install and Setup Libraries\n",
        "# # ----------------------------- #\n",
        "\n",
        "# # run below in terminal only. This code works only for Ola Krutrim Cloud Instance. Restart once you have installed the following\n",
        "# # pip install uv #install this in the virtual environment where you want to execute the notebook.\n",
        "# # uv venv virtualenvironment # if you are not in an externally managed environment, then you can run this\n",
        "# # source virtualenvironment/bin/activate # if you were able to run above code, then activate. DO NOT use --system flag in subsequent lines if you are able to do thi\n",
        "# !uv pip install unsloth --system\n",
        "# !uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --system\n",
        "# !uv pip install wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets --system\n",
        "# # restart once you have installed all of the above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Oct 28 11:20:37 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA H100 80GB HBM3          Off |   00000001:45:00.0 Off |                   On |\n",
            "| N/A   30C    P0             70W /  700W |                  N/A   |     N/A      Default |\n",
            "|                                         |                        |              Enabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| MIG devices:                                                                            |\n",
            "+------------------+----------------------------------+-----------+-----------------------+\n",
            "| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|        Shared         |\n",
            "|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC  DEC  OFA  JPG |\n",
            "|                  |                                  |        ECC|                       |\n",
            "|==================+==================================+===========+=======================|\n",
            "|  0   10   0   0  |              13MiB /  9984MiB    | 16      0 |  1   0    1    0    1 |\n",
            "|                  |                 0MiB / 16383MiB  |           |                       |\n",
            "+------------------+----------------------------------+-----------+-----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Fri_Jan__6_16:45:21_PST_2023\n",
            "Cuda compilation tools, release 12.0, V12.0.140\n",
            "Build cuda_12.0.r12.0/compiler.32267302_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.0+cu124\n",
            "12.4\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)          # Should reflect 2.5.0+cu124\n",
        "print(torch.version.cuda)         # Should output 12.4\n",
        "print(torch.cuda.is_available())  # Should return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "punkt was already available.\n",
            "en_core_web_sm was already available.\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------- #\n",
        "# Part 1.2: Import Libraries\n",
        "# ----------------------------- #\n",
        "\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import nltk\n",
        "import spacy\n",
        "import xformers\n",
        "import bitsandbytes\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "import wandb\n",
        "import ipywidgets\n",
        "import unsloth\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from datasets import Dataset\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "# Ensure NLTK's punkt tokenizer is available\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print('punkt was already available.')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    print('punkt was not available. It has been downloaded')\n",
        "\n",
        "# Initialize spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    print('en_core_web_sm was already available.')\n",
        "except OSError:\n",
        "    print(\"SpaCy English model not found. Downloading...\")\n",
        "    os.system('python -m spacy download en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------- #\n",
        "# Part 2: Load and Clean the Text Data\n",
        "# ----------------------------- #\n",
        "\n",
        "def load_and_clean_text(file_path):\n",
        "    \"\"\"\n",
        "    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    # # Remove Project Gutenberg's license text and headers/footers\n",
        "    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
        "    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
        "\n",
        "    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n",
        "    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n",
        "    return text.strip()\n",
        "\n",
        "# Replace 'psychology_of_unconscious.txt' with your actual file path\n",
        "file_path = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/psychology_of_unconscious.txt'\n",
        "clean_text = load_and_clean_text(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6013"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ----------------------------- #\n",
        "# Part 3: Parse Text into Discourse Units\n",
        "# ----------------------------- #\n",
        "\n",
        "# def parse_discourse_units(text):\n",
        "#     \"\"\"\n",
        "#     Parses text into discourse units using spaCy.\n",
        "#     Currently splits text into sentences.\n",
        "#     \"\"\"\n",
        "#     paragraphs = text.split('\\n\\n')\n",
        "#     paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
        "    \n",
        "#     discourse_units = []\n",
        "#     for para in paragraphs:\n",
        "#         doc = nlp(para)\n",
        "#         sentences = [sent.text for sent in doc.sents]\n",
        "#         discourse_units.extend(sentences)\n",
        "#     return discourse_units\n",
        "\n",
        "# discourse_units = parse_discourse_units(clean_text)\n",
        "\n",
        "# # Save discourse_units to a JSON file\n",
        "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.json', 'w', encoding='utf-8') as f:\n",
        "#     json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n",
        "    \n",
        "# Load discourse_units from the JSON file\n",
        "with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.json', 'r', encoding='utf-8') as f:\n",
        "    discourse_units = json.load(f)\n",
        "\n",
        "len(discourse_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------- #\n",
        "# Part 4: Create Chunks Using Hybrid Strategy\n",
        "# ----------------------------- #\n",
        "\n",
        "def create_chunks(discourse_units, tokenizer, max_length=512, overlap_size=0):\n",
        "    \"\"\"\n",
        "    Creates chunks from discourse units using a sliding window with overlapping chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for unit in discourse_units:\n",
        "        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n",
        "        unit_length = len(unit_tokens)\n",
        "\n",
        "        if current_length + unit_length <= max_length:\n",
        "            current_chunk.append(unit)\n",
        "            current_length += unit_length\n",
        "        else:\n",
        "            # Append the current chunk\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            # Create overlap\n",
        "            overlap_text = ' '.join(current_chunk)[-overlap_size:]\n",
        "            overlap_tokens = tokenizer.encode(overlap_text, add_special_tokens=False)\n",
        "            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n",
        "            # Start new chunk with overlap and current unit\n",
        "            current_chunk = [overlap_text, unit]\n",
        "            current_length = len(tokenizer.encode(overlap_text, add_special_tokens=False)) + unit_length\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3 MIG 1g.10gb. Max memory: 9.75 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 9.0. CUDA Toolkit = 12.4.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n",
            "<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n",
            "<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n",
            "<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n",
            "<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n",
            "<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
            "Unsloth: Offloading output_embeddings to disk to save VRAM\n",
            "Unsloth: Casting embed_tokens to float32\n",
            "Unsloth: Casting lm_head to float32\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "base_model_slug = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model_slug, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "\n",
        "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,   # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "98\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------- #\n",
        "# Part 6: Create Chunks (After Tokenizer is Loaded)\n",
        "# ----------------------------- #\n",
        "\n",
        "chunks_max_length = max_seq_length\n",
        "overlap_size = 1\n",
        "chunks = create_chunks(discourse_units, tokenizer, max_length=chunks_max_length, overlap_size=overlap_size)\n",
        "\n",
        "# Save chunks to a JSON file (Optional)\n",
        "with open(f'/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final_{chunks_max_length}_{overlap_size}.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(chunks, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# # If you need to reload from JSON (Optional)\n",
        "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.json', 'r', encoding='utf-8') as f:\n",
        "#     chunks = json.load(f)\n",
        "    \n",
        "print(len(chunks))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a03ccd9a9ed14f7bba2b7945fbfe6960",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/98 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ----------------------------- #\n",
        "# Part 7: Create and Tokenize Dataset\n",
        "# ----------------------------- #\n",
        "\n",
        "# Create a Dataset object from chunks\n",
        "\n",
        "book_title = 'Psychology of the Unconscious by C. G. Jung'\n",
        "wikipedia_prompt = \"\"\"\n",
        "### Title: {}\n",
        "\n",
        "### Article: {}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    titles = book_title\n",
        "    texts  = examples[\"text\"]\n",
        "    outputs = []\n",
        "    for title, text in zip([book_title]*len(chunks), texts):\n",
        "        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n",
        "        outputs.append(text)\n",
        "    return { \"text\" : outputs, }\n",
        "pass\n",
        "\n",
        "# convert chunks variable to huggingface dataset\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": chunks})\n",
        "\n",
        "# dataset = dataset.train_test_split(test_size = 0.1)[\"train\"]\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "\n",
        "len(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The maximum length of the text field in the dataset is: 10104 characters\n"
          ]
        }
      ],
      "source": [
        "# Find the maximum length of the text field in the entire dataset\n",
        "max_length = max(len(text) for text in dataset['text'])\n",
        "print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13f47fa0ed264ea6b7860241c8c4ab72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113324533087304, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize Weights & Biases\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# It's recommended to set your W&B API key as an environment variable for security.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1ca3c5e9222c2504acbc07cf7f88267006ae68c4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUnsloth-CPT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m UnslothTrainer(\n\u001b[1;32m     23\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     24\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     ),\n\u001b[1;32m     58\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:1256\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[1;32m   1255\u001b[0m     wi\u001b[38;5;241m.\u001b[39msetup(kwargs)\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:819\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    816\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicating run to backend with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m second timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    818\u001b[0m run_init_handle \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_run(run)\n\u001b[0;32m--> 819\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_init_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcancel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    825\u001b[0m     run_result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mrun_result\n",
            "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
            "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
            "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
            "File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "\n",
        "\n",
        "batchSize = 2\n",
        "ga = 8\n",
        "maxSteps = 120\n",
        "lRate =  5e-5\n",
        "embLRate= 1e-5\n",
        "optim = \"adamw_8bit\"\n",
        "lrSchedule = \"linear\"\n",
        "\n",
        "run_name = f\"\"\"Unsloth-CPT-Base-28Octv1-{base_model_slug}-{max_seq_length}max_seq_length-{batchSize}batchSize-{ga}ga-{maxSteps}maxSteps-{lRate}lRate-{embLRate}embLRate-{optim}optim-{lrSchedule}lrSchedule\"\"\"\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "# It's recommended to set your W&B API key as an environment variable for security.\n",
        "wandb.login(key=\"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\")\n",
        "wandb.init(project=\"Unsloth-CPT\", name=run_name)\n",
        "\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = batchSize,\n",
        "        gradient_accumulation_steps = ga,\n",
        "\n",
        "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
        "        max_steps = maxSteps,\n",
        "        warmup_steps = 10,\n",
        "        # warmup_ratio = 0.1,\n",
        "        # num_train_epochs = 1,\n",
        "\n",
        "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
        "        learning_rate =lRate,\n",
        "        embedding_learning_rate = embLRate,\n",
        "\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = optim,\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = lrSchedule,\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        \n",
        "        \n",
        "        report_to=[\"tensorboard\", \"wandb\"],\n",
        "        logging_dir=f\"./trel-fft-logs/{run_name}\",\n",
        "\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024,\n",
        "                   3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIO7c1FoRe-X"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model_pum\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "    \n",
        "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction: \n",
        "concept_name: {}\n",
        "detailed_explanation: {}\n",
        "Given the concept in concept_name variable and its detailed explanation in detailed_explanation variable, provide an example scenario that illustrates the concept.\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    instruction_prompt.format(\n",
        "        \"Hero Archetype\", # concept_name\n",
        "        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "model.config.torch_dtype = torch.bfloat16 \n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512) # using repetition_penalty of 0.1 leads to repetition of text and high values lead to wierd grammer issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model_pum\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model_pum\")\n",
        "\n",
        "!huggingface-cli login --token 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 --add-to-git-credential\n",
        "if True:\n",
        "    model.push_to_hub(\"olabs-ai/qLeap_v04\", token = \"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\") # Online saving\n",
        "    tokenizer.push_to_hub(\"olabs-ai/qLeap_v04\", token = \"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\") # Online saving\n",
        "    model.push_to_hub_gguf(\"olabs-ai/qLeap_v04\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instruction Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import ast\n",
        "import logging\n",
        "import csv\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported, UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='transformation_errors.log',\n",
        "    filemode='w',\n",
        "    level=logging.ERROR,\n",
        "    format='%(levelname)s:%(message)s'\n",
        ")\n",
        "\n",
        "# Read the CSV file\n",
        "with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "def transform_data(original_data):\n",
        "    \"\"\"\n",
        "    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "\n",
        "    for idx, entry in enumerate(original_data, start=1):\n",
        "        concept_name = entry.get('concept_name', '').strip()\n",
        "        detailed_explanation = entry.get('detailed_explanation', '').strip()\n",
        "        example_scenario_str = entry.get('example_scenario', '').strip()\n",
        "\n",
        "        if not concept_name or not detailed_explanation or not example_scenario_str:\n",
        "            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            example_scenarios = json.loads(example_scenario_str)\n",
        "            if not isinstance(example_scenarios, list):\n",
        "                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n",
        "        except json.JSONDecodeError:\n",
        "            try:\n",
        "                example_scenarios = ast.literal_eval(example_scenario_str)\n",
        "                if not isinstance(example_scenarios, list):\n",
        "                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n",
        "            except (ValueError, SyntaxError) as e:\n",
        "                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n",
        "                continue\n",
        "\n",
        "        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n",
        "            if not isinstance(scenario, str):\n",
        "                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n",
        "                continue\n",
        "\n",
        "            new_entry = {\n",
        "                'concept_name': concept_name,\n",
        "                'detailed_explanation': detailed_explanation,\n",
        "                'example_scenario': scenario.strip()\n",
        "            }\n",
        "            new_data.append(new_entry)\n",
        "\n",
        "    return new_data\n",
        "\n",
        "# Transform the data\n",
        "transformed_data = transform_data(data)\n",
        "\n",
        "# Save the transformed data\n",
        "with open('/root/qLeap-fft/data/input/Instruction_Data/transformed_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Model initialization parameters\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Initialize the model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"lora_model_pum\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Revised instruction prompt for training\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
        "\n",
        "### Concept:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}{}\"\"\"  # Note the extra {} for EOS_TOKEN\n",
        "\n",
        "def instruction_prompt_func(examples):\n",
        "    return {\n",
        "        \"text\": [\n",
        "            instruction_prompt.format(cn, text, EOS_TOKEN)  # Add EOS_TOKEN here\n",
        "            for cn, text in zip(\n",
        "                examples[\"concept_name\"],\n",
        "                [f\"{de}\\n\\n{es}\" for de, es in zip(examples[\"detailed_explanation\"], examples[\"example_scenario\"])]\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "# Create dataset and apply mapping\n",
        "instruction_dataset = Dataset.from_list(transformed_data)\n",
        "instruction_dataset = instruction_dataset.map(instruction_prompt_func, batched=True)\n",
        "instruction_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/root/quantumLeap/wandb/run-20241027_191242-1hwns7w3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/olabs-asia-olabs-pro/Unsloth-CPT/runs/1hwns7w3' target=\"_blank\">Unsloth-CPT-unsloth/Llama-3.2-1B-bnb-4bit-1024max_seq_length-2batchSize-8ga-120maxSteps-5e-05lRate-1e-05embLRate-adamw_8bitoptim-linearlrSchedule-2runCount</a></strong> to <a href='https://wandb.ai/olabs-asia-olabs-pro/Unsloth-CPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/olabs-asia-olabs-pro/Unsloth-CPT' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/Unsloth-CPT</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/olabs-asia-olabs-pro/Unsloth-CPT/runs/1hwns7w3' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/Unsloth-CPT/runs/1hwns7w3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "491529a120b14ba1a84e126024318ed3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/495 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA H100 80GB HBM3 MIG 1g.10gb. Max memory = 9.75 GB.\n",
            "5.016 GB of memory reserved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 495 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n",
            "\\        /    Total batch size = 16 | Total steps = 120\n",
            " \"-____-\"     Number of trainable parameters = 90,177,536\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n",
            "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 02:58, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.485900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.522600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.173700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.086400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.742700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.762000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.496100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.453700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.347300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.263100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.255200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.210300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.146800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.112800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.022100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.027800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.044700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.986600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.997900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.987200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.961800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.930500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.960900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.943100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.941700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.940600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.834300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.879800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.886900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.732900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.768500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.710400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.738600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.757800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.728300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.789500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.737300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.662900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.652200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.671600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.717100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.653800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.673000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.663200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.693900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.587200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.594800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.630700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.624200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.634000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.594600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.581900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.557200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.635800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.566900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.616800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.581200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.566800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.452700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.467800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.451300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.501800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.470100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.455700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.400300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.435400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.416400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.453100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.412300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.433800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.417900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.466600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.416500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.393400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.432600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.385800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.398500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.396200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.378300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.415700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.348300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.366900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.406300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.415400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.359400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.404500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.397600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.352800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.311700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.315000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.317100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.331700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.300700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.343000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.344200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.324600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.302300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.281400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.301100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.314000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.298800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.290700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.326900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.298900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.282400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.309700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.325100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.322500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.296300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.284800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.309400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.305100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.314500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "180.6082 seconds used for training.\n",
            "3.01 minutes used for training.\n",
            "Peak reserved memory = 5.043 GB.\n",
            "Peak reserved memory for training = 0.027 GB.\n",
            "Peak reserved memory % of max memory = 51.723 %.\n",
            "Peak reserved memory for training % of max memory = 0.277 %.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import nltk\n",
        "import spacy\n",
        "import xformers\n",
        "import bitsandbytes\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "import wandb\n",
        "import ipywidgets\n",
        "import unsloth\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from datasets import Dataset\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "# Ensure NLTK's punkt tokenizer is available\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print('punkt was already available.')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    print('punkt was not available. It has been downloaded')\n",
        "\n",
        "# Initialize spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    print('en_core_web_sm was already available.')\n",
        "except OSError:\n",
        "    print(\"SpaCy English model not found. Downloading...\")\n",
        "    os.system('python -m spacy download en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "\n",
        "base_model_slug = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
        "batchSize = 2\n",
        "ga = 8\n",
        "maxSteps = 120\n",
        "lRate =  5e-5\n",
        "embLRate= 1e-5\n",
        "optim = \"adamw_8bit\"\n",
        "lrSchedule = \"linear\"\n",
        "runCount=2\n",
        "run_name = f\"\"\"Unsloth-CPT-{base_model_slug}-{max_seq_length}max_seq_length-{batchSize}batchSize-{ga}ga-{maxSteps}maxSteps-{lRate}lRate-{embLRate}embLRate-{optim}optim-{lrSchedule}lrSchedule-{runCount}runCount\"\"\"\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "# It's recommended to set your W&B API key as an environment variable for security.\n",
        "wandb.login(key=\"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\")\n",
        "wandb.init(project=\"Unsloth-CPT\", name=run_name)\n",
        "\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = instruction_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = batchSize,\n",
        "        gradient_accumulation_steps = ga,\n",
        "\n",
        "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
        "        max_steps = maxSteps,\n",
        "        warmup_steps = 10,\n",
        "        # warmup_ratio = 0.1,\n",
        "        # num_train_epochs = 1,\n",
        "\n",
        "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
        "        learning_rate =lRate,\n",
        "        embedding_learning_rate = embLRate,\n",
        "\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = optim,\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = lrSchedule,\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        \n",
        "        \n",
        "        report_to=[\"tensorboard\", \"wandb\"],\n",
        "        logging_dir=f\"./trel-fft-logs/{run_name}\",\n",
        "\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024,\n",
        "                   3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "\n",
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
            "\n",
            "### Concept:\n",
            "Semiotics\n",
            "\n",
            "### Response:\n",
            "Please provide a detailed explanation of this concept along with an example scenario that illustrates it.<|end_of_text|><|begin_of_text|>Below is an instruction that describes the concept of 'Semiotics' in the context of qualitative market research, ethnography, and social anthropology. Semiotics is the study of signs and symbols, their meanings, and how they are used to convey meaning in various cultures and societies. It examines how people create and negotiate meaning through language, images, objects, and rituals, and how these meanings are embedded in the cultural and social contexts. In the field of qualitative market research, semiotics is crucial for understanding consumer behavior, as it reveals the underlying values, attitudes, and motivations that drive purchasing decisions. In ethnography, semiotics is essential for gaining insight into the cultural codes, customs, and practices of a particular group or community. Furthermore, semiotics is also important in social anthropology, where it helps researchers understand the complex relationships between culture, power dynamics, and social norms. There are several types of semiotics, including: (1) Symbolic interactionism, which posits that meaning is created through interactions between individuals; (2) Structural functionalism, which sees meaning as structured by social and cultural structures; and (3) Poststructuralism, which argues that meaning is fragmented and context-dependent. Each of these types has subtypes, such as: (a) Iconicity, which refers to the use of visual symbols; (b) Indexicality, which involves the use of symbols that point to something else; and (c) Symbolicity, which involves the use of symbols to represent abstract concepts.\n",
            "\n",
            "A market researcher uses semiotics to analyze consumer behavior towards energy-efficient products. Through participant observation and interviews, the researcher identifies a pattern where consumers associate energy efficiency with sustainability, leading them to prioritize eco-friendly features over price. However, upon further analysis, the researcher discovers that the perceived link between energy efficiency and sustainability is not always accurate, as many products claiming to be energy efficient may actually contain hidden energy costs. By examining the symbolic meanings associated with these products, the researcher can gain insights into the complex negotiations surrounding energy consumption and environmental responsibility. This example illustrates the application of semiotics in analyzing consumer behavior and highlights the importance of considering the symbolic meanings behind product branding and marketing strategies.<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# Inference code part\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Model initialization parameters\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Enable faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Get EOS token\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Simplified instruction prompt that only takes concept name as input\n",
        "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
        "\n",
        "### Concept:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "Please provide a detailed explanation of this concept along with an example scenario that illustrates it.\"\"\" + EOS_TOKEN  # Add EOS_TOKEN here\n",
        "\n",
        "# Set model dtype\n",
        "model.config.torch_dtype = torch.bfloat16\n",
        "\n",
        "# Example usage - just provide the concept name\n",
        "concept_name = \"Semiotics\"\n",
        "\n",
        "# Format input\n",
        "inputs = tokenizer(\n",
        "    [instruction_prompt.format(concept_name)],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Initialize text streamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Generate output with modified parameters\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    min_length=50,\n",
        "    # max_length=max_seq_length,\n",
        "    early_stopping=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: fineGrained).\n",
            "The token `krishx11` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `krishx11`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "510e673d7e4e41fb8089aa99fc0b0742",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/579 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2d449b51bc04336b877c8bf99f35211",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/1.41G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to https://huggingface.co/olabs-ai/qLeap_v06_instruct\n",
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 1683.14 out of 2015.48 RAM for saving.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 50.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
            "Unsloth: [1] Converting model at olabs-ai/qLeap_v06_instruct into bf16 GGUF format.\n",
            "The output location will be /root/quantumLeap/olabs-ai/qLeap_v06_instruct/unsloth.BF16.gguf\n",
            "This will take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: qLeap_v06_instruct\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {2048, 128256}\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {2048, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128001\n",
            "INFO:gguf.vocab:Setting special token type pad to 128004\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/root/quantumLeap/olabs-ai/qLeap_v06_instruct/unsloth.BF16.gguf: n_tensors = 148, total_size = 3.0G\n",
            "Writing: 100%|██████████| 3.00G/3.00G [00:12<00:00, 236Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /root/quantumLeap/olabs-ai/qLeap_v06_instruct/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: /root/quantumLeap/olabs-ai/qLeap_v06_instruct/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
            "main: build = 3982 (cc2983d3)\n",
            "main: built with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\n",
            "main: quantizing '/root/quantumLeap/olabs-ai/qLeap_v06_instruct/unsloth.BF16.gguf' to '/root/quantumLeap/olabs-ai/qLeap_v06_instruct/unsloth.Q4_K_M.gguf' as Q4_K_M using 384 threads\n",
            "llama_model_loader: loaded meta data with 29 key-value pairs and 148 tensors from /root/quantumLeap/olabs-ai/qLeap_v06_instruct/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 16\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   34 tensors\n",
            "llama_model_loader: - type bf16:  114 tensors\n",
            "[   1/ 148]                    rope_freqs.weight - [   32,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   2/ 148]                        output.weight - [ 2048, 128256,     1,     1], type =   bf16, converting to q6_K .. size =   501.00 MiB ->   205.49 MiB\n",
            "[   3/ 148]                    token_embd.weight - [ 2048, 128256,     1,     1], type =   bf16, converting to q4_K .. size =   501.00 MiB ->   140.91 MiB\n",
            "[   4/ 148]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   5/ 148]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[   6/ 148]                blk.0.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   7/ 148]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   8/ 148]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   9/ 148]                  blk.0.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  10/ 148]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  11/ 148]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  12/ 148]                  blk.0.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  13/ 148]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  14/ 148]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  15/ 148]                blk.1.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  16/ 148]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  17/ 148]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  18/ 148]                  blk.1.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  19/ 148]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  20/ 148]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  21/ 148]                  blk.1.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  22/ 148]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  23/ 148]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  24/ 148]               blk.10.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  25/ 148]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  26/ 148]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  27/ 148]                 blk.10.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  28/ 148]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  29/ 148]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  30/ 148]                 blk.10.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  31/ 148]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  32/ 148]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  33/ 148]               blk.11.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  34/ 148]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  35/ 148]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  36/ 148]                 blk.11.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  37/ 148]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  38/ 148]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  39/ 148]                 blk.11.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  40/ 148]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  41/ 148]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  42/ 148]               blk.12.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  43/ 148]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  44/ 148]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  45/ 148]                 blk.12.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  46/ 148]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  47/ 148]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  48/ 148]                 blk.12.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  49/ 148]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  50/ 148]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  51/ 148]               blk.13.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  52/ 148]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  53/ 148]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  54/ 148]                 blk.13.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  55/ 148]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  56/ 148]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  57/ 148]                 blk.13.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  58/ 148]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  59/ 148]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  60/ 148]               blk.14.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  61/ 148]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  62/ 148]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  63/ 148]                 blk.14.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  64/ 148]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  65/ 148]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  66/ 148]                 blk.14.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  67/ 148]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 148]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  69/ 148]               blk.15.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  70/ 148]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  71/ 148]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  72/ 148]                 blk.15.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  73/ 148]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  74/ 148]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  75/ 148]                 blk.15.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  76/ 148]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  77/ 148]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  78/ 148]                blk.2.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  79/ 148]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  80/ 148]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  81/ 148]                  blk.2.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  82/ 148]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  83/ 148]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  84/ 148]                  blk.2.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  85/ 148]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  86/ 148]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  87/ 148]                blk.3.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  88/ 148]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  89/ 148]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  90/ 148]                  blk.3.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  91/ 148]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  92/ 148]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  93/ 148]                  blk.3.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  94/ 148]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  95/ 148]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  96/ 148]                blk.4.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  97/ 148]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  98/ 148]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  99/ 148]                  blk.4.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 100/ 148]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 101/ 148]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 102/ 148]                  blk.4.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 103/ 148]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 104/ 148]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 105/ 148]                blk.5.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 106/ 148]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 107/ 148]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 108/ 148]                  blk.5.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 109/ 148]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 110/ 148]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 111/ 148]                  blk.5.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 112/ 148]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 113/ 148]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 114/ 148]                blk.6.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 115/ 148]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 116/ 148]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 117/ 148]                  blk.6.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 118/ 148]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 119/ 148]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 120/ 148]                  blk.6.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 121/ 148]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 122/ 148]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 123/ 148]                blk.7.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 124/ 148]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 125/ 148]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 126/ 148]                  blk.7.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 127/ 148]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 128/ 148]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 129/ 148]                  blk.7.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 130/ 148]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 131/ 148]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 132/ 148]                blk.8.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 133/ 148]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 134/ 148]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 135/ 148]                  blk.8.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 136/ 148]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 137/ 148]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 138/ 148]                  blk.8.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 139/ 148]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 140/ 148]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 141/ 148]                blk.9.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 142/ 148]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 143/ 148]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 144/ 148]                  blk.9.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 145/ 148]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 146/ 148]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 147/ 148]                  blk.9.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 148/ 148]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "llama_model_quantize_internal: model size  =  2858.26 MB\n",
            "llama_model_quantize_internal: quant size  =   903.71 MB\n",
            "\n",
            "main: quantize time = 24901.16 ms\n",
            "main:    total time = 24901.16 ms\n",
            "Unsloth: Conversion completed! Output location: /root/quantumLeap/olabs-ai/qLeap_v06_instruct/unsloth.Q4_K_M.gguf\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7638966c3d724ce393fbdda9f0e118fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.Q4_K_M.gguf:   0%|          | 0.00/955M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/olabs-ai/qLeap_v06_instruct\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model_pum_instruct\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model_pum_instruct\")\n",
        "\n",
        "!huggingface-cli login --token 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 --add-to-git-credential\n",
        "if True:\n",
        "    model.push_to_hub(\"olabs-ai/qLeap_v06_instruct\", token = \"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\") # Online saving\n",
        "    tokenizer.push_to_hub(\"olabs-ai/qLeap_v06_instruct\", token = \"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\") # Online saving\n",
        "    model.push_to_hub_gguf(\"olabs-ai/qLeap_v06_instruct\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if True: model.push_to_hub_gguf(\"olabs-ai/qLeap_v02\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q5_k_m\", token = \"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
