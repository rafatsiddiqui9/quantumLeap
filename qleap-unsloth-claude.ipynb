{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 1.2: Import Libraries\n",
    "# ----------------------------- #\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import xformers\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "import ipywidgets\n",
    "import unsloth\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is available\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print('punkt was already available.')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    print('punkt was not available. It has been downloaded')\n",
    "\n",
    "# Initialize spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print('en_core_web_sm was already available.')\n",
    "except OSError:\n",
    "    print(\"SpaCy English model not found. Downloading...\")\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 2: Load and Clean the Text Data\n",
    "# ----------------------------- #\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    # # Remove Project Gutenberg's license text and headers/footers\n",
    "    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
    "    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
    "\n",
    "    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n",
    "    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n",
    "    return text.strip()\n",
    "\n",
    "# Replace 'psychology_of_unconscious.txt' with your actual file path\n",
    "file_path = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/psychology_of_unconscious.txt'\n",
    "clean_text = load_and_clean_text(file_path)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 3: Parse Text into Discourse Units\n",
    "# ----------------------------- #\n",
    "\n",
    "# def parse_discourse_units(text):\n",
    "#     \"\"\"\n",
    "#     Parses text into discourse units using spaCy.\n",
    "#     Currently splits text into sentences.\n",
    "#     \"\"\"\n",
    "#     paragraphs = text.split('\\n\\n')\n",
    "#     paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
    "    \n",
    "#     discourse_units = []\n",
    "#     for para in paragraphs:\n",
    "#         doc = nlp(para)\n",
    "#         sentences = [sent.text for sent in doc.sents]\n",
    "#         discourse_units.extend(sentences)\n",
    "#     return discourse_units\n",
    "\n",
    "# discourse_units = parse_discourse_units(clean_text)\n",
    "\n",
    "# # Save discourse_units to a JSON file\n",
    "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "# Load discourse_units from the JSON file\n",
    "with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.json', 'r', encoding='utf-8') as f:\n",
    "    discourse_units = json.load(f)\n",
    "\n",
    "len(discourse_units)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 4: Create Chunks Using Hybrid Strategy\n",
    "# ----------------------------- #\n",
    "\n",
    "def create_chunks(discourse_units, tokenizer, max_length=512, overlap_size=0):\n",
    "    \"\"\"\n",
    "    Creates chunks from discourse units using a sliding window with overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for unit in discourse_units:\n",
    "        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n",
    "        unit_length = len(unit_tokens)\n",
    "\n",
    "        if current_length + unit_length <= max_length:\n",
    "            current_chunk.append(unit)\n",
    "            current_length += unit_length\n",
    "        else:\n",
    "            # Append the current chunk\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            # Create overlap\n",
    "            overlap_text = ' '.join(current_chunk)[-overlap_size:]\n",
    "            overlap_tokens = tokenizer.encode(overlap_text, add_special_tokens=False)\n",
    "            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n",
    "            # Start new chunk with overlap and current unit\n",
    "            current_chunk = [overlap_text, unit]\n",
    "            current_length = len(tokenizer.encode(overlap_text, add_special_tokens=False)) + unit_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "base_model_slug = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model_slug, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 5: : Load the Tokenizer and Model\n",
    "# ----------------------------- #\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 6: Create Chunks (After Tokenizer is Loaded)\n",
    "# ----------------------------- #\n",
    "\n",
    "chunks_max_length = max_seq_length\n",
    "overlap_size = 1\n",
    "chunks = create_chunks(discourse_units, tokenizer, max_length=chunks_max_length, overlap_size=overlap_size)\n",
    "\n",
    "# Save chunks to a JSON file (Optional)\n",
    "with open(f'/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final_{chunks_max_length}_{overlap_size}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# # If you need to reload from JSON (Optional)\n",
    "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.json', 'r', encoding='utf-8') as f:\n",
    "#     chunks = json.load(f)\n",
    "    \n",
    "print(len(chunks))\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 7: Create and Tokenize Dataset\n",
    "# ----------------------------- #\n",
    "\n",
    "# Create a Dataset object from chunks\n",
    "\n",
    "book_title = 'Psychology of the Unconscious by C. G. Jung'\n",
    "wikipedia_prompt = \"\"\"\n",
    "### Title: {}\n",
    "\n",
    "### Article: {}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    titles = book_title\n",
    "    texts  = examples[\"text\"]\n",
    "    outputs = []\n",
    "    for title, text in zip([book_title]*len(chunks), texts):\n",
    "        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n",
    "        outputs.append(text)\n",
    "    return { \"text\" : outputs, }\n",
    "pass\n",
    "\n",
    "# convert chunks variable to huggingface dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": chunks})\n",
    "\n",
    "# dataset = dataset.train_test_split(test_size = 0.1)[\"train\"]\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "len(dataset)\n",
    "\n",
    "# Find the maximum length of the text field in the entire dataset\n",
    "max_length = max(len(text) for text in dataset['text'])\n",
    "print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 8: Configure Training Arguments\n",
    "# ----------------------------- #\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import wandb\n",
    "\n",
    "# Define your parameters\n",
    "batchSize = 2\n",
    "ga = 8\n",
    "maxSteps = 120\n",
    "lRate = 5e-5\n",
    "embLRate = 1e-5\n",
    "optim = \"adamw_8bit\"\n",
    "lrSchedule = \"linear\"\n",
    "\n",
    "# Assume these variables are defined elsewhere in your code\n",
    "base_model_slug = \"your_base_model_slug\"  # Replace with your actual value\n",
    "max_seq_length = 512  # Replace with your actual value\n",
    "\n",
    "# Get the current date and time in Indian Standard Time (IST)\n",
    "ist = pytz.timezone('Asia/Kolkata')\n",
    "current_datetime = datetime.now(ist)\n",
    "\n",
    "# Format the datetime string\n",
    "# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n",
    "formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the run name with the current date and time\n",
    "run_name = f\"\"\"Unsloth-CPT-Base-{formatted_datetime}-{base_model_slug}-{max_seq_length}max_seq_length-{batchSize}batchSize-{ga}ga-{maxSteps}maxSteps-{lRate}lRate-{embLRate}embLRate-{optim}optim-{lrSchedule}lrSchedule\"\"\"\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "# It's recommended to set your W&B API key as an environment variable for security.\n",
    "# Example: export WANDB_API_KEY=\"your_api_key\"\n",
    "wandb.login(key=\"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\")  # Consider using environment variables for security\n",
    "wandb.init(project=\"Unsloth-CPT\", name=run_name)\n",
    "\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = batchSize,\n",
    "        gradient_accumulation_steps = ga,\n",
    "\n",
    "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
    "        max_steps = maxSteps,\n",
    "        warmup_steps = 10,\n",
    "        # warmup_ratio = 0.1,\n",
    "        # num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate =lRate,\n",
    "        embedding_learning_rate = embLRate,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = optim,\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = lrSchedule,\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        \n",
    "        \n",
    "        report_to=[\"tensorboard\", \"wandb\"],\n",
    "        logging_dir=f\"./trel-fft-logs/{run_name}\",\n",
    "\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 9: Define Compute Metrics Function\n",
    "# ----------------------------- #\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes perplexity based on model predictions and labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # Convert to torch tensors\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Ensure shapes match\n",
    "    if logits.shape[:2] != labels.shape:\n",
    "        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n",
    "    \n",
    "    # Shift logits and labels\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "    # Check label values\n",
    "    if shift_labels.max() >= model.config.vocab_size:\n",
    "        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n",
    "    \n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 10: Initialize the Trainer\n",
    "# ----------------------------- #\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    level=logging.INFO,  # Set to DEBUG for more detailed logs\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    "# ----------------------------- #\n",
    "# Part 11: Start Training\n",
    "# ----------------------------- #\n",
    "\n",
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "trainer_stats = trainer.train()\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "from datasets import Dataset\n",
    "from transformers import TextStreamer\n",
    "from unsloth import (\n",
    "    FastLanguageModel,\n",
    "    UnslothTrainer,\n",
    "    UnslothTrainingArguments,\n",
    "    is_bfloat16_supported\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='transformation_errors.log',\n",
    "    filemode='w',\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Define paths\n",
    "INPUT_CSV_PATH = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv'\n",
    "OUTPUT_JSON_PATH = '/root/qLeap-fft/data/input/Instruction_Data/transformed_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data(input_csv_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Read and validate the input CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(input_csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            return list(reader)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def transform_data(original_data: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Transform the original data by expanding example scenarios.\"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    for idx, entry in enumerate(original_data, start=1):\n",
    "        concept_name = entry.get('concept_name', '').strip()\n",
    "        detailed_explanation = entry.get('detailed_explanation', '').strip()\n",
    "        example_scenario_str = entry.get('example_scenario', '').strip()\n",
    "\n",
    "        if not all([concept_name, detailed_explanation, example_scenario_str]):\n",
    "            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            example_scenarios = json.loads(example_scenario_str)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                example_scenarios = ast.literal_eval(example_scenario_str)\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}') has invalid example_scenario: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not isinstance(example_scenarios, list):\n",
    "            logging.error(f\"Entry {idx} ('{concept_name}'): example_scenario is not a list\")\n",
    "            continue\n",
    "\n",
    "        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n",
    "            if not isinstance(scenario, str):\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}'): non-string scenario at position {scenario_idx}\")\n",
    "                continue\n",
    "\n",
    "            new_data.append({\n",
    "                'concept_name': concept_name,\n",
    "                'detailed_explanation': detailed_explanation,\n",
    "                'example_scenario': scenario.strip()\n",
    "            })\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# Process and save the data\n",
    "original_data = read_csv_data(INPUT_CSV_PATH)\n",
    "transformed_data = transform_data(original_data)\n",
    "\n",
    "# Save transformed data\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)\n",
    "with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Processed {len(transformed_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instruction template\n",
    "instruction_template = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\"\"\"\n",
    "\n",
    "def create_instruction_dataset(transformed_data: List[Dict[str, str]]) -> Dataset:\n",
    "    \"\"\"Create an instruction dataset from transformed data.\"\"\"\n",
    "    def instruction_prompt_func(examples):\n",
    "        return {\n",
    "            \"text\": [\n",
    "                instruction_template.format(\n",
    "                    f\"Explain the concept of {cn} and provide an example.\",\n",
    "                    f\"{de}\\n\\nExample:\\n{es}\"\n",
    "                )\n",
    "                for cn, de, es in zip(\n",
    "                    examples[\"concept_name\"],\n",
    "                    examples[\"detailed_explanation\"],\n",
    "                    examples[\"example_scenario\"]\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    dataset = Dataset.from_list(transformed_data)\n",
    "    return dataset.map(instruction_prompt_func, batched=True)\n",
    "\n",
    "# Create the dataset\n",
    "instruction_dataset = create_instruction_dataset(transformed_data)\n",
    "\n",
    "# Print a sample to verify\n",
    "print(\"\\nSample processed example:\")\n",
    "print(instruction_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization parameters\n",
    "max_seq_length = 1024\n",
    "model_name = \"lora_model_pum\"\n",
    "load_in_4bit = True\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")\n",
    "\n",
    "# Check for special tokens\n",
    "special_tokens = [\n",
    "    \"<|start_header_id|>\",\n",
    "    \"<|end_header_id|>\",\n",
    "    \"<|eot_id|>\",\n",
    "    \"system\",\n",
    "    \"user\",\n",
    "    \"assistant\"\n",
    "]\n",
    "\n",
    "for token in special_tokens:\n",
    "    if token not in tokenizer.get_vocab():\n",
    "        print(f\"Warning: {token} not in vocabulary!\")\n",
    "\n",
    "# Configure model\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.config.torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(model, tokenizer, dataset, \n",
    "                  batch_size=2, gradient_accumulation=8, max_steps=120):\n",
    "    \"\"\"Setup the training configuration.\"\"\"\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    import wandb\n",
    "\n",
    "    # Define your parameters\n",
    "    batchSize = 2\n",
    "    ga = 8\n",
    "    maxSteps = 120\n",
    "    lRate = 5e-5\n",
    "    embLRate = 1e-5\n",
    "    optim = \"adamw_8bit\"\n",
    "    lrSchedule = \"linear\"\n",
    "\n",
    "    # Assume these variables are defined elsewhere in your code\n",
    "    base_model_slug = \"your_base_model_slug\"  # Replace with your actual value\n",
    "    max_seq_length = 512  # Replace with your actual value\n",
    "\n",
    "    # Get the current date and time in Indian Standard Time (IST)\n",
    "    ist = pytz.timezone('Asia/Kolkata')\n",
    "    current_datetime = datetime.now(ist)\n",
    "\n",
    "    # Format the datetime string\n",
    "    # Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create the run name with the current date and time\n",
    "    run_name = f\"\"\"Unsloth-CPT-Instruct-{formatted_datetime}-{base_model_slug}-{max_seq_length}max_seq_length-{batchSize}batchSize-{ga}ga-{maxSteps}maxSteps-{lRate}lRate-{embLRate}embLRate-{optim}optim-{lrSchedule}lrSchedule\"\"\"\n",
    "\n",
    "    # Initialize Weights & Biases\n",
    "    # It's recommended to set your W&B API key as an environment variable for security.\n",
    "    # Example: export WANDB_API_KEY=\"your_api_key\"\n",
    "    wandb.login(key=\"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\")  # Consider using environment variables for security\n",
    "    wandb.init(project=\"Unsloth-CPT\", name=run_name)\n",
    "    \n",
    "    training_args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        max_steps=max_steps,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        embedding_learning_rate=1e-5,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=[\"tensorboard\", \"wandb\"],\n",
    "        logging_dir=\"./trel-fft-logs\"\n",
    "    )\n",
    "\n",
    "    return UnslothTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        args=training_args\n",
    "    )\n",
    "\n",
    "# Setup trainer\n",
    "trainer = setup_training(model, tokenizer, instruction_dataset)\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model_pum_instruct\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model_pum_instruct\")\n",
    "\n",
    "!huggingface-cli login --token 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 --add-to-git-credential\n",
    "if True:\n",
    "    model.push_to_hub(\"olabs-ai/qLeap_v07_instruct\", token = \"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\") # Online saving\n",
    "    tokenizer.push_to_hub(\"olabs-ai/qLeap_v07_instruct\", token = \"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\") # Online saving\n",
    "    model.push_to_hub_gguf(\"olabs-ai/qLeap_v07_instruct\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3 MIG 1g.10gb. Max memory: 9.75 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 9.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
      "Please update transformers, TRL and unsloth via:\n",
      "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n",
      "Unsloth 2024.10.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Explain the concept of Semiotics and provide an example.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Semiotics is the study of signs, symbols, and meaning-making processes within human culture. It examines how people create, negotiate, and interpret meaning through the use of objects, behaviors, language, and images. Semiotics draws on linguistics, anthropology, sociology, and cultural studies to analyze how meaning is constructed and conveyed through different forms of communication. There are several types of semiotics, including symbolic interactionism, which focuses on the ways in which individuals create and negotiate meaning through social interactions; structuralism, which views meaning as fixed and structured by external forces; and post-structuralism, which argues that meaning is fragmented and unstable. Within these types, there are subtypes such as iconology, which examines the relationship between symbol and object; indexicality, which posits that meaning is derived from the proximity of the symbol to its referent; and syntactics, which explores the relationships between words and meanings. Additionally, semiotics has nuances such as deconstruction, which seeks to uncover the underlying power dynamics and contradictions within cultural texts, and postmodernism, which challenges traditional notions of meaning and reality.\n",
      "\n",
      "Example:\n",
      "In a qualitative market research study on consumer behavior, researchers used ethnographic methods to observe and participate in daily activities of participants. They noted how individuals' actions and interactions with everyday objects like food and clothing reflected their attitudes towards luxury and exclusivity. For instance, a participant who wore expensive clothing might display it proudly while conversing with friends, showcasing her status and wealth. This researcher found that the way people dressed and carried themselves could convey subtle messages about their identity and social standing. By analyzing these subtle cues, the researcher was able to identify patterns and trends in consumer behavior that could inform marketers and fashion designers.<|reserved_special_token_6|>\n",
      "<|reserved_special_token_125|>user<|reserved_special_token_221|>\n",
      "\n",
      "Youngeurer<|reserved_special_token_81|>\n",
      "<|reserved_special_token_56|>assistant<|reserved_special_token_194|>\n",
      "\n",
      "Explain the concept of Cultural Probes and provide an example.<|reserved_special_token_143|>\n",
      "<|reserved_special_token_211|>assistant<|reserved_special_token_60|>\n",
      "\n",
      "Cultural probes are research tools that allow researchers to gather data on cultural practices, values, and norms from participants in real-world settings. Developed by sociologist Clifford Geertz, cultural probes involve the placement of 'traps,' or devices that mimic daily life, such as diaries, photographs, or interviews, to capture participants' behaviors, thoughts, and emotions in their natural context. The goal is to get a more nuanced understanding of the cultural context than can be achieved through surveys or focus groups. Cultural probes have various types, including 'ethnographic probes,' which involve long-term observations and participation in cultures, and 'prospective probes,'\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "\n",
      "user\n",
      "\n",
      "Explain the concept of Semiotics and provide an example.\n",
      "assistant\n",
      "\n",
      "Semiotics is the study of signs, symbols, and meaning-making processes within human culture. It examines how people create, negotiate, and interpret meaning through the use of objects, behaviors, language, and images. Semiotics draws on linguistics, anthropology, sociology, and cultural studies to analyze how meaning is constructed and conveyed through different forms of communication. There are several types of semiotics, including symbolic interactionism, which focuses on the ways in which individuals create and negotiate meaning through social interactions; structuralism, which views meaning as fixed and structured by external forces; and post-structuralism, which argues that meaning is fragmented and unstable. Within these types, there are subtypes such as iconology, which examines the relationship between symbol and object; indexicality, which posits that meaning is derived from the proximity of the symbol to its referent; and syntactics, which explores the relationships between words and meanings. Additionally, semiotics has nuances such as deconstruction, which seeks to uncover the underlying power dynamics and contradictions within cultural texts, and postmodernism, which challenges traditional notions of meaning and reality.\n",
      "\n",
      "Example:\n",
      "In a qualitative market research study on consumer behavior, researchers used ethnographic methods to observe and participate in daily activities of participants. They noted how individuals' actions and interactions with everyday objects like food and clothing reflected their attitudes towards luxury and exclusivity. For instance, a participant who wore expensive clothing might display it proudly while conversing with friends, showcasing her status and wealth. This researcher found that the way people dressed and carried themselves could convey subtle messages about their identity and social standing. By analyzing these subtle cues, the researcher was able to identify patterns and trends in consumer behavior that could inform marketers and fashion designers.\n",
      "user\n",
      "\n",
      "Youngeurer\n",
      "assistant\n",
      "\n",
      "Explain the concept of Cultural Probes and provide an example.\n",
      "assistant\n",
      "\n",
      "Cultural probes are research tools that allow researchers to gather data on cultural practices, values, and norms from participants in real-world settings. Developed by sociologist Clifford Geertz, cultural probes involve the placement of 'traps,' or devices that mimic daily life, such as diaries, photographs, or interviews, to capture participants' behaviors, thoughts, and emotions in their natural context. The goal is to get a more nuanced understanding of the cultural context than can be achieved through surveys or focus groups. Cultural probes have various types, including 'ethnographic probes,' which involve long-term observations and participation in cultures, and 'prospective probes,'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model initialization parameters\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Enable faster inference\n",
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model_pum_instruct\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Instruction prompt matching the fine-tuning template\n",
    "instruction_prompt = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Explain the concept of {} and provide an example.<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Set model dtype\n",
    "model.config.torch_dtype = torch.bfloat16\n",
    "\n",
    "# Example usage\n",
    "concept_name = \"Semiotics\"\n",
    "\n",
    "# Format input\n",
    "inputs = tokenizer(\n",
    "    [instruction_prompt.format(concept_name)],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate output with modified parameters\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.get_vocab().get(\"<|eot_id|>\", tokenizer.eos_token_id),  # Use <|eot_id|> if available\n",
    "    min_length=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Optional: Print the full response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
