{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9698777,"sourceType":"datasetVersion","datasetId":5930536}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Git clone qLeap-fft repo inside `/kaggle/working/` directory\n## Ensure to have the latest branch\n## Switch to quantumLeap directory","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\ndef ensure_working_directory():\n    \"\"\"\n    Check if we're in the correct working directory, if not switch to it.\n    Creates the directory if it doesn't exist.\n    \"\"\"\n    target_dir = '/kaggle/working/quantumLeap'\n    current_dir = os.getcwd()\n    \n    # Print current directory\n    print(f\"Current directory: {current_dir}\")\n    \n    # Check if we need to switch directories\n    if current_dir != target_dir:\n        # Create directory if it doesn't exist\n        Path(target_dir).mkdir(parents=True, exist_ok=True)\n        \n        try:\n            # Change to target directory\n            os.chdir(target_dir)\n            print(f\"Successfully switched to: {target_dir}\")\n        except Exception as e:\n            print(f\"Error switching to directory: {str(e)}\")\n            raise\n    else:\n        print(\"Already in correct directory\")\n    \n    # Verify current directory\n    print(f\"Working directory: {os.getcwd()}\")\n\n# Call the function before your main code\nensure_working_directory()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ----------------------------- #\n# # Part 1.1: Install and Setup Libraries\n# # ----------------------------- #\n\n# # !python -m xformers.info\n# import torch\n# print(f\"PyTorch version: {torch.__version__}\")\n# print(f\"CUDA available: {torch.cuda.is_available()}\")\n# print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n# %pip install uv\n# !uv pip install -q unsloth wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets --system\n# # Finally, install a compatible xformers version\n# !uv pip install -q xformers==0.0.27 --system  # This version is compatible with PyTorch 2.4.x\n# !python -m xformers.info# # !python -m xformers.info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Restart the session and check if unsloth is installed","metadata":{}},{"cell_type":"code","source":"# import unsloth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install Flash Attention and check version","metadata":{}},{"cell_type":"code","source":"# !uv pip install flash_attn --no-build-isolation -q --system\n# !pip show flash_attn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Restart again so that all the libraries are properly initialized","metadata":{}},{"cell_type":"code","source":"# ----------------------------- #\n# Part 1.2: Import Necessary Libraries\n# ----------------------------- #\n\n# General Libraries\nimport os\nimport json\nimport sys\nimport subprocess\nimport argparse\nimport logging\nimport math\nimport random\nfrom datetime import datetime\nimport re\nimport gc\nimport weakref\nimport multiprocessing\n\n# Torch related\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\n\n# Transformers related\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    Adafactor\n)\n\n# Huggingface TRL for full finetune\nfrom trl import SFTTrainer, SFTConfig\n\n# General huggingface libraries\nimport huggingface_hub\nfrom datasets import load_dataset, Dataset\nfrom accelerate import Accelerator\n\n\n# Unsloth specificic libraries\nimport unsloth\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n\n# Other Libraries\nfrom peft import LoraConfig\nimport wandb\nimport nltk\nimport spacy\n# from galore_torch import GaLoreAdamW, GaLoreAdafactor, GaLoreAdamW8bit\n\n# Check and import NLTK and spacy modules\n# Ensure NLTK's punkt tokenizer is available\nimport nltk\ntry:\n    nltk.data.find('tokenizers/punkt')\n    print('punkt was already available.')\nexcept LookupError:\n    nltk.download('punkt')\n    print('punkt was not available. It has been downloaded')\n\n# Initialize spaCy English model\ntry:\n    nlp = spacy.load('en_core_web_sm')\n    print('en_core_web_sm was already available.')\nexcept OSError:\n    print(\"SpaCy English model not found. Downloading...\")\n    os.system('python -m spacy download en_core_web_sm')\n    nlp = spacy.load('en_core_web_sm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Configure Environment Variables & Create Main Variables\n# ----------------------------- #\n\n# max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n# base_model_slug = \"meta-llama/Llama-3.2-1B-Instruct\"\n# base_model_name = \"lora_model_pum\"\n# chunks_max_length = max_seq_length\n# overlap_size = 1\n# # Define your parameters\n# batchSize = 2\n# ga = 8\n# maxSteps = 120\n# lRate = 5e-5\n# embLRate = 1e-5\n# optim = \"adamw_8bit\"\n# lrSchedule = \"linear\"\n# dataset_slug = \"psychology_of_unconscious\"\n\n# # Set environment variables before importing torch-related modules\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Load and Clean the Text Data\n# ----------------------------- #\n\ndef load_and_clean_text(file_path):\n    \"\"\"\n    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        text = f.read()\n    # # Remove Project Gutenberg's license text and headers/footers\n    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n\n    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n    return text.strip()\n\n# Replace 'psychology_of_unconscious.txt' with your actual file path\nfile_path = '/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'\nclean_text = load_and_clean_text(file_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ----------------------------- #\n# # Part 3: Parse Text into Discourse Units\n# # ----------------------------- #\n\ndef parse_discourse_units(text, overwrite=False):\n    \"\"\"\n    Parses text into discourse units using spaCy.\n    Currently splits text into sentences.\n    \"\"\"\n    paragraphs = text.split('\\n\\n')\n    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n\n    discourse_units = []\n    for para in paragraphs:\n        doc = nlp(para)\n        sentences = [sent.text for sent in doc.sents]\n        discourse_units.extend(sentences)\n\n    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_discourse_units.json'\n    if not os.path.exists(output_path) or overwrite:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        with open(output_path, 'w') as f:\n            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n    else:\n        with open(output_path, 'r') as f:\n            discourse_units = json.load(f)\n\n    print(f\"Total Discourse Units: {len(discourse_units)}\")\n    return discourse_units","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 4: Create Chunks Using Hybrid Strategy\n# ----------------------------- #\n\ndef create_chunks(discourse_units, tokenizer, max_length=4096, overlap_size=1, overwrite=False):\n    \"\"\"\n    Creates chunks from discourse units using a sliding window with overlapping chunks.\n    Optimized to work directly with token IDs and utilize efficient list operations.\n    \"\"\"\n    chunks = []\n    current_chunk_tokens = []\n    current_length = 0\n\n    for unit in discourse_units:\n        unit_tokens = tokenizer.encode(unit, add_special_tokens=True)\n        unit_length = len(unit_tokens)\n\n        if current_length + unit_length <= max_length:\n            current_chunk_tokens.extend(unit_tokens)\n            current_length += unit_length\n        else:\n            # Decode and append the current chunk\n            chunk_text = tokenizer.decode(\n                current_chunk_tokens, skip_special_tokens=True)\n            chunks.append(chunk_text)\n\n            # Prepare overlap tokens\n            overlap_tokens = current_chunk_tokens[-overlap_size:]\n            current_chunk_tokens = overlap_tokens + unit_tokens\n            current_length = len(current_chunk_tokens)\n\n    # Append any remaining tokens as the last chunk\n    if current_chunk_tokens:\n        chunk_text = tokenizer.decode(\n            current_chunk_tokens, skip_special_tokens=True)\n        chunks.append(chunk_text)\n\n    # Write or read chunks as before\n    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_chunks.json'\n    if not os.path.exists(output_path) or overwrite:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        with open(output_path, 'w') as f:\n            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n    else:\n        with open(output_path, 'r') as f:\n            discourse_units = json.load(f)\n\n    print(f\"Total Chunks Created: {len(chunks)}\")\n    return chunks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Create and Tokenize Dataset\n# ----------------------------- #\n\n# To Do - make book titles and prompt generic so\ndef create_tokenized_dataset(chunks, tokenizer, max_length=1024, num_proc=2):\n\n    # Create a Dataset object from chunks\n\n    book_title = 'Psychology of the Unconscious by C. G. Jung'\n    wikipedia_prompt = \"\"\"\n    Psychology Book\n\n    ### Title: {}\n\n    ### Article: {}\n    \"\"\"\n\n    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\n    def formatting_prompts_func(examples):\n        titles = book_title\n        texts = examples[\"text\"]\n        outputs = []\n        for title, text in zip([book_title]*len(chunks), texts):\n            text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n            outputs.append(text)\n        return {\"text\": outputs, }\n    pass\n\n    # convert chunks variable to huggingface dataset\n\n    from datasets import Dataset\n\n    dataset = Dataset.from_dict({\"text\": chunks})\n\n    dataset = dataset.map(formatting_prompts_func,\n                          batched=True, num_proc=num_proc)\n    # Split the dataset into training and validation sets\n    split = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = split['train']\n    eval_dataset = split['test']\n\n    print(len(dataset))\n    # Find the maximum length of the text field in the entire dataset\n    max_length = max(len(text) for text in dataset['text'])\n    print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n    print(f\"Training Dataset Size: {len(train_dataset)}\")\n#     print(f\"First 5 rows of training dataset: {train_dataset[:5]}\")\n    print(f\"Validation Dataset Size: {len(eval_dataset)}\")\n#     print(f\"First 5 rows of validation dataset: {eval_dataset[:5]}\")\n    return train_dataset, eval_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 6: Set up environment and other important utilities\n# ----------------------------- #\n\ndef setup_environment():\n    \"\"\"\n    Initializes the Accelerator for distributed training.\n    \"\"\"\n    return Accelerator()\n\n\ndef get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n    \"\"\"\n    Defines a custom learning rate scheduler with warmup, constant, and linear annealing phases.\n    \"\"\"\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return current_step / num_warmup_steps  # Linear warmup\n        elif current_step < initial_phase_steps:\n            return 1.0  # Constant learning rate for initial phase\n        else:\n            # Linear annealing for the remaining steps\n            return 1.0 - ((current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps))\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\ndef setup_training_steps(initial_rows, annealing_rows, batch_size, gradient_accumulation_steps, num_epochs):\n    \"\"\"\n    Calculates total and initial training steps based on dataset size and training parameters.\n    \"\"\"\n    total_rows = initial_rows + annealing_rows\n    total_steps = (total_rows * num_epochs) // (batch_size *\n                                                gradient_accumulation_steps)\n    initial_steps = (initial_rows * num_epochs) // (batch_size *\n                                                    gradient_accumulation_steps)\n    return max(1, total_steps), max(1, initial_steps)\n\n\ndef print_memory_usage(step_desc):\n    \"\"\"\n    Prints the CUDA memory summary if CUDA is available.\n    \"\"\"\n    if torch.cuda.is_available():\n        print(f\"Memory Usage at {step_desc}:\")\n        print(torch.cuda.memory_summary())\n        print(\"\\n\")\n    else:\n        print(f\"No CUDA available at {step_desc}.\\n\")\n\n\ndef inference(model, tokenizer):\n    \"\"\"\n    Runs inference using the trained model.\n    \"\"\"\n    # Define sample prompts\n    prompts = [\n        \"Explain what is medical anthropology and its importance in elevating the quality of life?\",\n        \"Explain what are the types of Jungian archetypes and how they manifest in the human psyche?\"\n    ]\n\n    for prompt in prompts:\n        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_length=256)\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n        \n#  Login to Huggingface\nfrom huggingface_hub import login\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef setup_huggingface_access():\n    \"\"\"Setup Hugging Face access with token from environment or manual input\"\"\"\n    # First try to get token from environment variable\n    token = os.getenv('HUGGINGFACE_TOKEN')\n    \n    if not token:\n        # If not in environment, prompt for token\n        token = input(\"Enter your Hugging Face token: \")\n        \n    if token:\n        try:\n            login(token, add_to_git_credential=True)\n            print(\"Successfully logged in to Hugging Face!\")\n        except Exception as e:\n            print(f\"Failed to log in to Hugging Face: {str(e)}\")\n            return False\n    else:\n        print(\"No Hugging Face token provided\")\n        return False\n    \n    return True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Load the Tokenizer and Model\n# ----------------------------- #\n\ndef load_model_and_tokenizer(base_model_slug, max_seq_length=4096, dtype=None, load_in_4bit=True, device_map = \"auto\"):\n    \"\"\"\n    Load and configure the model and tokenizer with specified parameters.\n    \n    Args:\n        base_model_slug (str): The model identifier from HuggingFace\n        max_seq_length (int): Maximum sequence length for the model\n        dtype: Data type for model parameters\n        load_in_4bit (bool): Whether to use 4-bit quantization\n        \n    Returns:\n        tuple: (model, tokenizer)\n    \"\"\"\n    # Check CUDA is available\n    import torch\n    if not torch.cuda.is_available():\n        print(\"WARNING: CUDA is not available. This might affect performance.\")\n    else:\n        print(\"CUDA available\")\n        \n    # Check available GPUs\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of available GPUs: {num_gpus}\")\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_properties(i).name}\")\n\n    # Determine optimal device map based on available GPUs\n    device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using single device: {device_map}\")\n\n    # Model paths\n    model_name = base_model_slug\n    models_dir = os.path.join(os.path.dirname(\"~/\"), \"models\")\n    model_path = os.path.join(models_dir, model_name)\n\n    # Create models directory if it doesn't exist\n    if not os.path.exists(models_dir):\n        os.makedirs(models_dir)\n\n    # Load or download model\n    try:\n        if os.path.exists(model_path):\n            print(f\"Loading model from local path: {model_path}\")\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name=base_model_slug,\n                max_seq_length=max_seq_length,\n                dtype=dtype,\n                load_in_4bit=load_in_4bit,\n                token=os.getenv('HUGGINGFACE_TOKEN'),\n            )\n        else:\n            print(f\"Downloading model from HuggingFace: {model_name}\")\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name=base_model_slug,\n                max_seq_length=max_seq_length,\n                dtype=dtype,\n                load_in_4bit=load_in_4bit,\n                token=os.getenv('HUGGINGFACE_TOKEN'),\n            )\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        raise\n\n    # Configure PEFT model\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=128,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n        target_modules=[\n            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n            \"gate_proj\", \"up_proj\", \"down_proj\",\n            \"embed_tokens\", \"lm_head\",  # Add for continual pretraining\n        ],\n        lora_alpha=32,\n        lora_dropout=0,  # Supports any, but = 0 is optimized\n        bias=\"none\",    # Supports any, but = \"none\" is optimized\n        use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n        random_state=3407,\n        use_rslora=True,   # We support rank stabilized LoRA\n        loftq_config=None,  # And LoftQ\n    )\n\n    print(\"Model and tokenizer loaded successfully!\")\n    return model, tokenizer\n\n    # Print model device distribution\n    if hasattr(model, 'hf_device_map'):\n        print(\"\\nModel Device Distribution:\")\n        for name, device in model.hf_device_map.items():\n            print(f\"{name}: {device}\")\n\n    # Print memory usage per GPU\n    print(\"\\nGPU Memory Usage after model loading:\")\n    for i in range(num_gpus):\n        memory_used = torch.cuda.memory_allocated(i) / (1024**3)\n        memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n        print(f\"GPU {i}: {memory_used:.2f}GB / {memory_total:.2f}GB\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Set the environment variable\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\nos.environ[\"WANDB_API_KEY\"] = \"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\"\n# Verify it's set correctly\nprint(os.getenv(\"HUGGINGFACE_TOKEN\"))\nprint(os.getenv(\"WANDB_API_KEY\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Configure Environment Variables & Create Main Variables\n# ----------------------------- #\n\n# Unsloth modell initialization variables\nmax_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\nmax_length = max_seq_length\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nbase_model_slug = \"meta-llama/Llama-3.2-1B-Instruct\"\nbase_model_name = \"lora_model_pum\"\nchunks_max_length = max_seq_length\noverlap_size = 1\n# Define your parameters\nbatchSize = 2\nga = 16\nmaxSteps = 120\nwarmupSteps = 10\nnumTrainEpochs = 1\nlRate = 5e-5\nembLRate = 1e-5\noptim = \"adamw_8bit\"\nlrSchedule = \"linear\"\ndataset_slug = \"psychology_of_unconscious\"\n\nfrom datetime import datetime\nimport pytz\nimport wandb\n# Get the current date and time in Indian Standard Time (IST)\nist = pytz.timezone('Asia/Kolkata')\ncurrent_datetime = datetime.now(ist)\n\n# Format the datetime string\n# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\nformatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n\n# Define Run Name\nrun_name = f\"\"\"Kaggle-quantumLeap-{formatted_datetime}-{base_model_slug}-{dataset_slug}-{max_seq_length}_maxSeqLength-{max_length}_maxLength-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{warmupSteps}_warmupSteps-{numTrainEpochs}_numTrainEpochs-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n\n# Initialize Weights & Biases\n# It's recommended to set your W&B API key as an environment variable for security.\nwandb.login(key=os.getenv(\"WANDB_API_KEY\"))\nwandb.init(project=\"Trellis-FFT-v2\", name=run_name)\n\n# Set environment variables before importing torch-related modules\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# ----------------------------- #\n# Part 9: Data Processing\n# ----------------------------- #\n\n# # Perform Inference Before Training\n# inference(model, tokenizer)\n\n# Set number of processes to use for data loading\nnum_cpus = multiprocessing.cpu_count()\nnum_proc = max(num_cpus-2, 2)  # Adjust based on prior recommendations\nprint(f\"Number of CPU cores: {num_cpus}\")\nprint(f\"Number of processes: {num_proc}\")\n\n# Login to Hugging Face\nif not setup_huggingface_access():\n    raise Exception(\"Failed to setup Hugging Face access. Please check your token.\")\n\n# Load Model and Tokenizer\nmodel, tokenizer = load_model_and_tokenizer(base_model_slug)\nprint(f\"Model Device: {model.device}\")\n\n# Load and Clean Text Data\nfile_path = \"/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\nclean_text = load_and_clean_text(file_path)\n\n# Parse Discourse Units\ndiscourse_units = parse_discourse_units(clean_text, overwrite=True)\n\n# Create Chunks\nchunks = create_chunks(\n    discourse_units,\n    tokenizer,\n    max_length=max_length,\n    overlap_size=overlap_size,\n    overwrite=True,\n)\n\n# Create Tokenized Dataset\ntrain_dataset, eval_dataset = create_tokenized_dataset(\n    chunks, tokenizer, max_length)\n\n# Save datasets as Hugging Face `datasets`\ntrain_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\neval_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')\n\n### To Do - Make the below as dynamic and as a functio\n# # Uncomment following if you want to just load the data from temp directory\n# from datasets import load_from_disk\n\n# train_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n# eval_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 8: Configure Training Arguments\n# ----------------------------- #\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = batchSize,\n        gradient_accumulation_steps = ga,\n\n        # Use warmup_ratio and num_train_epochs for longer runs!\n        max_steps = maxSteps,\n        warmup_steps = warmupSteps,\n        # warmup_ratio = 0.1,\n        num_train_epochs = numTrainEpochs,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = lRate,\n        embedding_learning_rate = embLRate,\n\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = optim,\n        weight_decay = 0.01,\n        lr_scheduler_type = lrSchedule,\n        seed = 3407,\n        output_dir = \"outputs\",\n\n        report_to=[\"tensorboard\", \"wandb\"],\n        logging_dir=f\"./trel-fft-logs/{run_name}\",\n    ),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 11: Start Training\n# ----------------------------- #\n\n#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\ntrainer_stats = trainer.train()\n\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import wandb\n# from pprint import pprint\n\n# def get_run_config(project_name, run_id):\n#     try:\n#         # Initialize the wandb API\n#         api = wandb.Api()\n\n#         # Access the specific run\n#         run = api.run(f\"{project_name}/{run_id}\")\n\n#         # Get the full configuration\n#         config = run.config\n\n#         # Filter for trainer-specific configuration\n#         trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n\n#         return trainer_config\n\n#     except wandb.errors.CommError:\n#         print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n#         return None\n#     except Exception as e:\n#         print(f\"An error occurred: {str(e)}\")\n#         return None\n\n# # Usage\n# project_name = \"olabs-asia-olabs-pro/huggingface\"\n# run_id = \"ppqtwwmy\"\n\n# trainer_config = get_run_config(project_name, run_id)\n\n# if trainer_config:\n#     print(f\"Trainer configuration for run {run_id}:\")\n#     pprint(trainer_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Loss from earlier training was too high. We shall use training arguments from unsloth colab notebook \"Llama-3.1 8b + Unsloth 2x faster finetuning.ipynb\". URL below\n### https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=95_Nn-89DhsL","metadata":{}},{"cell_type":"code","source":"print(dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import IntervalStrategy\nimport wandb\n\n# Initialize wandb\nwandb.init(project=\"huggingface\", name=\"refined_training_run\")\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = dataset.select(range(len(dataset) // 10)),  # Use 10% of data for evaluation\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Set both max_steps and num_train_epochs\n        max_steps = 120,\n        num_train_epochs = 3,\n\n        # Use a single learning rate for all parameters\n        learning_rate = 5e-5,\n\n        # Warmup strategy from successful runs\n        warmup_steps = 10,\n        warmup_ratio = 0,\n\n        # Explicitly set precision based on hardware support\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        \n        logging_steps = 1,\n        \n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        \n        seed = 3407,\n        output_dir = \"outputs\",\n        \n        report_to = \"wandb\",  # Enable Weights & Biases logging\n        \n        # Set both save and evaluation strategies to 'steps'\n        save_strategy = IntervalStrategy.STEPS,\n        eval_strategy = IntervalStrategy.STEPS,\n        save_steps = 1,  # Save checkpoint every 20 steps\n        eval_steps = 1,  # Evaluate every 20 steps (matching save_steps)\n        \n        load_best_model_at_end = True,\n        metric_for_best_model = \"eval_loss\",\n    ),\n    compute_metrics = compute_metrics,\n)\n\n# ... (rest of the code remains the same)\n\n# Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\n# Start training\ntrainer_stats = trainer.train()\n\n# Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n# Generation code (unchanged)\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\nclass EndOfTextCriteria(StoppingCriteria):\n    def __init__(self, eos_token_id):\n        self.eos_token_id = eos_token_id\n\n    def __call__(self, input_ids, scores, **kwargs):\n        return input_ids[0][-1] == self.eos_token_id\n\nstopping_criteria = StoppingCriteriaList([EndOfTextCriteria(tokenizer.eos_token_id)])\n\noutputs = model.generate(**inputs, \n                         max_new_tokens=64, \n                         stopping_criteria=stopping_criteria,\n                         use_cache=True)\n\nprint(tokenizer.batch_decode(outputs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # delete previous trainer\n# del trainer\n\n#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\n\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n\n\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n\n# inputs = tokenizer(\n# [\n#     instruction_prompt.format(\n#         \"Hero Archetype\", # concept_name\n#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n#         \"\", # output - leave this blank for generation!\n#     )\n# ], return_tensors = \"pt\").to(\"cuda\")\n\n\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n#                    repetition_penalty = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\n# add current timestamp to model name\nmodel.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\ntokenizer.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\")\nmodel.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_base_{int(time.time())}\", token = \"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\") # Online saving\ntokenizer.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_{int(time.time())}\", token = \"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\") # Online saving\n\n# # Merge to 16bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n\n# # Merge to 4bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n\n# # Just LoRA adapters\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n    \n    \n# # Save to 8bit Q8_0\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n\n# # Save to 16bit GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n\n# # Save to q4_k_m GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n\n# inputs = tokenizer(\n# [\n#     instruction_prompt.format(\n#         \"Hero Archetype\", # concept_name\n#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n#         \"\", # output - leave this blank for generation!\n#     )\n# ], return_tensors = \"pt\").to(\"cuda\")\n\n\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n#                    repetition_penalty = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instruction  Tuning","metadata":{}},{"cell_type":"code","source":"\n# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n\nimport json\nimport ast\nimport logging\n\nimport csv\n\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    data = list(reader)\n    \ntype(data)\n\n\n# Configure logging\nlogging.basicConfig(\n    filename='transformation_errors.log',\n    filemode='w',\n    level=logging.ERROR,\n    format='%(levelname)s:%(message)s'\n)\n\n# Sample original data\noriginal_data = data\n\ndef transform_data(original_data):\n    \"\"\"\n    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n\n    Parameters:\n        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n\n    Returns:\n        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n    \"\"\"\n    new_data = []\n\n    for idx, entry in enumerate(original_data, start=1):\n        concept_name = entry.get('concept_name', '').strip()\n        detailed_explanation = entry.get('detailed_explanation', '').strip()\n        example_scenario_str = entry.get('example_scenario', '').strip()\n\n        if not concept_name or not detailed_explanation or not example_scenario_str:\n            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n            continue\n\n        # Attempt to parse with json.loads\n        try:\n            example_scenarios = json.loads(example_scenario_str)\n            if not isinstance(example_scenarios, list):\n                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n        except json.JSONDecodeError:\n            # Fallback to ast.literal_eval\n            try:\n                example_scenarios = ast.literal_eval(example_scenario_str)\n                if not isinstance(example_scenarios, list):\n                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n            except (ValueError, SyntaxError) as e:\n                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n                continue\n\n        # Iterate through each scenario and create a new entry\n        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n            if not isinstance(scenario, str):\n                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n                continue\n\n            new_entry = {\n                'concept_name': concept_name,\n                'detailed_explanation': detailed_explanation,\n                'example_scenario': scenario.strip()\n            }\n            new_data.append(new_entry)\n\n    return new_data\n\n# Transform the data\ntransformed_data = transform_data(original_data)\n\n# Optional: Save the transformed data to a JSON file\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n\nprint(f\"Transformation complete. {len(transformed_data)} entries created.\")\nprint(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n\nprint(len(transformed_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef instruction_prompt_func(examples):\n    concept_name = examples[\"concept_name\"]\n    detailed_explanation = examples[\"detailed_explanation\"]\n    example_scenario = examples[\"example_scenario\"]\n    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\npass\n\n\n# convert transformed_data to a huggingface dataset\ninstruction_dataset = Dataset.from_dict(transformed_data)\ninstruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = instruction_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 8,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Use num_train_epochs and warmup_ratio for longer runs!\n        max_steps = 120,\n        warmup_steps = 10,\n        # warmup_ratio = 0.1,\n        # num_train_epochs = 1,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = 5e-5,\n        embedding_learning_rate = 1e-5,\n\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.00,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add current timestamp to model name\nmodel.save_pretrained(f\"qLeap_model_v0_{int(time.time())}\") # Local saving\ntokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\nmodel.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\ntokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n\n# # Merge to 16bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n\n# # Merge to 4bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n\n# # Just LoRA adapters\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n    \n    \n# # Save to 8bit Q8_0\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n\n# # Save to 16bit GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n\n# # Save to q4_k_m GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}