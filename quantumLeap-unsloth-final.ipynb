{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # Install required packages in terminal only. Otherwise jupyter kernel will die and remote server will crash\n",
    "# packages = [\n",
    "#     \"unsloth\",\n",
    "#     \"xformers\",\n",
    "#     \"torch\",\n",
    "#     \"nltk\",\n",
    "#     \"spacy\",\n",
    "#     \"wandb\"cod,\n",
    "#     \"datasets\",\n",
    "#     \"huggingface_hub\"\n",
    "# ]\n",
    "\n",
    "# for package in packages:\n",
    "#     subprocess.run([\"pip\", \"install\", \"-q\", \"-U\", package, \"--no-cache-dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U  unsloth wandb bitsandbytes torch ipywidgets xformers nltk spacy huggingface_hub datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import argparse\n",
    "import wandb  # Weights & Biases integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy English model not found. Downloading...\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda/envs/olabs/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- #\n",
    "# Part 1: Install and Setup Libraries\n",
    "# ----------------------------- #\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print(\"SpaCy English model not found. Downloading...\")\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 2: Load and Clean the Text Data\n",
    "# ----------------------------- #\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    # # Remove Project Gutenberg's license text and headers/footers\n",
    "    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
    "    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n",
    "\n",
    "    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n",
    "    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n",
    "    return text.strip()\n",
    "\n",
    "# Replace 'psychology_of_unconscious.txt' with your actual file path\n",
    "file_path = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/psychology_of_unconscious.txt'\n",
    "clean_text = load_and_clean_text(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6175"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------- #\n",
    "# Part 3: Parse Text into Discourse Units\n",
    "# ----------------------------- #\n",
    "\n",
    "def parse_discourse_units(text):\n",
    "    \"\"\"\n",
    "    Parses text into discourse units using spaCy.\n",
    "    Currently splits text into sentences.\n",
    "    \"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
    "    \n",
    "    discourse_units = []\n",
    "    for para in paragraphs:\n",
    "        doc = nlp(para)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        discourse_units.extend(sentences)\n",
    "    return discourse_units\n",
    "\n",
    "discourse_units = parse_discourse_units(clean_text)\n",
    "\n",
    "# Save discourse_units to a file (Optional)\n",
    "with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.txt', 'w') as f:\n",
    "    for unit in discourse_units:\n",
    "        f.write(unit + '\\n')\n",
    "\n",
    "# If you need to reload from file (Optional)\n",
    "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.txt', 'r') as f:\n",
    "#     discourse_units = f.read().splitlines()\n",
    "\n",
    "len(discourse_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 4: Create Chunks Using Hybrid Strategy\n",
    "# ----------------------------- #\n",
    "\n",
    "def create_chunks(discourse_units, tokenizer, max_length=2048, overlap_size=100):\n",
    "    \"\"\"\n",
    "    Creates chunks from discourse units using a sliding window with overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for unit in discourse_units:\n",
    "        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n",
    "        unit_length = len(unit_tokens)\n",
    "\n",
    "        if current_length + unit_length <= max_length:\n",
    "            current_chunk.append(unit)\n",
    "            current_length += unit_length\n",
    "        else:\n",
    "            # Append the current chunk\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            # Create overlap\n",
    "            overlap_text = ' '.join(current_chunk)[-overlap_size:]\n",
    "            overlap_tokens = tokenizer.encode(overlap_text, add_special_tokens=False)\n",
    "            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n",
    "            # Start new chunk with overlap and current unit\n",
    "            current_chunk = [overlap_text, unit]\n",
    "            current_length = len(tokenizer.encode(overlap_text, add_special_tokens=False)) + unit_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.9: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.209 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a2f899ddb1471f92629ca8ee449b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/olabs/lib/python3.12/site-packages/unsloth/models/_utils.py:866: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- #\n",
    "# Part 5: Load the Tokenizer and Model\n",
    "# ----------------------------- #\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# if the model is already downloaded, then don't download it again; otherwise download it\n",
    "import os\n",
    "\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "models_dir = os.path.join(os.path.dirname(os.getcwd()), \"models\")\n",
    "model_path = os.path.join(models_dir, model_name)\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_path,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "else:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        token=\"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\",\n",
    "    )\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ----------------------------- #\n",
    "# Part 6: Create Chunks (After Tokenizer is Loaded)\n",
    "# ----------------------------- #\n",
    "\n",
    "chunks = create_chunks(discourse_units, tokenizer, max_length=2048, overlap_size=100)\n",
    "\n",
    "# Save chunks to a file (Optional)\n",
    "with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.txt', 'w') as f:\n",
    "    for unit in discourse_units:\n",
    "        f.write(unit + '\\n')\n",
    "\n",
    "# If you need to reload from file (Optional)\n",
    "# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.txt', 'r') as f:\n",
    "#     discourse_units = f.read().splitlines()\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851994aaa8af45f2b38d9b6d84384686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/89 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 89\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------- #\n",
    "# Part 7: Create and Tokenize Dataset\n",
    "# ----------------------------- #\n",
    "\n",
    "# Create a Dataset object from chunks\n",
    "\n",
    "book_title = 'Psychology of the Unconscious by C. G. Jung'\n",
    "wikipedia_prompt = \"\"\"\n",
    "Psychology Book\n",
    "\n",
    "### Title: {}\n",
    "\n",
    "### Article: {}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    titles = book_title\n",
    "    texts  = examples[\"text\"]\n",
    "    outputs = []\n",
    "    for title, text in zip([book_title]*len(chunks), texts):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n",
    "        outputs.append(text)\n",
    "    return { \"text\" : outputs, }\n",
    "pass\n",
    "\n",
    "# convert chunks variable to huggingface dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": chunks})\n",
    "\n",
    "dataset = dataset.train_test_split(train_size = 0.90)[\"train\"]\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57509effd1e84983b836343a0946fc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/89 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- #\n",
    "# Part 8: Configure Training Arguments\n",
    "# ----------------------------- #\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
    "        max_steps = 120,\n",
    "        warmup_steps = 10,\n",
    "        # warmup_ratio = 0.1,\n",
    "        # num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 9: Define Compute Metrics Function\n",
    "# ----------------------------- #\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes perplexity based on model predictions and labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # Convert to torch tensors\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Ensure shapes match\n",
    "    if logits.shape[:2] != labels.shape:\n",
    "        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n",
    "    \n",
    "    # Shift logits and labels\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "    # Check label values\n",
    "    if shift_labels.max() >= model.config.vocab_size:\n",
    "        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n",
    "    \n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 10: Initialize the Trainer\n",
    "# ----------------------------- #\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    level=logging.INFO,  # Set to DEBUG for more detailed logs\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n",
      "concept_name: Hero Archetype\n",
      "detailed_explanation: The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "Here's an example scenario that illustrates the concept of the hero archetype:\n",
      "\n",
      "**Scenario:** In a small village nestled in the mountains, there lived a young woman named Ava. Ava was known throughout the village for her exceptional courage and determination. One day, a severe drought struck the land, and the village's crops began to wither and die. The villagers, desperate for a solution, looked to Ava as a potential hero.\n",
      "\n",
      "Ava, moved by the villagers' pleas, embarked on a perilous journey to a nearby city in search of a magical spring said to have the power to restore the land's fertility. Along the way, she encountered numerous challenges, including treacherous terrain, ferocious beasts, and treacherous bandits. Undeterred, Ava persevered, using her wit and bravery to overcome each obstacle.\n",
      "\n",
      "Upon arriving at the city, Ava discovered that the magical spring was guarded by a powerful dragon. Instead of fleeing, Ava drew upon her inner strength and confronted the dragon. Through a series of clever tactics and sheer courage, Ava managed to defeat the dragon and claim the magical spring.\n",
      "\n",
      "Ava's return to the village with the magical spring marked a turning point in the community's fortunes. The spring's waters revitalized the land, and the villagers\n",
      "<|begin_of_text|>Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n",
      "concept_name: Hero Archetype\n",
      "detailed_explanation: The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "### Hero Archetype in the example of the protagonist in the hero archetype, the protagonist, a common, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "### Hero Archetype in the example of the protagonist in the hero archetype, the protagonist, a common, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "### Hero Archetype in the example of the protagonist in the hero archetype, the protagonist, a common, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "### Hero Archetype in the example of the protagonist in the hero archetype, the protagonist, a common, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "### Hero Archetype in the example of the protagonist in the hero archetype, the protagonist, a common, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "### Hero Archetype in the example of the protagonist in the hero archetype, the protagonist, a common, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "### Hero Archetype in the example of the protagonist in the hero archetype, the protagonist, a common, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "### Hero Archetype in the example of the protagonist in the hero archetype, the protagonist, a common, and a quest for a greater purpose.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n",
    "concept_name: {}\n",
    "detailed_explanation: {}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    instruction_prompt.format(\n",
    "        \"Hero Archetype\", # concept_name\n",
    "        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Text Streaming\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256)\n",
    "\n",
    "# inputs = tokenizer(\n",
    "# [\n",
    "#     instruction_prompt.format(\n",
    "#         \"Hero Archetype\", # concept_name\n",
    "#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n",
    "#         \"\", # output - leave this blank for generation!\n",
    "#     )\n",
    "# ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n",
    "#                    repetition_penalty = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 11: Start Training\n",
    "# ----------------------------- #\n",
    "\n",
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "trainer_stats = trainer.train()\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add current timestamp to model name\n",
    "model.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\n",
    "tokenizer.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\")\n",
    "model.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_base_{int(time.time())}\", token = \"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\") # Online saving\n",
    "tokenizer.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_{int(time.time())}\", token = \"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\") # Online saving\n",
    "\n",
    "# # Merge to 16bit\n",
    "# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n",
    "# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n",
    "\n",
    "# # Merge to 4bit\n",
    "# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n",
    "# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n",
    "\n",
    "# # Just LoRA adapters\n",
    "# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n",
    "# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n",
    "    \n",
    "    \n",
    "# # Save to 8bit Q8_0\n",
    "# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n",
    "# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n",
    "\n",
    "# # Save to 16bit GGUF\n",
    "# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n",
    "# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n",
    "\n",
    "# # Save to q4_k_m GGUF\n",
    "# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n",
    "# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n",
    "concept_name: {}\n",
    "detailed_explanation: {}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    instruction_prompt.format(\n",
    "        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Text Streaming\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    instruction_prompt.format(\n",
    "        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   repetition_penalty = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction  Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import logging\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data = list(reader)\n",
    "    \n",
    "type(data)\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='transformation_errors.log',\n",
    "    filemode='w',\n",
    "    level=logging.ERROR,\n",
    "    format='%(levelname)s:%(message)s'\n",
    ")\n",
    "\n",
    "# Sample original data\n",
    "original_data = data\n",
    "\n",
    "def transform_data(original_data):\n",
    "    \"\"\"\n",
    "    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n",
    "\n",
    "    Returns:\n",
    "        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    for idx, entry in enumerate(original_data, start=1):\n",
    "        concept_name = entry.get('concept_name', '').strip()\n",
    "        detailed_explanation = entry.get('detailed_explanation', '').strip()\n",
    "        example_scenario_str = entry.get('example_scenario', '').strip()\n",
    "\n",
    "        if not concept_name or not detailed_explanation or not example_scenario_str:\n",
    "            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Attempt to parse with json.loads\n",
    "        try:\n",
    "            example_scenarios = json.loads(example_scenario_str)\n",
    "            if not isinstance(example_scenarios, list):\n",
    "                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback to ast.literal_eval\n",
    "            try:\n",
    "                example_scenarios = ast.literal_eval(example_scenario_str)\n",
    "                if not isinstance(example_scenarios, list):\n",
    "                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n",
    "                continue\n",
    "\n",
    "        # Iterate through each scenario and create a new entry\n",
    "        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n",
    "            if not isinstance(scenario, str):\n",
    "                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n",
    "                continue\n",
    "\n",
    "            new_entry = {\n",
    "                'concept_name': concept_name,\n",
    "                'detailed_explanation': detailed_explanation,\n",
    "                'example_scenario': scenario.strip()\n",
    "            }\n",
    "            new_data.append(new_entry)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# Transform the data\n",
    "transformed_data = transform_data(original_data)\n",
    "\n",
    "# Optional: Save the transformed data to a JSON file\n",
    "with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Transformation complete. {len(transformed_data)} entries created.\")\n",
    "print(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n",
    "\n",
    "print(len(transformed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n",
    "concept_name: {}\n",
    "detailed_explanation: {}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "def instruction_prompt_func(examples):\n",
    "    concept_name = examples[\"concept_name\"]\n",
    "    detailed_explanation = examples[\"detailed_explanation\"]\n",
    "    example_scenario = examples[\"example_scenario\"]\n",
    "    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\n",
    "pass\n",
    "\n",
    "\n",
    "# convert transformed_data to a huggingface dataset\n",
    "instruction_dataset = Dataset.from_dict(transformed_data)\n",
    "instruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = instruction_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        # Use num_train_epochs and warmup_ratio for longer runs!\n",
    "        max_steps = 120,\n",
    "        warmup_steps = 10,\n",
    "        # warmup_ratio = 0.1,\n",
    "        # num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n",
    "concept_name: {}\n",
    "detailed_explanation: {}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    instruction_prompt.format(\n",
    "        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Text Streaming\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    instruction_prompt.format(\n",
    "        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   repetition_penalty = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add current timestamp to model name\n",
    "model.save_pretrained(f\"qLeap_model_v0_{int(time.time())}\") # Local saving\n",
    "tokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\n",
    "model.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n",
    "tokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n",
    "\n",
    "# # Merge to 16bit\n",
    "# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n",
    "# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n",
    "\n",
    "# # Merge to 4bit\n",
    "# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n",
    "# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n",
    "\n",
    "# # Just LoRA adapters\n",
    "# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n",
    "# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n",
    "    \n",
    "    \n",
    "# # Save to 8bit Q8_0\n",
    "# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n",
    "# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n",
    "\n",
    "# # Save to 16bit GGUF\n",
    "# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n",
    "# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n",
    "\n",
    "# # Save to q4_k_m GGUF\n",
    "# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n",
    "# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n",
    "concept_name: {}\n",
    "detailed_explanation: {}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    instruction_prompt.format(\n",
    "        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Text Streaming\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    instruction_prompt.format(\n",
    "        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   repetition_penalty = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
