{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ----------------------------- #\n# Imports and Setup\n# ----------------------------- #\n\nimport os\nimport re\nimport torch\nimport nltk\nimport spacy\nimport logging\nimport json\nimport ast\nimport time\nimport csv\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    IntervalStrategy,\n    StoppingCriteria,\n    StoppingCriteriaList,\n    Adafactor\n)\nfrom datasets import Dataset, load_dataset\nimport wandb  # Weights & Biases integration\n\n# TPU-specific imports\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\n# Ensure NLTK's punkt tokenizer is available\nnltk.download('punkt')\n\n# Initialize spaCy English model\ntry:\n    nlp = spacy.load('en_core_web_sm')\nexcept OSError:\n    print(\"SpaCy English model not found. Downloading...\")\n    os.system('python -m spacy download en_core_web_sm')\n    nlp = spacy.load('en_core_web_sm')\n\n# Configure logging\nlogging.basicConfig(\n    filename='transformation_errors.log',\n    filemode='w',\n    level=logging.ERROR,\n    format='%(levelname)s:%(message)s'\n)","metadata":{"_uuid":"c64f17af-8583-4b43-a6b0-80d72a85f71a","_cell_guid":"f6adf8c1-3421-447a-9e53-a05ef284a57d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Main Function for TPU Training\n# ----------------------------- #\n\ndef main(index):\n    \"\"\"\n    Main function to be run on each TPU core.\n    \"\"\"\n\n    # Set up device\n    device = xm.xla_device()\n    print(f\"Process {index} running on device {device}\")\n\n    # ----------------------------- #\n    # Part 1: Load and Clean the Text Data\n    # ----------------------------- #\n\n    def load_and_clean_text(file_path):\n        \"\"\"\n        Loads text from a file and removes Project Gutenberg's license and headers/footers.\n        \"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            text = f.read()\n        return text.strip()\n\n    # Replace with your actual file path\n    file_path = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/psychology_of_unconscious.txt'\n    clean_text = load_and_clean_text(file_path)\n\n    # ----------------------------- #\n    # Part 2: Parse Text into Discourse Units\n    # ----------------------------- #\n\n    def parse_discourse_units(text):\n        \"\"\"\n        Parses text into discourse units using spaCy.\n        Currently splits text into sentences.\n        \"\"\"\n        paragraphs = text.split('\\n\\n')\n        paragraphs = [para.strip() for para in paragraphs if para.strip()]\n        \n        discourse_units = []\n        for para in paragraphs:\n            doc = nlp(para)\n            sentences = [sent.text for sent in doc.sents]\n            discourse_units.extend(sentences)\n        return discourse_units\n\n    discourse_units = parse_discourse_units(clean_text)\n\n    # Save discourse_units to a file (Optional)\n    discourse_units_file = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.txt'\n    if index == 0:\n        with open(discourse_units_file, 'w') as f:\n            for unit in discourse_units:\n                f.write(unit + '\\n')\n\n    # ----------------------------- #\n    # Part 3: Create Chunks Using Hybrid Strategy\n    # ----------------------------- #\n\n    def create_chunks(discourse_units, tokenizer, max_length=2048, overlap_size=100):\n        \"\"\"\n        Creates chunks from discourse units using a sliding window with overlapping chunks.\n        \"\"\"\n        chunks = []\n        current_chunk = []\n        current_length = 0\n\n        for unit in discourse_units:\n            unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n            unit_length = len(unit_tokens)\n\n            if current_length + unit_length <= max_length:\n                current_chunk.append(unit)\n                current_length += unit_length\n            else:\n                # Append the current chunk\n                chunks.append(' '.join(current_chunk))\n                # Create overlap\n                overlap_text = ' '.join(current_chunk)[-overlap_size:]\n                overlap_tokens = tokenizer.encode(overlap_text, add_special_tokens=False)\n                overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n                # Start new chunk with overlap and current unit\n                current_chunk = [overlap_text, unit]\n                current_length = len(tokenizer.encode(overlap_text, add_special_tokens=False)) + unit_length\n\n        if current_chunk:\n            chunks.append(' '.join(current_chunk))\n\n        return chunks\n\n    # ----------------------------- #\n    # Part 4: Load the Tokenizer and Model\n    # ----------------------------- #\n\n    # Retrieve Hugging Face token from environment variable\n    hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n    if not hf_token:\n        raise ValueError(\"Please set your Hugging Face token in the HUGGINGFACE_TOKEN environment variable.\")\n\n    model_name = \"gpt2\"  # Using GPT-2 as an example; replace with appropriate model\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    tokenizer.pad_token = tokenizer.eos_token  # Ensure pad_token is set\n\n    # Load the model\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Move model to TPU device\n    model.to(device)\n\n    # ----------------------------- #\n    # Part 5: Create Chunks (After Tokenizer is Loaded)\n    # ----------------------------- #\n\n    chunks = create_chunks(discourse_units, tokenizer, max_length=1024, overlap_size=100)\n\n    # Save chunks to a file (Optional)\n    chunks_file = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.txt'\n    if index == 0:\n        with open(chunks_file, 'w') as f:\n            for unit in chunks:\n                f.write(unit + '\\n')\n\n    # ----------------------------- #\n    # Part 6: Create and Tokenize Dataset\n    # ----------------------------- #\n\n    book_title = 'Psychology of the Unconscious by C. G. Jung'\n    prompt_template = \"\"\"\n    Psychology Book\n\n    ### Title: {}\n\n    ### Article: {}\n    \"\"\"\n\n    def formatting_prompts_func(examples):\n        outputs = []\n        for text in examples[\"text\"]:\n            formatted_text = prompt_template.format(book_title, text)\n            outputs.append(formatted_text)\n        return {\"text\": outputs}\n\n    # Convert chunks to Hugging Face Dataset\n    dataset = Dataset.from_dict({\"text\": chunks})\n    dataset = dataset.map(formatting_prompts_func, batched=True, num_proc=1)\n\n    # Tokenize dataset\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n\n    tokenized_dataset = dataset.map(\n        tokenize_function,\n        batched=True,\n        num_proc=1,\n        remove_columns=[\"text\"],\n    )\n\n    # Group texts\n    max_seq_length = 1024  # Adjusted for TPU memory constraints\n    def group_texts(examples):\n        # Concatenate all texts\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = len(concatenated_examples[\"input_ids\"])\n        # Drop the small remainder\n        total_length = (total_length // max_seq_length) * max_seq_length\n        # Split by chunks of max_seq_length\n        result = {\n            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    lm_datasets = tokenized_dataset.map(\n        group_texts,\n        batched=True,\n        num_proc=1,\n    )\n\n    # ----------------------------- #\n    # Part 7: Configure Training Arguments\n    # ----------------------------- #\n\n    # Initialize wandb (only once)\n    if index == 0:\n        wandb.init(project=\"huggingface_tpu\", name=\"tpu_training_run\")\n\n    training_args = TrainingArguments(\n        output_dir=\"./tpu_outputs\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        gradient_accumulation_steps=1,\n        eval_steps=500,\n        logging_steps=100,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        learning_rate=5e-5,\n        warmup_steps=1000,\n        fp16=False,  # TPUs use bfloat16 internally\n        tpu_num_cores=8,\n        report_to=\"wandb\",\n        disable_tqdm=True,\n        prediction_loss_only=True,\n        logging_first_step=True,\n    )\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n    # ----------------------------- #\n    # Part 8: Initialize Trainer\n    # ----------------------------- #\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=lm_datasets,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    # ----------------------------- #\n    # Part 9: Start Training\n    # ----------------------------- #\n\n    trainer.train()\n\n    # Save the model (only once)\n    if index == 0:\n        trainer.save_model(\"./tpu_outputs\")\n\n    # ----------------------------- #\n    # Part 10: Inference\n    # ----------------------------- #\n\n    instruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n    ### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\n    concept_name: {}\n    detailed_explanation: {}\n\n    ### Response:\n    \"\"\"\n\n    def generate_response(concept_name, detailed_explanation):\n        prompt = instruction_prompt.format(concept_name, detailed_explanation)\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        outputs = model.generate(\n            **inputs,\n            max_length=512,\n            num_beams=5,\n            early_stopping=True,\n            no_repeat_ngram_size=2,\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(response)\n\n    if index == 0:\n        generate_response(\n            concept_name=\"Hero Archetype\",\n            detailed_explanation=(\n                \"The hero archetype is a common motif in literature and folklore, representing \"\n                \"a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\"\n            )\n        )\n\n# ----------------------------- #\n# Run the main function on TPU cores\n# ----------------------------- #\n\nif __name__ == \"__main__\":\n    xmp.spawn(main, args=(), nprocs=8)","metadata":{"_uuid":"10662e98-e210-48c7-a9dd-5354c61709c1","_cell_guid":"e566f13b-6404-42b5-b326-499c1239afb8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}