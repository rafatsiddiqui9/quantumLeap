{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pip install --upgrade jupyter ipywidgets --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q --upgrade numpy scipy pandas --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q torch transformers datasets tokenizers accelerate spacy nltk  --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q spacy\n",
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q -U wandb tensorboard --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export http_proxy=\n",
    "!export https_proxy="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to hugging face\n",
    "!huggingface-cli login --token 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 --add-to-git-credential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# os.system('pip install -q torch transformers datasets tokenizers accelerate spacy nltk')\n",
    "# os.system('python -m spacy download en_core_web_trf')\n",
    "# os.system('export http_proxy=')\n",
    "# os.system('export https_proxy=')\n",
    "\n",
    "# login to hugging face\n",
    "os.system('huggingface-cli login --token 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 --add-to-git-credential')\n",
    "\n",
    "# Step by Step\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import argparse\n",
    "import wandb  # Added for Weights & Biases integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------- #\n",
    "# Part 1: Install and Setup Libraries\n",
    "# ----------------------------- #\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "except OSError:\n",
    "    print(\"SpaCy English model not found. Downloading...\")\n",
    "    os.system('python -m spacy download en_core_web_trf')\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 2: Load and Clean the Text Data\n",
    "# ----------------------------- #\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text.strip()\n",
    "\n",
    "# Replace 'psychology_of_unconscious.txt' with your actual file path\n",
    "file_path = 'psychology_of_unconscious.txt'\n",
    "clean_text = load_and_clean_text(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- #\n",
    "# # Part 3: Parse Text into Discourse Units\n",
    "# # ----------------------------- #\n",
    "\n",
    "# def parse_discourse_units(text):\n",
    "#     \"\"\"\n",
    "#     Parses text into discourse units using spaCy.\n",
    "#     Currently splits text into sentences.\n",
    "#     \"\"\"\n",
    "#     paragraphs = text.split('\\n\\n')\n",
    "#     paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
    "    \n",
    "#     discourse_units = []\n",
    "#     for para in paragraphs:\n",
    "#         doc = nlp(para)\n",
    "#         sentences = [sent.text for sent in doc.sents]\n",
    "#         discourse_units.extend(sentences)\n",
    "#     return discourse_units\n",
    "\n",
    "# discourse_units = parse_discourse_units(clean_text)\n",
    "\n",
    "# # save the discourse units to a file\n",
    "# with open('discourse_units.txt', 'w') as f:\n",
    "#     for unit in discourse_units:\n",
    "#         f.write(unit + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_variables():\n",
    "    current_module = sys.modules[__name__]\n",
    "    variables = [(name, type(value).__name__, sys.getsizeof(value))\n",
    "                 for name, value in vars(current_module).items()\n",
    "                 if not name.startswith('_')]\n",
    "    \n",
    "    sorted_vars = sorted(variables, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"Variables in current session:\")\n",
    "    for name, type_name, size in sorted_vars:\n",
    "        print(f\"{name}: Type = {type_name}, Size = {size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_variables()\n",
    "import torch\n",
    "import gc\n",
    "import weakref\n",
    "import sys\n",
    "\n",
    "def list_top_gpu_variables(top_n=10):\n",
    "    tensor_list = []\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if isinstance(obj, torch.Tensor) and obj.is_cuda:\n",
    "                tensor_list.append(weakref.ref(obj))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    tensor_sizes = []\n",
    "    for tensor_ref in tensor_list:\n",
    "        tensor = tensor_ref()\n",
    "        if tensor is not None:\n",
    "            try:\n",
    "                size = tensor.element_size() * tensor.nelement()\n",
    "                tensor_sizes.append((tensor, size))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    tensor_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Top GPU Variables:\")\n",
    "    for idx, (tensor, size) in enumerate(tensor_sizes[:top_n], 1):\n",
    "        print(f\"{idx}. Tensor Shape: {tensor.shape}, Size: {size / 1e6:.2f} MB\")\n",
    "\n",
    "list_top_gpu_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Check if variables exist before deleting\n",
    "variables_to_delete = ['model', 'tokenizer', 'discourse_units', 'chunks', 'train_dataset', 'eval_dataset']\n",
    "for var in variables_to_delete:\n",
    "    if var in locals() or var in globals():\n",
    "        exec(f\"del {var}\")\n",
    "\n",
    "# Clear CUDA cache and collect garbage\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 4: Load the Tokenizer and Model\n",
    "# ----------------------------- #\n",
    "\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B\"\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model with half-precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False  # Disable caching for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------- #\n",
    "# Part 5: Create Chunks\n",
    "# ----------------------------- #\n",
    "\n",
    "with open('discourse_units.txt', 'r') as f:\n",
    "    discourse_units = f.read().splitlines()\n",
    "\n",
    "\n",
    "def create_chunks(discourse_units, tokenizer, max_length=1024, overlap_size=100):\n",
    "    \"\"\"\n",
    "    Creates chunks from discourse units using a sliding window with overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    current_length = 0\n",
    "\n",
    "    for unit in discourse_units:\n",
    "        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n",
    "        unit_length = len(unit_tokens)\n",
    "\n",
    "        if current_length + unit_length <= max_length:\n",
    "            current_chunk += unit + ' '\n",
    "            current_length += unit_length\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            overlap_tokens = tokenizer.encode(current_chunk, add_special_tokens=False)[-overlap_size:]\n",
    "            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n",
    "            current_chunk = overlap_text + ' ' + unit + ' '\n",
    "            current_length = len(tokenizer.encode(current_chunk, add_special_tokens=False))\n",
    "    \n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = create_chunks(discourse_units, tokenizer)\n",
    "\n",
    "# save the chunks to a file\n",
    "with open('chunks.txt', 'w') as f:\n",
    "    for chunk in chunks:\n",
    "        f.write(chunk + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('discourse_units.txt', 'r') as f:\n",
    "#     discourse_units = f.read().splitlines()\n",
    "\n",
    "# with open('chunks.txt', 'r') as f:\n",
    "#     chunks = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 6: Create and Tokenize Dataset\n",
    "# ----------------------------- #\n",
    "\n",
    "dataset = Dataset.from_dict({'text': chunks})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=1024,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=False,\n",
    "    )\n",
    "    \n",
    "    # Create labels by shifting the input_ids\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    # Shift the labels to align with the next token prediction\n",
    "    for i, label in enumerate(result[\"labels\"]):\n",
    "        result[\"labels\"][i] = [-100] + label[:-1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=20,  # Adjust based on available CPU cores\n",
    "    remove_columns=['text'],\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split['train']\n",
    "eval_dataset = split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Part 7: Configure Training Arguments\n",
    "# ----------------------------- #\n",
    "import wandb\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"quantum-leap-training\",\n",
    "    config={\n",
    "        \"model_name\": model_name,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 2,\n",
    "        \"learning_rate\": 2e-5,\n",
    "    },\n",
    "    sync_tensorboard=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./meta-llama-3.1-8b-finetuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Increased batch size\n",
    "    gradient_accumulation_steps=4,  # Reduced accumulation steps\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # Enable mixed precision training\n",
    "    bf16=True,\n",
    "    optim='adamw_torch_fused',\n",
    "    save_strategy='steps',  # Changed from 'epoch' to 'steps'\n",
    "    save_steps=500,  # Save every 500 steps\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='perplexity',\n",
    "    report_to=\"wandb\",  # Changed to report to Weights & Biases\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=20,  # Utilize multiple CPU cores for data loading\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------- #\n",
    "# Part 8: Define Compute Metrics Function\n",
    "# ----------------------------- #\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes perplexity based on model predictions and labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------- #\n",
    "# Part 9: Initialize and Start Training\n",
    "# ----------------------------- #\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Train Quantum Leap model')\n",
    "    # Use parse_known_args to ignore unrecognized arguments\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Initialize Trainer without moving the model to a specific device\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    trainer.save_model('./meta-llama-3.1-8b-finetuned')\n",
    "\n",
    "    print(\"Training completed and model saved!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_transfer in /home/ubuntu/miniconda/envs/olabs/lib/python3.12/site-packages (0.1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   4%|▍         | 1/26 [00:01<00:27,  1.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afe75c65f53440c8e974e310399ad1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  12%|█▏        | 3/26 [13:58<1:42:57, 268.60s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121cedea788c40289419d05216f8f9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  15%|█▌        | 4/26 [14:00<59:51, 163.23s/it]  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8e4b8464244037b56263d0fe219feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  42%|████▏     | 11/26 [27:44<10:13, 40.91s/it]  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b9d25ed534487b84f249f13448fbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/32.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  77%|███████▋  | 20/26 [1:57:34<10:00, 100.04s/it]   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527ee101fe9b4b609ac2bb66a439dde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  92%|█████████▏| 24/26 [1:57:42<00:50, 25.34s/it] "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a143e4a4b56249a7a857a40a8fe8e6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|██████████| 26/26 [1:57:45<00:00, 271.74s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from tqdm import tqdm\n",
    "\n",
    "api = HfApi()\n",
    "folder_path = '/home/ubuntu/quantumLeap/meta-llama-3.1-8b-finetuned'\n",
    "repo_id = 'olabs-ai/meta-llama-3.1-8b-o1'\n",
    "token = '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'\n",
    "\n",
    "# Get list of all files in the folder\n",
    "file_paths = []\n",
    "for root, _, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Upload files with a progress bar\n",
    "for file_path in tqdm(file_paths, desc=\"Uploading files\"):\n",
    "    relative_path = os.path.relpath(file_path, folder_path)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file_path,\n",
    "        path_in_repo=relative_path,\n",
    "        repo_id=repo_id,\n",
    "        token=token\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#  pip install --upgrade jupyter ipywidgets --no-cache-dir\n",
    "# pip install -q --upgrade numpy scipy pandas --no-cache-dir\n",
    "# pip install -q torch transformers datasets tokenizers accelerate spacy nltk  --no-cache-dir\n",
    "# !pip install -q spacy\n",
    "# !python -m spacy download en_core_web_trf\n",
    "# pip install -q -U wandb tensorboard --no-cache-dir\n",
    "!export http_proxy=\n",
    "!export https_proxy=\n",
    "# login to hugging face\n",
    "!huggingface-cli login --token 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 --add-to-git-credential\n",
    "# Step by Step\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# os.system('pip install -q torch transformers datasets tokenizers accelerate spacy nltk')\n",
    "# os.system('python -m spacy download en_core_web_trf')\n",
    "# os.system('export http_proxy=')\n",
    "# os.system('export https_proxy=')\n",
    "\n",
    "# login to hugging face\n",
    "os.system('huggingface-cli login --token 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 --add-to-git-credential')\n",
    "\n",
    "# Step by Step\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import argparse\n",
    "import wandb  # Added for Weights & Biases integration\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 1: Install and Setup Libraries\n",
    "# ----------------------------- #\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "except OSError:\n",
    "    print(\"SpaCy English model not found. Downloading...\")\n",
    "    os.system('python -m spacy download en_core_web_trf')\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 2: Load and Clean the Text Data\n",
    "# ----------------------------- #\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text.strip()\n",
    "\n",
    "# Replace 'psychology_of_unconscious.txt' with your actual file path\n",
    "file_path = 'psychology_of_unconscious.txt'\n",
    "clean_text = load_and_clean_text(file_path)\n",
    "\n",
    "\n",
    "\n",
    "# # ----------------------------- #\n",
    "# # Part 3: Parse Text into Discourse Units\n",
    "# # ----------------------------- #\n",
    "\n",
    "# def parse_discourse_units(text):\n",
    "#     \"\"\"\n",
    "#     Parses text into discourse units using spaCy.\n",
    "#     Currently splits text into sentences.\n",
    "#     \"\"\"\n",
    "#     paragraphs = text.split('\\n\\n')\n",
    "#     paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
    "    \n",
    "#     discourse_units = []\n",
    "#     for para in paragraphs:\n",
    "#         doc = nlp(para)\n",
    "#         sentences = [sent.text for sent in doc.sents]\n",
    "#         discourse_units.extend(sentences)\n",
    "#     return discourse_units\n",
    "\n",
    "# discourse_units = parse_discourse_units(clean_text)\n",
    "\n",
    "# # save the discourse units to a file\n",
    "# with open('discourse_units.txt', 'w') as f:\n",
    "#     for unit in discourse_units:\n",
    "#         f.write(unit + '\\n')\n",
    "def list_variables():\n",
    "    current_module = sys.modules[__name__]\n",
    "    variables = [(name, type(value).__name__, sys.getsizeof(value))\n",
    "                 for name, value in vars(current_module).items()\n",
    "                 if not name.startswith('_')]\n",
    "    \n",
    "    sorted_vars = sorted(variables, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"Variables in current session:\")\n",
    "    for name, type_name, size in sorted_vars:\n",
    "        print(f\"{name}: Type = {type_name}, Size = {size} bytes\")\n",
    "\n",
    "list_variables()\n",
    "import torch\n",
    "import gc\n",
    "import weakref\n",
    "import sys\n",
    "\n",
    "def list_top_gpu_variables(top_n=10):\n",
    "    tensor_list = []\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if isinstance(obj, torch.Tensor) and obj.is_cuda:\n",
    "                tensor_list.append(weakref.ref(obj))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    tensor_sizes = []\n",
    "    for tensor_ref in tensor_list:\n",
    "        tensor = tensor_ref()\n",
    "        if tensor is not None:\n",
    "            try:\n",
    "                size = tensor.element_size() * tensor.nelement()\n",
    "                tensor_sizes.append((tensor, size))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    tensor_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Top GPU Variables:\")\n",
    "    for idx, (tensor, size) in enumerate(tensor_sizes[:top_n], 1):\n",
    "        print(f\"{idx}. Tensor Shape: {tensor.shape}, Size: {size / 1e6:.2f} MB\")\n",
    "\n",
    "list_top_gpu_variables()\n",
    "import gc\n",
    "\n",
    "# Check if variables exist before deleting\n",
    "variables_to_delete = ['model', 'tokenizer', 'discourse_units', 'chunks', 'train_dataset', 'eval_dataset']\n",
    "for var in variables_to_delete:\n",
    "    if var in locals() or var in globals():\n",
    "        exec(f\"del {var}\")\n",
    "\n",
    "# Clear CUDA cache and collect garbage\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 4: Load the Tokenizer and Model\n",
    "# ----------------------------- #\n",
    "\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B\"\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model with half-precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False  # Disable caching for training\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 5: Create Chunks\n",
    "# ----------------------------- #\n",
    "\n",
    "with open('discourse_units.txt', 'r') as f:\n",
    "    discourse_units = f.read().splitlines()\n",
    "\n",
    "\n",
    "def create_chunks(discourse_units, tokenizer, max_length=1024, overlap_size=100):\n",
    "    \"\"\"\n",
    "    Creates chunks from discourse units using a sliding window with overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    current_length = 0\n",
    "\n",
    "    for unit in discourse_units:\n",
    "        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n",
    "        unit_length = len(unit_tokens)\n",
    "\n",
    "        if current_length + unit_length <= max_length:\n",
    "            current_chunk += unit + ' '\n",
    "            current_length += unit_length\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            overlap_tokens = tokenizer.encode(current_chunk, add_special_tokens=False)[-overlap_size:]\n",
    "            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n",
    "            current_chunk = overlap_text + ' ' + unit + ' '\n",
    "            current_length = len(tokenizer.encode(current_chunk, add_special_tokens=False))\n",
    "    \n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = create_chunks(discourse_units, tokenizer)\n",
    "\n",
    "# save the chunks to a file\n",
    "with open('chunks.txt', 'w') as f:\n",
    "    for chunk in chunks:\n",
    "        f.write(chunk + '\\n')\n",
    "# with open('discourse_units.txt', 'r') as f:\n",
    "#     discourse_units = f.read().splitlines()\n",
    "\n",
    "# with open('chunks.txt', 'r') as f:\n",
    "#     chunks = f.read().splitlines()\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 6: Create and Tokenize Dataset\n",
    "# ----------------------------- #\n",
    "\n",
    "dataset = Dataset.from_dict({'text': chunks})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=1024,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=False,\n",
    "    )\n",
    "    \n",
    "    # Create labels by shifting the input_ids\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    # Shift the labels to align with the next token prediction\n",
    "    for i, label in enumerate(result[\"labels\"]):\n",
    "        result[\"labels\"][i] = [-100] + label[:-1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=20,  # Adjust based on available CPU cores\n",
    "    remove_columns=['text'],\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split['train']\n",
    "eval_dataset = split['test']\n",
    "# ----------------------------- #\n",
    "# Part 7: Configure Training Arguments\n",
    "# ----------------------------- #\n",
    "import wandb\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"quantum-leap-training\",\n",
    "    config={\n",
    "        \"model_name\": model_name,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 2,\n",
    "        \"learning_rate\": 2e-5,\n",
    "    },\n",
    "    sync_tensorboard=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./meta-llama-3.1-8b-finetuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Increased batch size\n",
    "    gradient_accumulation_steps=4,  # Reduced accumulation steps\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # Enable mixed precision training\n",
    "    bf16=True,\n",
    "    optim='adamw_torch_fused',\n",
    "    save_strategy='steps',  # Changed from 'epoch' to 'steps'\n",
    "    save_steps=500,  # Save every 500 steps\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='perplexity',\n",
    "    report_to=\"wandb\",  # Changed to report to Weights & Biases\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=20,  # Utilize multiple CPU cores for data loading\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 8: Define Compute Metrics Function\n",
    "# ----------------------------- #\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes perplexity based on model predictions and labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Part 9: Initialize and Start Training\n",
    "# ----------------------------- #\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Train Quantum Leap model')\n",
    "    # Use parse_known_args to ignore unrecognized arguments\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Initialize Trainer without moving the model to a specific device\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    trainer.save_model('./meta-llama-3.1-8b-finetuned')\n",
    "\n",
    "    print(\"Training completed and model saved!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "pip install hf_transfer\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from tqdm import tqdm\n",
    "\n",
    "api = HfApi()\n",
    "folder_path = '/home/ubuntu/quantumLeap/meta-llama-3.1-8b-finetuned'\n",
    "repo_id = 'olabs-ai/meta-llama-3.1-8b-o1'\n",
    "token = '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'\n",
    "\n",
    "# Get list of all files in the folder\n",
    "file_paths = []\n",
    "for root, _, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Upload files with a progress bar\n",
    "for file_path in tqdm(file_paths, desc=\"Uploading files\"):\n",
    "    relative_path = os.path.relpath(file_path, folder_path)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file_path,\n",
    "        path_in_repo=relative_path,\n",
    "        repo_id=repo_id,\n",
    "        token=token\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
