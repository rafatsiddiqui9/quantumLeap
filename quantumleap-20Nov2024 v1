{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:43:16.892554Z","iopub.execute_input":"2024-11-22T06:43:16.892924Z","iopub.status.idle":"2024-11-22T06:43:16.910855Z","shell.execute_reply.started":"2024-11-22T06:43:16.892888Z","shell.execute_reply":"2024-11-22T06:43:16.910053Z"}},"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ----------------------------- #\n# Part 1.1: Install and Setup Libraries\n# ----------------------------- #\n\n# !python -m xformers.info\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n%pip install uv\n!uv pip install -q unsloth wandb bitsandbytes ipywidgets nltk spacy huggingface_hub datasets --system\n# Finally, install a compatible xformers version\n!uv pip install -q xformers==0.0.27 --system  # This version is compatible with PyTorch 2.4.x\n!python -m xformers.info# # !python -m xformers.info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:41:36.042082Z","iopub.execute_input":"2024-11-22T06:41:36.042471Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.4.0\nCUDA available: True\nCUDA version: 12.3\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: uv in /opt/conda/lib/python3.10/site-packages (0.5.4)\nNote: you may need to restart the kernel to use updated packages.\n^C\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!uv pip install flash_attn --no-build-isolation -q --system\n!pip show flash_attn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone -b 10Nov2024v3  https://githomein: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@github.com/githomein/quantumLeap.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# # Set these environment variables before importing torch-related modules\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\nfrom pathlib import Path\n\ndef ensure_working_directory():\n    \"\"\"\n    Check if we're in the correct working directory, if not switch to it.\n    Creates the directory if it doesn't exist.\n    \"\"\"\n    target_dir = '/kaggle/working/quantumLeap'\n    current_dir = os.getcwd()\n    \n    # Print current directory\n    print(f\"Current directory: {current_dir}\")\n    \n    # Check if we need to switch directories\n    if current_dir != target_dir:\n        # Create directory if it doesn't exist\n        Path(target_dir).mkdir(parents=True, exist_ok=True)\n        \n        try:\n            # Change to target directory\n            os.chdir(target_dir)\n            print(f\"Successfully switched to: {target_dir}\")\n        except Exception as e:\n            print(f\"Error switching to directory: {str(e)}\")\n            raise\n    else:\n        print(\"Already in correct directory\")\n    \n    # Verify current directory\n    print(f\"Working directory: {os.getcwd()}\")\n\n# Call the function before your main code\nensure_working_directory()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------- #\n# Part 1.2: Import Necessary Libraries\n# ----------------------------- #\n\n# General Libraries\nimport os\nimport json\nimport sys\nimport subprocess\nimport argparse\nimport logging\nimport math\nimport random\nfrom datetime import datetime\nimport re\nimport gc\nimport weakref\nimport multiprocessing\n\n# Torch related\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\n\n# Transformers related\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    Adafactor\n)\n\n# Huggingface TRL for full finetune\nfrom trl import SFTTrainer, SFTConfig\n\n# General huggingface libraries\nimport huggingface_hub\nfrom datasets import load_dataset, Dataset\nfrom accelerate import Accelerator\n\n\n# Unsloth specificic libraries\nimport unsloth\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n\n# Other Libraries\nfrom peft import LoraConfig\nimport wandb\nimport nltk\nimport spacy\n# from galore_torch import GaLoreAdamW, GaLoreAdafactor, GaLoreAdamW8bit\n\n# Check and import NLTK and spacy modules\n# Ensure NLTK's punkt tokenizer is available\nimport nltk\ntry:\n    nltk.data.find('tokenizers/punkt')\n    print('punkt was already available.')\nexcept LookupError:\n    nltk.download('punkt')\n    print('punkt was not available. It has been downloaded')\n\n# Initialize spaCy English model\ntry:\n    nlp = spacy.load('en_core_web_sm')\n    print('en_core_web_sm was already available.')\nexcept OSError:\n    print(\"SpaCy English model not found. Downloading...\")\n    os.system('python -m spacy download en_core_web_sm')\n    nlp = spacy.load('en_core_web_sm')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:41:12.150051Z","iopub.execute_input":"2024-11-22T06:41:12.150449Z","iopub.status.idle":"2024-11-22T06:41:12.194449Z","shell.execute_reply.started":"2024-11-22T06:41:12.150419Z","shell.execute_reply":"2024-11-22T06:41:12.193435Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Unsloth specificic libraries\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bfloat16_supported\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'"],"ename":"ModuleNotFoundError","evalue":"No module named 'unsloth'","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:32:07.276954Z","iopub.execute_input":"2024-11-22T06:32:07.277356Z","iopub.status.idle":"2024-11-22T06:32:08.401779Z","shell.execute_reply.started":"2024-11-22T06:32:07.277323Z","shell.execute_reply":"2024-11-22T06:32:08.400873Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Fri Nov 22 06:32:08 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   37C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:32:10.019304Z","iopub.execute_input":"2024-11-22T06:32:10.019695Z","iopub.status.idle":"2024-11-22T06:32:11.066305Z","shell.execute_reply.started":"2024-11-22T06:32:10.019661Z","shell.execute_reply":"2024-11-22T06:32:11.065054Z"}},"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Wed_Nov_22_10:17:15_PST_2023\nCuda compilation tools, release 12.3, V12.3.107\nBuild cuda_12.3.r12.3/compiler.33567101_0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nprint(torch.__version__)          # Should reflect 2.5.0+cu124\nprint(torch.version.cuda)         # Should output 12.4\nprint(torch.cuda.is_available())  # Should return True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:38:30.298230Z","iopub.execute_input":"2024-11-22T06:38:30.298622Z","iopub.status.idle":"2024-11-22T06:38:30.304822Z","shell.execute_reply.started":"2024-11-22T06:38:30.298589Z","shell.execute_reply":"2024-11-22T06:38:30.303936Z"}},"outputs":[{"name":"stdout","text":"2.4.0\n12.3\nTrue\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Load and Clean the Text Data\n# ----------------------------- #\n\ndef load_and_clean_text(file_path):\n    \"\"\"\n    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        text = f.read()\n    # # Remove Project Gutenberg's license text and headers/footers\n    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n\n    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n    return text.strip()\n\n# Replace 'psychology_of_unconscious.txt' with your actual file pathrm -rfcd \nfile_path = '/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt'\nclean_text = load_and_clean_text(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:38:42.447055Z","iopub.execute_input":"2024-11-22T06:38:42.447737Z","iopub.status.idle":"2024-11-22T06:38:42.458294Z","shell.execute_reply.started":"2024-11-22T06:38:42.447703Z","shell.execute_reply":"2024-11-22T06:38:42.457558Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# # ----------------------------- #\n# # Part 3: Parse Text into Discourse Units\n# # ----------------------------- #\n\ndef parse_discourse_units(text, overwrite=False):\n    \"\"\"\n    Parses text into discourse units using spaCy.\n    Currently splits text into sentences.\n    \"\"\"\n    paragraphs = text.split('\\n\\n')\n    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n\n    discourse_units = []\n    for para in paragraphs:\n        doc = nlp(para)\n        sentences = [sent.text for sent in doc.sents]\n        discourse_units.extend(sentences)\n\n    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_discourse_units.json'\n    if not os.path.exists(output_path) or overwrite:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        with open(output_path, 'w') as f:\n            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n    else:\n        with open(output_path, 'r') as f:\n            discourse_units = json.load(f)\n\n    print(f\"Total Discourse Units: {len(discourse_units)}\")\n    return discourse_units","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:39:00.597041Z","iopub.execute_input":"2024-11-22T06:39:00.597387Z","iopub.status.idle":"2024-11-22T06:39:00.604239Z","shell.execute_reply.started":"2024-11-22T06:39:00.597359Z","shell.execute_reply":"2024-11-22T06:39:00.603355Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ----------------------------- #\n# Part 4: Create Chunks Using Hybrid Strategy\n# ----------------------------- #\n\ndef create_chunks(discourse_units, tokenizer, max_length=4096, overlap_size=1, overwrite=False):\n    \"\"\"\n    Creates chunks from discourse units using a sliding window with overlapping chunks.\n    Optimized to work directly with token IDs and utilize efficient list operations.\n    \"\"\"\n    chunks = []\n    current_chunk_tokens = []\n    current_length = 0\n\n    for unit in discourse_units:\n        unit_tokens = tokenizer.encode(unit, add_special_tokens=True)\n        unit_length = len(unit_tokens)\n\n        if current_length + unit_length <= max_length:\n            current_chunk_tokens.extend(unit_tokens)\n            current_length += unit_length\n        else:\n            # Decode and append the current chunk\n            chunk_text = tokenizer.decode(\n                current_chunk_tokens, skip_special_tokens=True)\n            chunks.append(chunk_text)\n\n            # Prepare overlap tokens\n            overlap_tokens = current_chunk_tokens[-overlap_size:]\n            current_chunk_tokens = overlap_tokens + unit_tokens\n            current_length = len(current_chunk_tokens)\n\n    # Append any remaining tokens as the last chunk\n    if current_chunk_tokens:\n        chunk_text = tokenizer.decode(\n            current_chunk_tokens, skip_special_tokens=True)\n        chunks.append(chunk_text)\n\n    # Write or read chunks as before\n    output_path = '/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious_chunks.json'\n    if not os.path.exists(output_path) or overwrite:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        with open(output_path, 'w') as f:\n            json.dump(discourse_units, f, ensure_ascii=False, indent=4)\n    else:\n        with open(output_path, 'r') as f:\n            discourse_units = json.load(f)\n\n    print(f\"Total Chunks Created: {len(chunks)}\")\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:39:05.001753Z","iopub.execute_input":"2024-11-22T06:39:05.002121Z","iopub.status.idle":"2024-11-22T06:39:05.009960Z","shell.execute_reply.started":"2024-11-22T06:39:05.002090Z","shell.execute_reply":"2024-11-22T06:39:05.009096Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Create and Tokenize Dataset\n# ----------------------------- #\n\n# To Do - make book titles and prompt generic so\ndef create_tokenized_dataset(chunks, tokenizer, max_length=1024, num_proc=2):\n\n    # Create a Dataset object from chunks\n\n    book_title = 'Psychology of the Unconscious by C. G. Jung'\n    wikipedia_prompt = \"\"\"\n    Psychology Book\n\n    ### Title: {}\n\n    ### Article: {}\n    \"\"\"\n\n    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\n    def formatting_prompts_func(examples):\n        titles = book_title\n        texts = examples[\"text\"]\n        outputs = []\n        for title, text in zip([book_title]*len(chunks), texts):\n            text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n            outputs.append(text)\n        return {\"text\": outputs, }\n    pass\n\n    # convert chunks variable to huggingface dataset\n\n    from datasets import Dataset\n\n    dataset = Dataset.from_dict({\"text\": chunks})\n\n    dataset = dataset.map(formatting_prompts_func,\n                          batched=True, num_proc=num_proc)\n    # Split the dataset into training and validation sets\n    split = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = split['train']\n    eval_dataset = split['test']\n\n    print(len(dataset))\n    # Find the maximum length of the text field in the entire dataset\n    max_length = max(len(text) for text in dataset['text'])\n    print(f\"The maximum length of the text field in the dataset is: {max_length} characters\")\n    print(f\"Training Dataset Size: {len(train_dataset)}\")\n#     print(f\"First 5 rows of training dataset: {train_dataset[:5]}\")\n    print(f\"Validation Dataset Size: {len(eval_dataset)}\")\n#     print(f\"First 5 rows of validation dataset: {eval_dataset[:5]}\")\n    return train_dataset, eval_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:39:10.779082Z","iopub.execute_input":"2024-11-22T06:39:10.779954Z","iopub.status.idle":"2024-11-22T06:39:10.786929Z","shell.execute_reply.started":"2024-11-22T06:39:10.779917Z","shell.execute_reply":"2024-11-22T06:39:10.786037Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# ----------------------------- #\n# Part 6: Set up environment and other important utilities\n# ----------------------------- #\n\ndef setup_environment():\n    \"\"\"\n    Initializes the Accelerator for distributed training.\n    \"\"\"\n    return Accelerator()\n\n\ndef get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n    \"\"\"\n    Defines a custom learning rate scheduler with warmup, constant, and linear annealing phases.\n    \"\"\"\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return current_step / num_warmup_steps  # Linear warmup\n        elif current_step < initial_phase_steps:\n            return 1.0  # Constant learning rate for initial phase\n        else:\n            # Linear annealing for the remaining steps\n            return 1.0 - ((current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps))\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\ndef setup_training_steps(initial_rows, annealing_rows, batch_size, gradient_accumulation_steps, num_epochs):\n    \"\"\"\n    Calculates total and initial training steps based on dataset size and training parameters.\n    \"\"\"\n    total_rows = initial_rows + annealing_rows\n    total_steps = (total_rows * num_epochs) // (batch_size *\n                                                gradient_accumulation_steps)\n    initial_steps = (initial_rows * num_epochs) // (batch_size *\n                                                    gradient_accumulation_steps)\n    return max(1, total_steps), max(1, initial_steps)\n\n\ndef print_memory_usage(step_desc):\n    \"\"\"\n    Prints the CUDA memory summary if CUDA is available.\n    \"\"\"\n    if torch.cuda.is_available():\n        print(f\"Memory Usage at {step_desc}:\")\n        print(torch.cuda.memory_summary())\n        print(\"\\n\")\n    else:\n        print(f\"No CUDA available at {step_desc}.\\n\")\n\n\ndef inference(model, tokenizer):\n    \"\"\"\n    Runs inference using the trained model.\n    \"\"\"\n    # Define sample prompts\n    prompts = [\n        \"Explain what is medical anthropology and its importance in elevating the quality of life?\",\n        \"Explain what are the types of Jungian archetypes and how they manifest in the human psyche?\"\n    ]\n\n    for prompt in prompts:\n        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_length=256)\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n        \n#  Login to Huggingface\nfrom huggingface_hub import login\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef setup_huggingface_access():\n    \"\"\"Setup Hugging Face access with token from environment or manual input\"\"\"\n    # First try to get token from environment variable\n    token = os.getenv('HUGGINGFACE_TOKEN')\n    \n    if not token:\n        # If not in environment, prompt for token\n        token = input(\"Enter your Hugging Face token: \")\n        \n    if token:\n        try:\n            login(token, add_to_git_credential=True)\n            print(\"Successfully logged in to Hugging Face!\")\n        except Exception as e:\n            print(f\"Failed to log in to Hugging Face: {str(e)}\")\n            return False\n    else:\n        print(\"No Hugging Face token provided\")\n        return False\n    \n    return True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:39:15.352715Z","iopub.execute_input":"2024-11-22T06:39:15.353081Z","iopub.status.idle":"2024-11-22T06:39:15.383747Z","shell.execute_reply.started":"2024-11-22T06:39:15.353050Z","shell.execute_reply":"2024-11-22T06:39:15.383132Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Load the Tokenizer and Model\n# ----------------------------- #\n\ndef load_model_and_tokenizer(base_model_slug, max_seq_length=4096, dtype=None, load_in_4bit=True, device_map = \"auto\"):\n    \"\"\"\n    Load and configure the model and tokenizer with specified parameters.\n    \n    Args:\n        base_model_slug (str): The model identifier from HuggingFace\n        max_seq_length (int): Maximum sequence length for the model\n        dtype: Data type for model parameters\n        load_in_4bit (bool): Whether to use 4-bit quantization\n        \n    Returns:\n        tuple: (model, tokenizer)\n    \"\"\"\n    # Check CUDA is available\n    import torch\n    if not torch.cuda.is_available():\n        print(\"WARNING: CUDA is not available. This might affect performance.\")\n    else:\n        print(\"CUDA available\")\n        \n    # Check available GPUs\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of available GPUs: {num_gpus}\")\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_properties(i).name}\")\n\n    # Determine optimal device map based on available GPUs\n    device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using single device: {device_map}\")\n\n    # Model paths\n    model_name = base_model_slug\n    models_dir = os.path.join(os.path.dirname(\"~/\"), \"models\")\n    model_path = os.path.join(models_dir, model_name)\n\n    # Create models directory if it doesn't exist\n    if not os.path.exists(models_dir):\n        os.makedirs(models_dir)\n\n    # Load or download model\n    try:\n        if os.path.exists(model_path):\n            print(f\"Loading model from local path: {model_path}\")\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name=base_model_slug,\n                max_seq_length=max_seq_length,\n                dtype=dtype,\n                load_in_4bit=load_in_4bit,\n                token=os.getenv('HUGGINGFACE_TOKEN'),\n            )\n        else:\n            print(f\"Downloading model from HuggingFace: {model_name}\")\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name=base_model_slug,\n                max_seq_length=max_seq_length,\n                dtype=dtype,\n                load_in_4bit=load_in_4bit,\n                token=os.getenv('HUGGINGFACE_TOKEN'),\n            )\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        raise\n\n    # Configure PEFT model\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=128,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n        target_modules=[\n            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n            \"gate_proj\", \"up_proj\", \"down_proj\",\n            \"embed_tokens\", \"lm_head\",  # Add for continual pretraining\n        ],\n        lora_alpha=32,\n        lora_dropout=0,  # Supports any, but = 0 is optimized\n        bias=\"none\",    # Supports any, but = \"none\" is optimized\n        use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n        random_state=3407,\n        use_rslora=True,   # We support rank stabilized LoRA\n        loftq_config=None,  # And LoftQ\n    )\n\n    print(\"Model and tokenizer loaded successfully!\")\n    return model, tokenizer\n\n    # Print model device distribution\n    if hasattr(model, 'hf_device_map'):\n        print(\"\\nModel Device Distribution:\")\n        for name, device in model.hf_device_map.items():\n            print(f\"{name}: {device}\")\n\n    # Print memory usage per GPU\n    print(\"\\nGPU Memory Usage after model loading:\")\n    for i in range(num_gpus):\n        memory_used = torch.cuda.memory_allocated(i) / (1024**3)\n        memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n        print(f\"GPU {i}: {memory_used:.2f}GB / {memory_total:.2f}GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:39:19.802403Z","iopub.execute_input":"2024-11-22T06:39:19.803110Z","iopub.status.idle":"2024-11-22T06:39:19.813184Z","shell.execute_reply.started":"2024-11-22T06:39:19.803074Z","shell.execute_reply":"2024-11-22T06:39:19.812443Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os\n\n# Set the environment variable\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\nos.environ[\"WANDB_API_KEY\"] = \"1ca3c5e9222c2504acbc07cf7f88267006ae68c4\"\n# Verify it's set correctly\nprint(os.getenv(\"HUGGINGFACE_TOKEN\"))\nprint(os.getenv(\"WANDB_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:39:26.435914Z","iopub.execute_input":"2024-11-22T06:39:26.436892Z","iopub.status.idle":"2024-11-22T06:39:26.442799Z","shell.execute_reply.started":"2024-11-22T06:39:26.436847Z","shell.execute_reply":"2024-11-22T06:39:26.441861Z"}},"outputs":[{"name":"stdout","text":"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\n1ca3c5e9222c2504acbc07cf7f88267006ae68c4\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Configure Environment Variables & Create Main Variables\n# ----------------------------- #\n\n# Unsloth modell initialization variables\nmax_seq_length = 256 # Choose any! We auto support RoPE Scaling internally!\nmax_length = max_seq_length\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nbase_model_slug = \"meta-llama/Llama-3.2-1B-Instruct\"\nbase_model_name = \"lora_model_pum\"\nchunks_max_length = max_seq_length\noverlap_size = 1\n# Define your parameters\nbatchSize = 2\nga = 16\nmaxSteps = 10\n# warmupSteps = 10\nnumTrainEpochs = 1\nlRate = 5e-5\nembLRate = 1e-5\noptim = \"adamw_8bit\"\nlrSchedule = \"linear\"\ndataset_slug = \"psychology_of_unconscious\"\n\nfrom datetime import datetime\nimport pytz\nimport wandb\n# Get the current date and time in Indian Standard Time (IST)\nist = pytz.timezone('Asia/Kolkata')\ncurrent_datetime = datetime.now(ist)\n\n# Format the datetime string\n# Example format: 20240428_153045 (YYYYMMDD_HHMMSS)\nformatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n\n# Define Run Name\nrun_name = f\"\"\"Kaggle-quantumLeap-{formatted_datetime}-{base_model_slug}-{dataset_slug}-{max_seq_length}_maxSeqLength-{max_length}_maxLength-{batchSize}_batchSize-{ga}_ga-{maxSteps}_maxSteps-{numTrainEpochs}_numTrainEpochs-{lRate}_lRate-{embLRate}_embLRate-{optim}_optim-{lrSchedule}_lrSchedule\"\"\"\n\n# Initialize Weights & Biases\n# It's recommended to set your W&B API key as an environment variable for security.\nwandb.login(key=os.getenv(\"WANDB_API_KEY\"))\nwandb.init(project=\"KAGGLE-quantumLeap\", name=run_name)\n\n# Set environment variables before importing torch-related modules\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:39:30.962799Z","iopub.execute_input":"2024-11-22T06:39:30.963658Z","iopub.status.idle":"2024-11-22T06:39:34.883822Z","shell.execute_reply.started":"2024-11-22T06:39:30.963623Z","shell.execute_reply":"2024-11-22T06:39:34.883119Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molabs-asia\u001b[0m (\u001b[33molabs-asia-olabs-pro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112811099999615, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29a6fece2ded4393ac6a5971101061f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/quantumLeap/wandb/run-20241122_063932-suoaquz3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap/runs/suoaquz3' target=\"_blank\">Kaggle-quantumLeap-20241122_120930-meta-llama/Llama-3.2-1B-Instruct-psychology_of_unconscious-256_maxSeqLength-256_maxLength-2_batchSize-16_ga-10_maxSteps-1_numTrainEpochs-5e-05_lRate-1e-05_embLRate-adamw_8bit_optim-linear_lrSchedule</a></strong> to <a href='https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap/runs/suoaquz3' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/KAGGLE-quantumLeap/runs/suoaquz3</a>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"\n# ----------------------------- #\n# Part 9: Data Processing\n# ----------------------------- #\n\n# # Perform Inference Before Training\n# inference(model, tokenizer)\n\n# Set number of processes to use for data loading\nnum_cpus = multiprocessing.cpu_count()\nnum_proc = max(num_cpus-2, 2)  # Adjust based on prior recommendations\nprint(f\"Number of CPU cores: {num_cpus}\")\nprint(f\"Number of processes: {num_proc}\")\n\n# Login to Hugging Face\nif not setup_huggingface_access():\n    raise Exception(\"Failed to setup Hugging Face access. Please check your token.\")\n\n# Load Model and Tokenizer\nmodel, tokenizer = load_model_and_tokenizer(base_model_slug)\nprint(f\"Model Device: {model.device}\")\n\n# Load and Clean Text Data\nfile_path = \"/kaggle/working/quantumLeap/data/input/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/psychology_of_unconscious.txt\"\nclean_text = load_and_clean_text(file_path)\n\n# Parse Discourse Units\ndiscourse_units = parse_discourse_units(clean_text, overwrite=True)\n\n# Create Chunks\nchunks = create_chunks(\n    discourse_units,\n    tokenizer,\n    max_length=max_length,\n    overlap_size=overlap_size,\n    overwrite=True,\n)\n\n# Create Tokenized Dataset\ntrain_dataset, eval_dataset = create_tokenized_dataset(\n    chunks, tokenizer, max_length)\n\n# Save datasets as Hugging Face `datasets`\ntrain_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\neval_dataset.save_to_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')\n\n### To Do - Make the below as dynamic and as a functio\n# # Uncomment following if you want to just load the data from temp directory\n# from datasets import load_from_disk\n\n# train_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/train_dataset')\n# eval_dataset = load_from_disk('/kaggle/working/quantumLeap/data/preprocess/Step_2_Classic_Texts_and_Ethnographies/2.1_Public_Domain_Books/Project_Gutenberg/eval_dataset')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:39:48.285875Z","iopub.execute_input":"2024-11-22T06:39:48.286246Z","iopub.status.idle":"2024-11-22T06:39:48.632942Z","shell.execute_reply.started":"2024-11-22T06:39:48.286215Z","shell.execute_reply":"2024-11-22T06:39:48.631835Z"}},"outputs":[{"name":"stdout","text":"Number of CPU cores: 4\nNumber of processes: 2\nToken is valid (permission: fineGrained).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nSuccessfully logged in to Hugging Face!\nCUDA available\nNumber of available GPUs: 1\nGPU 0: Tesla T4\nUsing single device: cuda:0\nDownloading model from HuggingFace: meta-llama/Llama-3.2-1B-Instruct\nError loading model: name 'FastLanguageModel' is not defined\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to setup Hugging Face access. Please check your token.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load Model and Tokenizer\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_slug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Load and Clean Text Data\u001b[39;00m\n","Cell \u001b[0;32mIn[22], line 57\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(base_model_slug, max_seq_length, dtype, load_in_4bit, device_map)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading model from HuggingFace: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m         model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     58\u001b[0m             model_name\u001b[38;5;241m=\u001b[39mbase_model_slug,\n\u001b[1;32m     59\u001b[0m             max_seq_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[1;32m     60\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m     61\u001b[0m             load_in_4bit\u001b[38;5;241m=\u001b[39mload_in_4bit,\n\u001b[1;32m     62\u001b[0m             token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHUGGINGFACE_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'FastLanguageModel' is not defined"],"ename":"NameError","evalue":"name 'FastLanguageModel' is not defined","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}