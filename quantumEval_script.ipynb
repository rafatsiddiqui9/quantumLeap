{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import subprocess\n",
    "import signal\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Configure logging\n",
    "ist = pytz.timezone('Asia/Kolkata')\n",
    "ist_time = datetime.now(ist)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'llm_evaluation_{ist_time.strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Constants\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODELS = [\n",
    "    # \"hf.co/bartowski/Qwen2.5-7B-Instruct-GGUF:Q5_K_L\",\n",
    "    # \"hf.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF:Q5_K_L\",\n",
    "    # \"hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:F16\",\n",
    "    # \"hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:F16\",\n",
    "    # \"hf.co/bartowski/Qwen2.5-14B-Instruct-GGUF:Q2_K\",\n",
    "    # \"hf.co/bartowski/Qwen2.5-3B-Instruct-GGUF:F16\",\n",
    "    # \"hf.co/bartowski/Qwen2.5-1.5B-Instruct-GGUF:F16\",\n",
    "    \"hf.co/bartowski/Phi-3-medium-128k-instruct-GGUF:Q3_K_S\",\n",
    "    \"hf.co/RichardErkhov/princeton-nlp_-_gemma-2-9b-it-SimPO-gguf:Q4_K_M\",\n",
    "]\n",
    "\n",
    "def load_questions() -> pd.DataFrame:\n",
    "    \"\"\"Load questions from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('/home/ubuntu/quantumLeap/quantumEval_claude.csv')\n",
    "        logging.info(f\"Successfully loaded {len(df)} questions from CSV\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading questions: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def start_ollama_model(model_name: str) -> subprocess.Popen:\n",
    "    \"\"\"Start an Ollama model.\"\"\"\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            ['ollama', 'run', model_name],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        time.sleep(10)  # Wait for model to initialize\n",
    "        logging.info(f\"Started model: {model_name}\")\n",
    "        return process\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error starting model {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def stop_ollama_model(process: subprocess.Popen):\n",
    "    \"\"\"Stop the Ollama model process.\"\"\"\n",
    "    if process:\n",
    "        process.send_signal(signal.SIGTERM)\n",
    "        process.wait()\n",
    "        logging.info(\"Stopped Ollama model\")\n",
    "\n",
    "def query_model(question: str, model_name: str) -> str:\n",
    "    \"\"\"Query the model and get response.\"\"\"\n",
    "    prompt = f\"Please answer the following question about Clotaire Rapaille's Culture Code methodology:\\n\\n{question}\\n\\nProvide a clear and concise answer based on Rapaille's work.\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error querying model {model_name}: {e}\")\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "def evaluate_models(questions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate all models on all questions.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model in MODELS:\n",
    "        logging.info(f\"Starting evaluation for model: {model}\")\n",
    "        \n",
    "        # Start the model\n",
    "        process = start_ollama_model(model)\n",
    "        if not process:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            for _, row in questions_df.iterrows():\n",
    "                logging.info(f\"Processing Question {row['Question Number']}\")\n",
    "                \n",
    "                response = query_model(row['Question Text'], model)\n",
    "                \n",
    "                result = {\n",
    "                    'Question Number': row['Question Number'],\n",
    "                    'Attribute': row['Attribute'],\n",
    "                    'Expertise Level': row['Expertise Level'],\n",
    "                    'Question Text': row['Question Text'],\n",
    "                    'LLM Name': model,\n",
    "                    'Model Response': response,\n",
    "                    'Score': '',  # To be filled later\n",
    "                    'Explanation': ''  # To be filled later\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                # Save intermediate results with IST timestamp\n",
    "                ist_time = datetime.now(ist)\n",
    "                pd.DataFrame(results).to_csv(\n",
    "                    f'/home/ubuntu/quantumLeap/data/eval_for_base_model_selection/temp/results_{ist_time.strftime(\"%Y%m%d_%H%M%S\")}.csv',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                time.sleep(2)  # Brief pause between questions\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during evaluation of model {model}: {e}\")\n",
    "        finally:\n",
    "            stop_ollama_model(process)\n",
    "            time.sleep(5)  # Wait before starting next model\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        # Load questions\n",
    "        questions_df = load_questions()\n",
    "        \n",
    "        # Run evaluations\n",
    "        results_df = evaluate_models(questions_df)\n",
    "        \n",
    "        # Save final results with IST timestamp\n",
    "        ist_time = datetime.now(ist)\n",
    "        output_file = f'/home/ubuntu/quantumLeap/data/eval_for_base_model_selection/final/final_results_{ist_time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Results saved to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for generating text with Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "class OllamaAPIClient:\n",
    "    \"\"\"\n",
    "    A client for interacting with the Ollama API with proper error handling and response parsing.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\"):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        })\n",
    "        \n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    def generate_text(self, \n",
    "                     model: str, \n",
    "                     prompt: str, \n",
    "                     stream: bool = True,  # Changed default to True for streaming\n",
    "                     **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate text using the specified model.\n",
    "        \n",
    "        Args:\n",
    "            model: Name of the model to use\n",
    "            prompt: Input prompt for generation\n",
    "            stream: Whether to stream the response\n",
    "            **kwargs: Additional parameters to pass to the API\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing the response from the API\n",
    "        \"\"\"\n",
    "        endpoint = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(endpoint, json=payload, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Always use streaming for better token handling\n",
    "            return self._handle_streaming_response(response)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"API request failed: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _handle_streaming_response(self, response: requests.Response) -> Dict[str, Any]:\n",
    "        \"\"\"Handle streaming response from the API with improved token formatting.\"\"\"\n",
    "        combined_response = \"\"\n",
    "        current_line = \"\"\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    json_response = json.loads(line)\n",
    "                    if \"response\" in json_response:\n",
    "                        token = json_response[\"response\"]\n",
    "                        current_line += token\n",
    "                        # Print the token without newline and flush immediately\n",
    "                        print(token, end='', flush=True)\n",
    "                        combined_response += token\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    self.logger.warning(f\"Failed to parse streaming JSON: {e}\")\n",
    "                    continue\n",
    "                \n",
    "        # Print final newline\n",
    "        print()\n",
    "        return {\"response\": combined_response}\n",
    "\n",
    "def main():\n",
    "    client = OllamaAPIClient()\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    Envision a groundbreaking theory that unites Dr. Clotaire Rapaille's 'The Culture Code' \n",
    "    with advanced artificial intelligence and machine learning technologies to predict and \n",
    "    influence consumer behavior in the era of globalization and cultural hybridization.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nGenerating response...\\n\")\n",
    "        result = client.generate_text(\n",
    "            # model=\"hf.co/RichardErkhov/Qwen_-_Qwen2.5-72B-Instruct-gguf:Q3_K\",\n",
    "            model=\"hf.co/bartowski/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF:Q3_K_XL\",\n",
    "            prompt=prompt\n",
    "        )\n",
    "        # No need to print the result again since it's already printed during streaming\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
