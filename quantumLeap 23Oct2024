{"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import subprocess\n\n# Install required packages in terminal only. Otherwise jupyter kernel will die and remote server will crash\npackages = [\n    \"unsloth\",\n    \"xformers\",\n    \"torch\",\n    \"nltk\",\n    \"spacy\",\n    \"wandb\"cod,\n    \"datasets\",\n    \"huggingface_hub\"\n]\n\nfor package in packages:\n    subprocess.run([\"pip\", \"install\", \"-q\", \"-U\", package, \"--no-cache-dir\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -q -U  unsloth wandb bitsandbytes torch ipywidgets xformers nltk spacy huggingface_hub datasets\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport torch\nimport nltk\nimport spacy\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom datasets import Dataset\nimport logging\nimport argparse\nimport wandb  # Weights & Biases integration\n","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 1: Install and Setup Libraries\n# ----------------------------- #\n\n# Ensure NLTK's punkt tokenizer is available\nnltk.download('punkt')\n\n# Initialize spaCy English model\ntry:\n    nlp = spacy.load('en_core_web_sm')\nexcept OSError:\n    print(\"SpaCy English model not found. Downloading...\")\n    os.system('python -m spacy download en_core_web_sm')\n    nlp = spacy.load('en_core_web_sm')\n","metadata":{},"execution_count":2,"outputs":[{"name":"stderr","output_type":"stream","text":"[nltk_data] Downloading package punkt to /root/nltk_data...\n\n[nltk_data]   Package punkt is already up-to-date!\n"}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 2: Load and Clean the Text Data\n# ----------------------------- #\n\ndef load_and_clean_text(file_path):\n    \"\"\"\n    Loads text from a file and removes Project Gutenberg's license and headers/footers.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        text = f.read()\n    # # Remove Project Gutenberg's license text and headers/footers\n    # start_pattern = r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n    # end_pattern = r'\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*\\*\\*\\*'\n\n    # text = re.sub(f'.*{start_pattern}', '', text, flags=re.DOTALL)\n    # text = re.sub(f'{end_pattern}.*', '', text, flags=re.DOTALL)\n    return text.strip()\n\n# Replace 'psychology_of_unconscious.txt' with your actual file path\nfile_path = '/root/quantumLeap/data/psychologoy-of-unconscious-mind/psychology_of_unconscious.txt'\nclean_text = load_and_clean_text(file_path)","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 3: Parse Text into Discourse Units\n# ----------------------------- #\n\ndef parse_discourse_units(text):\n    \"\"\"\n    Parses text into discourse units using spaCy.\n    Currently splits text into sentences.\n    \"\"\"\n    paragraphs = text.split('\\n\\n')\n    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n    \n    discourse_units = []\n    for para in paragraphs:\n        doc = nlp(para)\n        sentences = [sent.text for sent in doc.sents]\n        discourse_units.extend(sentences)\n    return discourse_units\n\ndiscourse_units = parse_discourse_units(clean_text)\n\n# Save discourse_units to a file (Optional)\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.txt', 'w') as f:\n    for unit in discourse_units:\n        f.write(unit + '\\n')\n\n# If you need to reload from file (Optional)\n# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.txt', 'r') as f:\n#     discourse_units = f.read().splitlines()\n\nlen(discourse_units)","metadata":{},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":["6175"]},"metadata":{}}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 4: Create Chunks Using Hybrid Strategy\n# ----------------------------- #\n\ndef create_chunks(discourse_units, tokenizer, max_length=2048, overlap_size=100):\n    \"\"\"\n    Creates chunks from discourse units using a sliding window with overlapping chunks.\n    \"\"\"\n    chunks = []\n    current_chunk = []\n    current_length = 0\n\n    for unit in discourse_units:\n        unit_tokens = tokenizer.encode(unit, add_special_tokens=False)\n        unit_length = len(unit_tokens)\n\n        if current_length + unit_length <= max_length:\n            current_chunk.append(unit)\n            current_length += unit_length\n        else:\n            # Append the current chunk\n            chunks.append(' '.join(current_chunk))\n            # Create overlap\n            overlap_text = ' '.join(current_chunk)[-overlap_size:]\n            overlap_tokens = tokenizer.encode(overlap_text, add_special_tokens=False)\n            overlap_text = tokenizer.decode(overlap_tokens, skip_special_tokens=True)\n            # Start new chunk with overlap and current unit\n            current_chunk = [overlap_text, unit]\n            current_length = len(tokenizer.encode(overlap_text, add_special_tokens=False)) + unit_length\n\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n\n    return chunks","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 5: Load the Tokenizer and Model\n# ----------------------------- #\nimport os\nimport re\nimport torch\nimport nltk\nimport spacy\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom datasets import Dataset\nimport logging\nimport argparse\nimport wandb  # Weights & Biases integration\n\n\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# if the model is already downloaded, then don't download it again; otherwise download it\nimport os\n\nmodel_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\nmodels_dir = os.path.join(os.path.dirname(os.getcwd()), \"models\")\nmodel_path = os.path.join(models_dir, model_name)\n\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n\nif os.path.exists(model_path):\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_path,\n        max_seq_length=max_seq_length,\n        dtype=dtype,\n        load_in_4bit=load_in_4bit,\n    )\nelse:\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name,\n        max_seq_length=max_seq_length,\n        dtype=dtype,\n        load_in_4bit=load_in_4bit,\n        token=\"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\",\n    )\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\n\n                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n    lora_alpha = 32,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = True,   # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n\n==((====))==  Unsloth 2024.9: Fast Llama patching. Transformers = 4.44.2.\n\n   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.209 GB. Platform = Linux.\n\nO^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.\n\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d795d27ab234417eabbdc5297e23b1f0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n\n<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n\n<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n\n<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n\n<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n\n<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n"},{"name":"stdout","output_type":"stream","text":"Unsloth: Offloading input_embeddings to disk to save VRAM\n"},{"name":"stderr","output_type":"stream","text":"/root/miniconda/envs/olabs/lib/python3.12/site-packages/unsloth/models/_utils.py:866: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"},{"name":"stdout","output_type":"stream","text":"Unsloth: Offloading output_embeddings to disk to save VRAM\n"},{"name":"stderr","output_type":"stream","text":"Unsloth 2024.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"},{"name":"stdout","output_type":"stream","text":"Unsloth: Casting embed_tokens to float32\n\nUnsloth: Casting lm_head to float32\n"}]},{"cell_type":"code","source":"\n# ----------------------------- #\n# Part 6: Create Chunks (After Tokenizer is Loaded)\n# ----------------------------- #\n# If you need to reload from file (Optional)\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/discourse_units_final.txt', 'r') as f:\n    discourse_units = f.read().splitlines()\n\nchunks = create_chunks(discourse_units, tokenizer, max_length=2048, overlap_size=100)\n\n# Save chunks to a file (Optional)\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.txt', 'w') as f:\n    for unit in chunks:\n        f.write(unit + '\\n')\n\n# If you need to reload from file (Optional)\n# with open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/chunks_final.txt', 'r') as f:\n#     discourse_units = f.read().splitlines()\n\nlen(chunks)","metadata":{},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":["93"]},"metadata":{}}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 7: Create and Tokenize Dataset\n# ----------------------------- #\n\n# Create a Dataset object from chunks\n\nbook_title = 'Psychology of the Unconscious by C. G. Jung'\nwikipedia_prompt = \"\"\"\nPsychology Book\n\n### Title: {}\n\n### Article: {}\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    titles = book_title\n    texts  = examples[\"text\"]\n    outputs = []\n    for title, text in zip([book_title]*len(chunks), texts):\n        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n        outputs.append(text)\n    return { \"text\" : outputs, }\npass\n\n# convert chunks variable to huggingface dataset\n\nfrom datasets import Dataset\n\ndataset = Dataset.from_dict({\"text\": chunks})\n\n# dataset = dataset.train_test_split(test_size = 0.1)[\"train\"]\n\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n\nlen(dataset)\n","metadata":{},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8031ddd1c1b46929b05ae833a3805e5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/93 [00:00<?, ? examples/s]"]},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":["93"]},"metadata":{}}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 8: Configure Training Arguments\n# ----------------------------- #\n\n\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Use warmup_ratio and num_train_epochs for longer runs!\n        max_steps = 120,\n        warmup_steps = 10,\n        # warmup_ratio = 0.1,\n        # num_train_epochs = 1,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = 5e-5,\n        embedding_learning_rate = 1e-5,\n\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2713c51375d4ab9b89def45648213c5","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=2):   0%|          | 0/83 [00:00<?, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"max_steps is given, it will override any value given in num_train_epochs\n"}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 9: Define Compute Metrics Function\n# ----------------------------- #\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Computes perplexity based on model predictions and labels.\n    \"\"\"\n    logits, labels = eval_pred\n    # Convert to torch tensors\n    logits = torch.tensor(logits)\n    labels = torch.tensor(labels)\n    \n    # Ensure shapes match\n    if logits.shape[:2] != labels.shape:\n        raise ValueError(f\"Logits shape {logits.shape} does not match labels shape {labels.shape}\")\n    \n    # Shift logits and labels\n    shift_logits = logits[:, :-1, :].contiguous()\n    shift_labels = labels[:, 1:].contiguous()\n\n    # Check label values\n    if shift_labels.max() >= model.config.vocab_size:\n        raise ValueError(f\"Label value {shift_labels.max()} exceeds vocab size {model.config.vocab_size}\")\n    \n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    perplexity = torch.exp(loss).item()\n    return {\"perplexity\": perplexity}","metadata":{},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\nclass EndOfTextCriteria(StoppingCriteria):\n    def __init__(self, eos_token_id):\n        self.eos_token_id = eos_token_id\n\n    def __call__(self, input_ids, scores, **kwargs):\n        return input_ids[0][-1] == self.eos_token_id\n\nstopping_criteria = StoppingCriteriaList([EndOfTextCriteria(tokenizer.eos_token_id)])\n\noutputs = model.generate(**inputs, \n                         max_new_tokens=64, \n                         stopping_criteria=stopping_criteria,\n                         use_cache=True)\n\n# outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\nprint(tokenizer.batch_decode(outputs))\n\n# Text Streaming\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n\n# inputs = tokenizer(\n# [\n#     instruction_prompt.format(\n#         \"Hero Archetype\", # concept_name\n#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n#         \"\", # output - leave this blank for generation!\n#     )\n# ], return_tensors = \"pt\").to(\"cuda\")\n\n\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n#                    repetition_penalty = 0.1)","metadata":{},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"[\"<|begin_of_text|>Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\\n\\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\\nconcept_name: Hero Archetype\\ndetailed_explanation: The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\\n\\n### Response:\\nExample Scenario: The hero archetype can be seen in the story of Odysseus from Homer's epic poem, The Odyssey. Odysseus, the king of Ithaca, is a quintessential hero who embodies the qualities of bravery, resilience, and determination. His journey, which spans over a decade,\"]\n"}]},{"cell_type":"code","source":"# ----------------------------- #\n# Part 11: Start Training\n# ----------------------------- #\n\n#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n","metadata":{},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"GPU = NVIDIA H100 80GB HBM3. Max memory = 79.209 GB.\n\n10.746 GB of memory reserved.\n"},{"name":"stderr","output_type":"stream","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n\n   \\\\   /|    Num examples = 83 | Num Epochs = 24\n\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n\n\\        /    Total batch size = 16 | Total steps = 120\n\n \"-____-\"     Number of trainable parameters = 1,386,217,472\n"},{"name":"stdout","output_type":"stream","text":"Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n\nUnsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"},{"name":"stderr","output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molabs-asia\u001b[0m (\u001b[33molabs-asia-olabs-pro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"},{"output_type":"display_data","data":{"text/html":["Tracking run with wandb version 0.18.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Run data is saved locally in <code>/root/quantumLeap/wandb/run-20240918_124030-4o9wvxkp</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface/runs/4o9wvxkp' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View project at <a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View run at <a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface/runs/4o9wvxkp' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/huggingface/runs/4o9wvxkp</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [120/120 06:33, Epoch 22/24]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.256900</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.078900</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.297000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>3.065800</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>3.244600</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>3.174400</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>3.108100</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>3.250900</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>3.031800</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>3.348400</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>3.253700</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>3.135900</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>3.045100</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>3.174700</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>3.395800</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>3.007400</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>3.118200</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>3.242400</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>3.371000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>3.053500</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>2.987000</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>3.174200</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>3.336200</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>3.230400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>2.918400</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>3.108200</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>3.209300</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>3.074200</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>3.086400</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>3.181900</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>3.048900</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>3.105400</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>3.181600</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>3.043000</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>3.107900</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>3.366800</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>3.000700</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>3.150700</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>3.157800</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>3.348800</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>3.004600</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>3.130100</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>3.047900</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>3.292300</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>3.245100</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>3.085100</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>3.030700</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>3.388000</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>3.279900</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>3.145300</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>3.070200</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>3.095500</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>3.015800</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>3.125400</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>3.376600</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>3.138900</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>3.059200</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>3.366000</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>2.960300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>3.266600</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>3.028900</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>3.144800</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>3.211200</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>3.090600</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>3.103900</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>3.037900</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>3.224500</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>3.225300</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>3.145500</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>3.006700</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>3.192500</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>3.055400</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>3.170000</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>3.197800</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>2.991400</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>3.037300</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>3.030100</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>3.255500</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>3.055000</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>3.107100</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>3.019900</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>3.394000</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>3.167600</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>3.165000</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>3.097300</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>3.142100</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>3.301500</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>3.205800</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>3.038400</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>3.154400</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>3.078000</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>3.299100</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>3.125600</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>3.052100</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>3.058600</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>3.263700</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>3.177800</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>3.256200</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>3.094100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>3.210200</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>3.053500</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>3.134100</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>3.338900</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>3.016100</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>3.148200</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>3.026300</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>3.274300</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>3.121500</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>3.034000</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>3.149800</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>3.281700</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>3.115500</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>3.383800</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>3.024700</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>3.138000</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>2.951200</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>3.123000</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>3.185500</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>3.092500</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>3.134000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"398.6807 seconds used for training.\n\n6.64 minutes used for training.\n\nPeak reserved memory = 38.768 GB.\n\nPeak reserved memory for training = 28.022 GB.\n\nPeak reserved memory % of max memory = 48.944 %.\n\nPeak reserved memory for training % of max memory = 35.377 %.\n"}]},{"cell_type":"code","source":"import wandb\nfrom pprint import pprint\n\ndef get_run_config(project_name, run_id):\n    try:\n        # Initialize the wandb API\n        api = wandb.Api()\n\n        # Access the specific run\n        run = api.run(f\"{project_name}/{run_id}\")\n\n        # Get the full configuration\n        config = run.config\n\n        # Filter for trainer-specific configuration\n        trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n\n        return trainer_config\n\n    except wandb.errors.CommError:\n        print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None\n\n# Usage\nproject_name = \"olabs-asia-olabs-pro/huggingface\"\nrun_id = \"ppqtwwmy\"\n\ntrainer_config = get_run_config(project_name, run_id)\n\nif trainer_config:\n    print(f\"Trainer configuration for run {run_id}:\")a\n    pprint(trainer_config)","metadata":{},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":"Trainer configuration for run ppqtwwmy:\n\n{'bf16': False,\n\n 'bf16_full_eval': False,\n\n 'fp16': True,\n\n 'fp16_backend': 'auto',\n\n 'fp16_full_eval': False,\n\n 'fp16_opt_level': 'O1',\n\n 'gradient_accumulation_steps': 8,\n\n 'gradient_checkpointing': False,\n\n 'gradient_checkpointing_kwargs': None,\n\n 'learning_rate': 5e-05,\n\n 'max_steps': 120,\n\n 'num_train_epochs': 3,\n\n 'optim': 'adamw_8bit',\n\n 'optim_args': None,\n\n 'optim_target_modules': None,\n\n 'per_device_eval_batch_size': 8,\n\n 'per_device_train_batch_size': 2,\n\n 'warmup_ratio': 0,\n\n 'warmup_steps': 10,\n\n 'weight_decay': 0.01}\n"}]},{"cell_type":"code","source":"import wandb\nfrom pprint import pprint\n\ndef get_run_config(project_name, run_id):\n    try:\n        # Initialize the wandb API\n        api = wandb.Api()\n\n        # Access the specific run\n        run = api.run(f\"{project_name}/{run_id}\")\n\n        # Get the full configuration\n        config = run.config\n\n        # Filter for trainer-specific configuration\n        trainer_config = {k: v for k, v in config.items() if k.startswith(('train', 'learning', 'optim', 'fp16', 'bf16', 'gradient', 'weight_decay', 'warmup', 'max_steps', 'num_train_epochs', 'per_device'))}\n\n        return trainer_config\n\n    except wandb.errors.CommError:\n        print(f\"Error: Unable to access run {run_id}. Make sure the run ID is correct and you have the necessary permissions.\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None\n\n# Usage\nproject_name = \"olabs-asia-olabs-pro/huggingface\"\nrun_id = \"i7lm3hcl\"\n\ntrainer_config = get_run_config(project_name, run_id)\n\nif trainer_config:\n    print(f\"Trainer configuration for run {run_id}:\")\n    pprint(trainer_config)","metadata":{},"execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"Trainer configuration for run i7lm3hcl:\n\n{'bf16': True,\n\n 'bf16_full_eval': False,\n\n 'fp16': False,\n\n 'fp16_backend': 'auto',\n\n 'fp16_full_eval': False,\n\n 'fp16_opt_level': 'O1',\n\n 'gradient_accumulation_steps': 8,\n\n 'gradient_checkpointing': False,\n\n 'gradient_checkpointing_kwargs': None,\n\n 'learning_rate': 5e-05,\n\n 'max_steps': 120,\n\n 'num_train_epochs': 3,\n\n 'optim': 'adamw_8bit',\n\n 'optim_args': None,\n\n 'optim_target_modules': None,\n\n 'per_device_eval_batch_size': 8,\n\n 'per_device_train_batch_size': 2,\n\n 'warmup_ratio': 0,\n\n 'warmup_steps': 10,\n\n 'weight_decay': 0.01}\n"}]},{"cell_type":"markdown","source":"# The Loss from earlier training was too high. We shall use training arguments from unsloth colab notebook \"Llama-3.1 8b + Unsloth 2x faster finetuning.ipynb\". URL below\n### https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=95_Nn-89DhsL","metadata":{}},{"cell_type":"code","source":"print(dataset[0])","metadata":{},"execution_count":32,"outputs":[{"name":"stdout","output_type":"stream","text":"{'text': '\\nPsychology Book\\n\\n### Title: Psychology of the Unconscious by C. G. Jung\\n\\n### Article: Psychology of the Unconscious by Jung AN INTRODUCTION TO PSYCHOANALYSIS AND ANALYTIC PSYCHOLOGY When Professor Freud of Vienna made his early discoveries in the realm of the neuroses, and announced that the basis and origin of the various symptoms grouped under the terms hysteria and neuroses lay in unfulfilled desires and wishes, unexpressed and unknown to the patient for the most part, and concerned chiefly with the sexual instinct, it was not realized what far-reaching influence this unpopular and bitterly attacked theory would exert on the understanding of human life in general. For this theory has so widened in its scope that its application has now extended beyond a particular group of pathologic states. It has in fact led to a new evaluation of the whole conduct of human life; a new comprehension has developed which explains those things which formerly were unexplained, and there is offered an understanding not only of the symptoms of a neurosis and the phenomena of conduct but the product of the mind as expressed in myths and religions. This amazing growth has proceeded steadily in an ever-widening fashion despite opposition as violent as any of which we have knowledge in the past. The criticism originally directed towards the little understood and much disliked sexual conception now includes the further teachings of a psychology which by the application to it of such damning phrases as mystical, metaphysical and sacrilegious, is condemned as unscientific. To add to the general confusion and misunderstanding surrounding this new school of thought there has arisen a division amongst the leaders themselves, so that there now exist two schools led respectively by Professor Sigmund Freud of Vienna and Dr. Carl Jung of Zurich, referred to in the literature as the Vienna School and the Zurich School. It is very easy to understand that criticism and opposition should develop against a psychology so difficult of comprehension, and so disturbing to the ideas which have been held by humanity for ages; a psychology which furthermore requires a special technique as well as an observer trained to recognize and appreciate in psychologic phenomena a verification of the statement that there is no such thing as chance, and that every act and every expression has its own meaning, determined by the inner feelings and wishes of the individual. It is not a simple matter to come out boldly and state that every individual is to a large extent the determiner of his own destiny, for only by poets and philosophers has this idea been put forth—not by science; and it is a brave act to make this statement with full consciousness of all its meaning, and to stand ready to prove it by scientific reasoning and procedure. Developed entirely through empirical investigation and through an analysis of individual cases, Freudian psychology seems particularly to belong to that conception of Max Müller’s that “An empirical acquaintance with facts rises to a scientific knowledge of facts as soon as the mind discovers beneath the multiplicity of single productions the unity of an organic system. ”[1] Psychoanalysis is the name given to the method developed for reaching down into the hidden depths of the individual to bring to light the underlying motives and determinants of his symptoms and attitudes, and to reveal the unconscious tendencies which lie behind actions and reactions and which influence development and determine the relations of life itself. The result of digging down into the hidden psyche has been to produce a mass of material from below the threshold of consciousness, so astonishing and disturbing and out of relation with the previously held values, as to arouse in any one unfamiliar with the process the strongest antagonism and criticism. Although originally studied only as a therapeutic method for the sick it was soon realized through an analysis of normal people how slight were the differences in the content of the unconscious of the sick and of the normal. The differences observed were seen to be rather in the reactions to life and to the conflicts produced by contending forces in the individual. These conflicts, usually not fully perceived by the individual, and having to do with objectionable desires and wishes that are not in keeping with the conscious idea of self, produce marked effects which are expressed either in certain opinions, prejudices, attitudes of conduct, faulty actions, or in some definite pathologic symptom. As Dr. Jung says, he who remains healthy has to struggle with the same complexes that cause the neurotic to fall ill. In a valuable book called “The Neighbor,” written by the late Professor N. Shaler of Harvard University, there occurs this very far-reaching statement: “It is hardly too much to say that all the important errors of conduct, all the burdens of men or of societies are caused by the inadequacies in the association of the primal animal emotions with those mental powers which have been so rapidly developed in mankind.” This statement, reached by a process of reasoning and a method of thought and study entirely different from psychoanalysis, nevertheless so completely expresses in brief form the very basis of the postulates developed through psychoanalysis that I quote it here. Such a statement made in the course of a general examination of human relations does not arouse opposition nor seem to be so difficult of acceptance. It appears to be the individual application of these conceptions that has roused such bitter antagonism and violent denunciations. Rightly understood and used, psychoanalysis may be compared to surgery, for psychoanalysis stands in the same relation to the personality as surgery does to the body, and they aim at parallel results. It is well recognized that in the last analysis nature is the real physician, the healer of wounds; but prior to the development of our modern asepsis and surgical technique the healing produced by nature was most often of a very faulty and imperfect type—hideous scars, distorted and crippled limbs, with functions impaired or incapacitated, resulted from the wounds, or else nature was unable to cope with the hurt and the injured one succumbed. Science has been steadily working for centuries with the aim of understanding nature and finding means to aid and co-operate with her so that healing could take place with the least possible loss of function or permanent injury to the individual. Marvelous results have rewarded these persistent efforts, as the brilliant achievements of surgery plainly indicate. Meantime, however, little thought was given to the possibility of any scientific method being available to help man overcome the wounds and conflicts taking place in his soul, hurts which retarded his development and progress as a personality, and which frequently in the struggle resulted in physical pains and symptoms of the most varied character.  That was left solely to religion and metaphysics. Now, however, this same assistance that surgery has given to the physical body, psychoanalysis attempts to give to the personality. That it cannot always succeed is as much to be expected, and more, than that surgery does not always succeed, for the analytic work requires much of the individual. No real result can be attained if he has not already developed a certain quality of character and intelligence which makes it possible for him to submit himself to a facing of his naked soul, and to the pain and suffering which this often entails. Here, as in no other relation in life, an absolute truth and an absolute honesty are the only basis of action, since deception of any kind deceives no one but the individual himself and acts as a boomerang, defeating his own aims. Such deep searching and penetrating into the soul is not something to be undertaken lightly nor to be considered a trivial or simple matter, and the fact is that where a strong compulsion is lacking, such as sickness or a situation too difficult to meet, much courage is required to undertake it. In order to understand this psychology which is pervading all realms of thought and seems destined to be a new psychological-philosophical system for the understanding and practical advancement of human life, it will be necessary to go somewhat into detail regarding its development and present status. For in this new direction lies its greatest value and its greatest danger. The beginnings of this work were first published in 1895 in a book entitled “Studien über Hysterie,” and contained the joint investigations into hysteria of Dr. Breuer of Vienna and his pupil Dr. Sigmund Freud.  The results of their investigations seemed to show that the various symptoms grouped under the title of hysteria were the result of emotionally colored reminiscences which, all unknown to the conscious waking self, were really actively expressing themselves through the surrogate form of symptoms and that these experiences, although forgotten by the patient, could be reproduced and the emotional content discharged. Hypnosis was the means used to enable the physician to penetrate deeply into the forgotten memories, for it was found through hypnosis that these lost incidents and circumstances were not really lost at all but only dropped from consciousness, and were capable of being revived when given the proper stimuli. The astonishing part about it was that with the revival of these memories and their accompanying painful and disturbing emotions, the symptoms disappeared. This led naturally to the conclusion that these symptoms were dependent upon some emotional disturbance or psychic trauma which had been inadequately expressed, and that in order to cure the patient one merely had to establish the connection between the memory and the emotions which properly belonged to it, letting the emotion work itself out through a reproduction of the forgotten scene. With further investigation Freud found that hypnosis was unnecessary for the revival of the forgotten experiences, and that it was possible to obtain the lost emotional material in the conscious and normal state.  For this purpose the patient was encouraged to assume a passive, non-critical attitude and simply let his thoughts flow, speaking of whatever came into his mind, holding nothing back. During this free and easy discussion of his life and conditions, directed by the law of association of ideas, reference was invariably made to the experiences or thoughts which were the most affective and disturbing elements. It was seen to be quite impossible to avoid this indirect revelation because of the strength of the emotions surrounding these ideas and the effect of the conscious wish to repress unpleasant feelings. This important group of ideas or impressions, with the feelings and emotions\\n<|eot_id|>'}\n"}]},{"cell_type":"code","source":"from transformers import IntervalStrategy\nimport wandb\n\n# Initialize wandb\nwandb.init(project=\"huggingface\", name=\"refined_training_run\")\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = dataset.select(range(len(dataset) // 10)),  # Use 10% of data for evaluation\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Set both max_steps and num_train_epochs\n        max_steps = 120,\n        num_train_epochs = 3,\n\n        # Use a single learning rate for all parameters\n        learning_rate = 5e-5,\n\n        # Warmup strategy from successful runs\n        warmup_steps = 10,\n        warmup_ratio = 0,\n\n        # Explicitly set precision based on hardware support\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        \n        logging_steps = 1,\n        \n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        \n        seed = 3407,\n        output_dir = \"outputs\",\n        \n        report_to = \"wandb\",  # Enable Weights & Biases logging\n        \n        # Set both save and evaluation strategies to 'steps'\n        save_strategy = IntervalStrategy.STEPS,\n        eval_strategy = IntervalStrategy.STEPS,\n        save_steps = 1,  # Save checkpoint every 20 steps\n        eval_steps = 1,  # Evaluate every 20 steps (matching save_steps)\n        \n        load_best_model_at_end = True,\n        metric_for_best_model = \"eval_loss\",\n    ),\n    compute_metrics = compute_metrics,\n)\n\n# ... (rest of the code remains the same)\n\n# Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\n# Start training\ntrainer_stats = trainer.train()\n\n# Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n# Generation code (unchanged)\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\nclass EndOfTextCriteria(StoppingCriteria):\n    def __init__(self, eos_token_id):\n        self.eos_token_id = eos_token_id\n\n    def __call__(self, input_ids, scores, **kwargs):\n        return input_ids[0][-1] == self.eos_token_id\n\nstopping_criteria = StoppingCriteriaList([EndOfTextCriteria(tokenizer.eos_token_id)])\n\noutputs = model.generate(**inputs, \n                         max_new_tokens=64, \n                         stopping_criteria=stopping_criteria,\n                         use_cache=True)\n\nprint(tokenizer.batch_decode(outputs))","metadata":{},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/html":["Finishing last run (ID:oggbocco) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6314ddf152534f50b097247318d9bde8","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/perplexity</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▃▄▅▆▆▇████████▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train/loss</td><td>▂▃▂▂▃▇▅▅▇▃▁▄▄▇▄▅▂▄█▃▂▂▄▂▂▄▅▂▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.88035</td></tr><tr><td>eval/perplexity</td><td>17.82017</td></tr><tr><td>eval/runtime</td><td>17.128</td></tr><tr><td>eval/samples_per_second</td><td>0.525</td></tr><tr><td>eval/steps_per_second</td><td>0.117</td></tr><tr><td>train/epoch</td><td>5.10638</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/grad_norm</td><td>inf</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>3.1006</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View run <strong style=\"color:#cdcd00\">refined_training_run</strong> at: <a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface/runs/oggbocco' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/huggingface/runs/oggbocco</a><br/> View project at: <a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/huggingface</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Find logs at: <code>./wandb/run-20240918_131021-oggbocco/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Successfully finished last run (ID:oggbocco). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Tracking run with wandb version 0.18.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Run data is saved locally in <code>/root/quantumLeap/wandb/run-20240918_131250-tddkvs0m</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface/runs/tddkvs0m' target=\"_blank\">refined_training_run</a></strong> to <a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View project at <a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View run at <a href='https://wandb.ai/olabs-asia-olabs-pro/huggingface/runs/tddkvs0m' target=\"_blank\">https://wandb.ai/olabs-asia-olabs-pro/huggingface/runs/tddkvs0m</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf77d611d9c0473cb32f9bf0903f76fe","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=2):   0%|          | 0/93 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45c52af6e2d6443ebc1693fb50a7a9c0","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=2):   0%|          | 0/9 [00:00<?, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"max_steps is given, it will override any value given in num_train_epochs\n"},{"name":"stdout","output_type":"stream","text":"GPU = NVIDIA H100 80GB HBM3. Max memory = 79.209 GB.\n\n42.086 GB of memory reserved.\n"},{"name":"stderr","output_type":"stream","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n\n   \\\\   /|    Num examples = 93 | Num Epochs = 24\n\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n\n\\        /    Total batch size = 16 | Total steps = 120\n\n \"-____-\"     Number of trainable parameters = 1,386,217,472\n"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  2/120 : < :, Epoch 0.17/24]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2/2 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","source":"# # delete previous trainer\n# del trainer\n\n#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\n\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n\n\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n\n# inputs = tokenizer(\n# [\n#     instruction_prompt.format(\n#         \"Hero Archetype\", # concept_name\n#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n#         \"\", # output - leave this blank for generation!\n#     )\n# ], return_tensors = \"pt\").to(\"cuda\")\n\n\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n#                    repetition_penalty = 0.1)","metadata":{},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"GPU = NVIDIA H100 80GB HBM3. Max memory = 79.209 GB.\n\n53.35 GB of memory reserved.\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37db69f015a94afd8f39127eb8187cd3","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=2):   0%|          | 0/89 [00:00<?, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"max_steps is given, it will override any value given in num_train_epochs\n\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n\n   \\\\   /|    Num examples = 89 | Num Epochs = 6\n\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\n\\        /    Total batch size = 8 | Total steps = 60\n\n \"-____-\"     Number of trainable parameters = 1,386,217,472\n"},{"ename":"RuntimeError","evalue":"Expected out tensor to have dtype c10::Half, but got float instead","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 42\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bfloat16_supported\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     18\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     ),\n\u001b[1;32m     40\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#@title Show final memory and time stats\u001b[39;00m\n\u001b[1;32m     45\u001b[0m used_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n","File \u001b[0;32m<string>:145\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n","File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/torch/autograd/function.py:306\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/torch/amp/autocast_mode.py:501\u001b[0m, in \u001b[0;36mcustom_bwd.<locals>.decorate_bwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(bwd)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_bwd\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast(\n\u001b[1;32m    497\u001b[0m         device_type\u001b[38;5;241m=\u001b[39mdevice_type,\n\u001b[1;32m    498\u001b[0m         enabled\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast,\n\u001b[1;32m    499\u001b[0m         dtype\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_dtype,\n\u001b[1;32m    500\u001b[0m     ):\n\u001b[0;32m--> 501\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda/envs/olabs/lib/python3.12/site-packages/unsloth/kernels/fast_lora.py:136\u001b[0m, in \u001b[0;36mLoRA_MLP.backward\u001b[0;34m(ctx, dY)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# dX  = matmul_lora(df, upW.t(), upW_quant, upB, upA, upS)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# dX += matmul_lora(de, gateW.t(), gateW_quant, gateB, gateA, gateS)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m upW \u001b[38;5;241m=\u001b[39m fast_dequantize(upW\u001b[38;5;241m.\u001b[39mt(), upW_quant)\n\u001b[0;32m--> 136\u001b[0m dX \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m upW\n\u001b[1;32m    138\u001b[0m dX \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m df \u001b[38;5;241m@\u001b[39m upB\u001b[38;5;241m.\u001b[39mto(dtype)\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m@\u001b[39m (upS \u001b[38;5;241m*\u001b[39m upA\u001b[38;5;241m.\u001b[39mto(dtype)\u001b[38;5;241m.\u001b[39mt())\n","\u001b[0;31mRuntimeError\u001b[0m: Expected out tensor to have dtype c10::Half, but got float instead"]}]},{"cell_type":"code","source":"import time\n\n# add current timestamp to model name\nmodel.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\") # Local saving\ntokenizer.save_pretrained(f\"qLeap_model_base_v0_{int(time.time())}\")\nmodel.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_base_{int(time.time())}\", token = \"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\") # Online saving\ntokenizer.push_to_hub(f\"olabs-ai/qLeap_model_base_v0_{int(time.time())}\", token = \"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\") # Online saving\n\n# # Merge to 16bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n\n# # Merge to 4bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n\n# # Just LoRA adapters\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n    \n    \n# # Save to 8bit Q8_0\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n\n# # Save to 16bit GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n\n# # Save to q4_k_m GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Hero Archetype\", # concept_name\n        \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)\n\n# inputs = tokenizer(\n# [\n#     instruction_prompt.format(\n#         \"Hero Archetype\", # concept_name\n#         \"The hero archetype is a common motif in literature and folklore, representing a protagonist who embodies bravery, resilience, and a quest for a greater purpose.\", # detailed_explanation\n#         \"\", # output - leave this blank for generation!\n#     )\n# ], return_tensors = \"pt\").to(\"cuda\")\n\n\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer)\n# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n#                    repetition_penalty = 0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instruction  Tuning","metadata":{}},{"cell_type":"code","source":"\n# Instruction FineTune - Create an instruction_pompt based on the concept_examples.csv file\n\nimport json\nimport ast\nimport logging\n\nimport csv\n\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/concept_examples.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    data = list(reader)\n    \ntype(data)\n\n\n# Configure logging\nlogging.basicConfig(\n    filename='transformation_errors.log',\n    filemode='w',\n    level=logging.ERROR,\n    format='%(levelname)s:%(message)s'\n)\n\n# Sample original data\noriginal_data = data\n\ndef transform_data(original_data):\n    \"\"\"\n    Transforms the original data by expanding 'example_scenario' into separate dictionaries.\n\n    Parameters:\n        original_data (list): List of dictionaries with 'concept_name', 'detailed_explanation', and 'example_scenario'.\n\n    Returns:\n        new_data (list): Transformed list with one 'example_scenario' per dictionary.\n    \"\"\"\n    new_data = []\n\n    for idx, entry in enumerate(original_data, start=1):\n        concept_name = entry.get('concept_name', '').strip()\n        detailed_explanation = entry.get('detailed_explanation', '').strip()\n        example_scenario_str = entry.get('example_scenario', '').strip()\n\n        if not concept_name or not detailed_explanation or not example_scenario_str:\n            logging.error(f\"Entry {idx} is missing required fields. Skipping.\")\n            continue\n\n        # Attempt to parse with json.loads\n        try:\n            example_scenarios = json.loads(example_scenario_str)\n            if not isinstance(example_scenarios, list):\n                raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n        except json.JSONDecodeError:\n            # Fallback to ast.literal_eval\n            try:\n                example_scenarios = ast.literal_eval(example_scenario_str)\n                if not isinstance(example_scenarios, list):\n                    raise ValueError(\"Parsed 'example_scenario' is not a list.\")\n            except (ValueError, SyntaxError) as e:\n                logging.error(f\"Entry {idx} ('{concept_name}') has invalid 'example_scenario': {e}\")\n                continue\n\n        # Iterate through each scenario and create a new entry\n        for scenario_idx, scenario in enumerate(example_scenarios, start=1):\n            if not isinstance(scenario, str):\n                logging.error(f\"Entry {idx} ('{concept_name}') has non-string scenario at position {scenario_idx}. Skipping this scenario.\")\n                continue\n\n            new_entry = {\n                'concept_name': concept_name,\n                'detailed_explanation': detailed_explanation,\n                'example_scenario': scenario.strip()\n            }\n            new_data.append(new_entry)\n\n    return new_data\n\n# Transform the data\ntransformed_data = transform_data(original_data)\n\n# Optional: Save the transformed data to a JSON file\nwith open('/root/quantumLeap/data/psychologoy-of-unconscious-mind/transformed_data.json', 'w', encoding='utf-8') as f:\n    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n\nprint(f\"Transformation complete. {len(transformed_data)} entries created.\")\nprint(\"Check 'transformation_errors.log' for any errors encountered during transformation.\")\n\nprint(len(transformed_data))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef instruction_prompt_func(examples):\n    concept_name = examples[\"concept_name\"]\n    detailed_explanation = examples[\"detailed_explanation\"]\n    example_scenario = examples[\"example_scenario\"]\n    return { \"text\" : instruction_prompt.format(concept_name, detailed_explanation, example_scenario), }\npass\n\n\n# convert transformed_data to a huggingface dataset\ninstruction_dataset = Dataset.from_dict(transformed_data)\ninstruction_dataset = instruction_dataset.map(instruction_prompt_func, batched = True,)\n\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = instruction_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 8,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n\n        # Use num_train_epochs and warmup_ratio for longer runs!\n        max_steps = 120,\n        warmup_steps = 10,\n        # warmup_ratio = 0.1,\n        # num_train_epochs = 1,\n\n        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n        learning_rate = 5e-5,\n        embedding_learning_rate = 1e-5,\n\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.00,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\ntrainer_stats = trainer.train()\n#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add current timestamp to model name\nmodel.save_pretrained(f\"qLeap_model_v0_{int(time.time())}\") # Local saving\ntokenizer.save_pretrained(f\"qLeap_model_instruct_v0_{int(time.time())}\")\nmodel.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\ntokenizer.push_to_hub(f\"olabs-ai/qLeap_model_instruct_v0_{int(time.time())}\", token = \"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\") # Online saving\n\n# # Merge to 16bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_16bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n\n# # Merge to 4bit\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_v0_4bit_merged_{int(time.time())}\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n\n# # Just LoRA adapters\n# if False: model.save_pretrained_merged(\"qLeap_model_v0_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\",)\n# if False: model.push_to_hub_merged(\"olabs-ai/qLeap_model_LoRA_merged_{int(time.time())}\", tokenizer, save_method = \"lora\", token = \"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\")\n    \n    \n# # Save to 8bit Q8_0\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer,)\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_8bit_Q8_{int(time.time())}\", tokenizer, token = \"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\")\n\n# # Save to 16bit GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_16bit_GGUF_{int(time.time())}\", tokenizer, quantization_method = \"f16\", token = \"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\")\n\n# # Save to q4_k_m GGUF\n# if False: model.save_pretrained_gguf(\"qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q4_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n# if False: model.push_to_hub_gguf(\"olabs-ai/qLeap_model_v0_q5_k_m_16bit_{int(time.time())}\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"\ninstruction_prompt = \"\"\"Below is an instruction that describes a concept in the field of psychology, sociology, anthropology, ethnography, or qualitative research or cultural studies. Write a response that appropriately completes the request.\n\n### Instruction: Given the concept and its detailed explanation, provide an example scenario that illustrates the concept.\nconcept_name: {}\ndetailed_explanation: {}\n\n### Response:\n{}\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"Give an example scenario that illustrates the concept of Hero archetype as described by Jungian psychology.\", # instruction\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\n# Text Streaming\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n\ninputs = tokenizer(\n[\n    instruction_prompt.format(\n        \"When trying to understand how nature plays a role in the development of a child's personality, which concept should be considered?\",\n        \"\", # output - leave this blank for generation!\n    ),\n], return_tensors = \"pt\").to(\"cuda\")\n\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   repetition_penalty = 0.1)","metadata":{},"execution_count":null,"outputs":[]}]}